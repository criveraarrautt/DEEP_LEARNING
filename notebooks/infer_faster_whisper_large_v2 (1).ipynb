{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eaa8ebf0495b422d907d212a688b191e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b752ed2b7cd4f17950973b88eae28c7",
              "IPY_MODEL_68ce779a7a6b4f1e81298d0677315a69",
              "IPY_MODEL_5742278dd3a247c4992c5239a9101f33"
            ],
            "layout": "IPY_MODEL_8ca4bad70207429299567ccb6a4fac1c"
          }
        },
        "3b752ed2b7cd4f17950973b88eae28c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b69c137f9b74f098be7916f61819f69",
            "placeholder": "​",
            "style": "IPY_MODEL_aa2c1752898a4ea6927da75f57d722d6",
            "value": "model.bin: 100%"
          }
        },
        "68ce779a7a6b4f1e81298d0677315a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca4df11ecd104b888fb63231264c832e",
            "max": 3086912962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_753aea2310c9479f83b76ac74a4141a5",
            "value": 3086912962
          }
        },
        "5742278dd3a247c4992c5239a9101f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa2a744cfdf045078750ed345532c9fd",
            "placeholder": "​",
            "style": "IPY_MODEL_1f9635b9007e43f0ab92db7b1e409785",
            "value": " 3.09G/3.09G [00:26&lt;00:00, 220MB/s]"
          }
        },
        "8ca4bad70207429299567ccb6a4fac1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b69c137f9b74f098be7916f61819f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa2c1752898a4ea6927da75f57d722d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca4df11ecd104b888fb63231264c832e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "753aea2310c9479f83b76ac74a4141a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa2a744cfdf045078750ed345532c9fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f9635b9007e43f0ab92db7b1e409785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bce6a55350e447d39ac1201762615c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01ece34d506f4d4293013a6fe22e3de5",
              "IPY_MODEL_cc1aa5925a114f8384279ad7993ec8fd",
              "IPY_MODEL_e01571f28f3c41d58e94774a261d13fe"
            ],
            "layout": "IPY_MODEL_10e564f34b5749beb55a50f9e2734dfa"
          }
        },
        "01ece34d506f4d4293013a6fe22e3de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ec0374be228426fb79ac2b21c5319f1",
            "placeholder": "​",
            "style": "IPY_MODEL_05e59e0c4546477eac54e72e23a0bfaa",
            "value": "vocabulary.txt: 100%"
          }
        },
        "cc1aa5925a114f8384279ad7993ec8fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cb3d7bda1b84917a8dbf1ea5f083324",
            "max": 459861,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c1ee4cf5c6e44cfa67d14c12db04c6c",
            "value": 459861
          }
        },
        "e01571f28f3c41d58e94774a261d13fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa1baf4ed26d41fb9ba22bd20b97d45b",
            "placeholder": "​",
            "style": "IPY_MODEL_ef2d76ed572a4620a619e81719250f51",
            "value": " 460k/460k [00:00&lt;00:00, 2.46MB/s]"
          }
        },
        "10e564f34b5749beb55a50f9e2734dfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ec0374be228426fb79ac2b21c5319f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05e59e0c4546477eac54e72e23a0bfaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cb3d7bda1b84917a8dbf1ea5f083324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c1ee4cf5c6e44cfa67d14c12db04c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa1baf4ed26d41fb9ba22bd20b97d45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef2d76ed572a4620a619e81719250f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd20f0a413ac4f9f931620a2ac0bb3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_846326c364834684b3f83c1224300c3c",
              "IPY_MODEL_f8871cdbfeb248e0945591c1ea26e733",
              "IPY_MODEL_90dfc3fc646a4bc5906a81f227f16550"
            ],
            "layout": "IPY_MODEL_55b89a797c2e423e89ffca9ffb2182d4"
          }
        },
        "846326c364834684b3f83c1224300c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bbc0acfd72342659979f3d142df10c0",
            "placeholder": "​",
            "style": "IPY_MODEL_ce6aac1363894d8492e225eca42aea8b",
            "value": "config.json: 100%"
          }
        },
        "f8871cdbfeb248e0945591c1ea26e733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ab00933c74145fc92ebc1aa99619c2d",
            "max": 2796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3b72d20e89f455891eabc61e36403c5",
            "value": 2796
          }
        },
        "90dfc3fc646a4bc5906a81f227f16550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9791dd8b7e74435915903132cd88f18",
            "placeholder": "​",
            "style": "IPY_MODEL_647f6c6df0aa442aae0ab12b1eb2eef8",
            "value": " 2.80k/2.80k [00:00&lt;00:00, 109kB/s]"
          }
        },
        "55b89a797c2e423e89ffca9ffb2182d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bbc0acfd72342659979f3d142df10c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce6aac1363894d8492e225eca42aea8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ab00933c74145fc92ebc1aa99619c2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b72d20e89f455891eabc61e36403c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9791dd8b7e74435915903132cd88f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "647f6c6df0aa442aae0ab12b1eb2eef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fe217700fac4a4894df3d0f8d744e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5698008eea504c3489fa0bae10d3de33",
              "IPY_MODEL_615729e3842c4212ac4889a0bd656bdc",
              "IPY_MODEL_2bdee3594b5b4f328dcd2f6da2c577ba"
            ],
            "layout": "IPY_MODEL_889117832e19494e9a559e8a093c58f1"
          }
        },
        "5698008eea504c3489fa0bae10d3de33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df27c55d33b048b39721f04fdb08675a",
            "placeholder": "​",
            "style": "IPY_MODEL_278f65e119b44e4eafd04aa8842f289e",
            "value": "tokenizer.json: 100%"
          }
        },
        "615729e3842c4212ac4889a0bd656bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19d55d916f6f4557955f5867ffd07cb4",
            "max": 2203239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9eecab7dd2fd480ca7a82990ed6b4832",
            "value": 2203239
          }
        },
        "2bdee3594b5b4f328dcd2f6da2c577ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f627d0aeb7004d2cac4fc697b20113a4",
            "placeholder": "​",
            "style": "IPY_MODEL_d5c8a4da834e4252bf284cd3d91ffafc",
            "value": " 2.20M/2.20M [00:00&lt;00:00, 7.02MB/s]"
          }
        },
        "889117832e19494e9a559e8a093c58f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df27c55d33b048b39721f04fdb08675a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "278f65e119b44e4eafd04aa8842f289e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19d55d916f6f4557955f5867ffd07cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eecab7dd2fd480ca7a82990ed6b4832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f627d0aeb7004d2cac4fc697b20113a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c8a4da834e4252bf284cd3d91ffafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhavs10/insanely-fast-whisper/blob/main/infer_faster_whisper_large_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q faster-whisper ipython-autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-3jAUK4or1d",
        "outputId": "e2726b2b-b9a5-4e6b-d532-0510bd3cd09a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqb4IGAnqThp",
        "outputId": "d780f524-4ec3-4d88-bcea-5b3251a12731"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 342 µs (started: 2024-08-24 11:35:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76vBxpWcK69I",
        "outputId": "d5e98e90-aa37-4b13-b335-56fa7bcb2679"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "time: 16 s (started: 2024-08-24 11:35:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Half-Precision"
      ],
      "metadata": {
        "id": "DNBvUbic_Gwi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "eaa8ebf0495b422d907d212a688b191e",
            "3b752ed2b7cd4f17950973b88eae28c7",
            "68ce779a7a6b4f1e81298d0677315a69",
            "5742278dd3a247c4992c5239a9101f33",
            "8ca4bad70207429299567ccb6a4fac1c",
            "3b69c137f9b74f098be7916f61819f69",
            "aa2c1752898a4ea6927da75f57d722d6",
            "ca4df11ecd104b888fb63231264c832e",
            "753aea2310c9479f83b76ac74a4141a5",
            "aa2a744cfdf045078750ed345532c9fd",
            "1f9635b9007e43f0ab92db7b1e409785",
            "bce6a55350e447d39ac1201762615c66",
            "01ece34d506f4d4293013a6fe22e3de5",
            "cc1aa5925a114f8384279ad7993ec8fd",
            "e01571f28f3c41d58e94774a261d13fe",
            "10e564f34b5749beb55a50f9e2734dfa",
            "5ec0374be228426fb79ac2b21c5319f1",
            "05e59e0c4546477eac54e72e23a0bfaa",
            "3cb3d7bda1b84917a8dbf1ea5f083324",
            "4c1ee4cf5c6e44cfa67d14c12db04c6c",
            "aa1baf4ed26d41fb9ba22bd20b97d45b",
            "ef2d76ed572a4620a619e81719250f51",
            "dd20f0a413ac4f9f931620a2ac0bb3b6",
            "846326c364834684b3f83c1224300c3c",
            "f8871cdbfeb248e0945591c1ea26e733",
            "90dfc3fc646a4bc5906a81f227f16550",
            "55b89a797c2e423e89ffca9ffb2182d4",
            "8bbc0acfd72342659979f3d142df10c0",
            "ce6aac1363894d8492e225eca42aea8b",
            "3ab00933c74145fc92ebc1aa99619c2d",
            "e3b72d20e89f455891eabc61e36403c5",
            "f9791dd8b7e74435915903132cd88f18",
            "647f6c6df0aa442aae0ab12b1eb2eef8",
            "9fe217700fac4a4894df3d0f8d744e98",
            "5698008eea504c3489fa0bae10d3de33",
            "615729e3842c4212ac4889a0bd656bdc",
            "2bdee3594b5b4f328dcd2f6da2c577ba",
            "889117832e19494e9a559e8a093c58f1",
            "df27c55d33b048b39721f04fdb08675a",
            "278f65e119b44e4eafd04aa8842f289e",
            "19d55d916f6f4557955f5867ffd07cb4",
            "9eecab7dd2fd480ca7a82990ed6b4832",
            "f627d0aeb7004d2cac4fc697b20113a4",
            "d5c8a4da834e4252bf284cd3d91ffafc"
          ]
        },
        "id": "PzoWzQwboMpk",
        "outputId": "121e8432-37b7-4f00-8f55-cfb5ad03afc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaa8ebf0495b422d907d212a688b191e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocabulary.txt:   0%|          | 0.00/460k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bce6a55350e447d39ac1201762615c66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/2.80k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd20f0a413ac4f9f931620a2ac0bb3b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fe217700fac4a4894df3d0f8d744e98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 35.1 s (started: 2024-08-24 11:39:51 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from faster_whisper import WhisperModel\n",
        "\n",
        "model_size = \"large-v2\"\n",
        "\n",
        "# Run on GPU with FP16\n",
        "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segments, info = model.transcribe(\"/content/drive/MyDrive/04 SEMESTRE MAD/Copia de Universidad Central - Deep Learning - Clase 2_ Regularización de Redes Neuronales (2024-08-17 07_15 GMT-5).mp3\", beam_size=1)\n",
        "\n",
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xwaBmkMrg5v",
        "outputId": "dee5cba1-d184-4932-8639-d6c538ae15ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language 'es' with probability 0.993652\n",
            "time: 20.2 s (started: 2024-08-24 11:40:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for segment in segments:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb6cf60qc1Y",
        "outputId": "0ea2ab77-3c27-4338-8bdb-8ea0ad5e1c60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00s -> 9.60s]  Iniciamos. Ah, pero un momento, si la grabación queda en inglés, tocaría ver cómo se está\n",
            "[9.60s -> 15.40s]  transcribiendo, no? Miremos en algún, en algún lado, yo no sé si se vea la transcripción.\n",
            "[16.72s -> 20.48s]  Bueno, veamos cómo se ve al final, pues porque estoy hablando en español, me imagino que la\n",
            "[20.48s -> 23.92s]  transcripción no va a tener sentido, pero bueno, miremos a ver, porque pues el team sí es\n",
            "[23.92s -> 33.20s]  interesante que se tiene. Bueno, listo. Bienvenidos y bienvenidas a esta segunda sesión de la maestría\n",
            "[33.20s -> 39.80s]  en analítica de datos, Deep Learning. Me excuso nuevamente que soy un poquito malito de la voz,\n",
            "[39.80s -> 53.88s]  pero entonces lo primero que quiero preguntarles es si alguien quiere exponer su tarea. Actualmente\n",
            "[54.60s -> 61.68s]  recibí. Listo, Cristian, dale adelante, pero antes de que comiences. Entonces actualmente recibí una,\n",
            "[64.40s -> 72.80s]  dos, tres, cuatro, faltan varias tareas, varias personas. Entonces tienen de aquí a las 10 para\n",
            "[72.80s -> 79.80s]  mandarla. Y la idea es que en este espacio socialicemos entre todos. ¿Qué hicieron? ¿Cómo les fue?\n",
            "[79.80s -> 90.68s]  Y pues vamos a dar la solución o mi solución. Listo. Entonces dale adelante, Cristian, cuéntanos\n",
            "[90.68s -> 99.00s]  cómo te fue. Perdón, si Cristian, yo que era Camilo. No, sí, sí, pero yo Camilo. Ahí estoy compartiendo.\n",
            "[99.00s -> 106.72s]  Listo, ya estamos viendo. Adelante. Listo, pues de acuerdo a la tarea, pues era transcribir la sesión\n",
            "[106.72s -> 113.68s]  completa de tres horas usando el modelo de Whisper. Usar ChagP3 para extraer los KPON de la sesión\n",
            "[113.68s -> 121.16s]  usando la transcripción Whisper. Crear un logo usando el modelo de generación de imágenes para\n",
            "[121.16s -> 127.92s]  cada uno de los grupos de la comunidad y pues leer los roles de la ciencia de datos. Pues acá lo que hice\n",
            "[127.92s -> 135.48s]  fue primero pues instalar OpenID Whisper. Posteriormente importé Whisper, el modelo de\n",
            "[135.48s -> 144.60s]  Whisper. Acá hay varios modelos. Uno que es largo, medio y pequeño que es Small. Acá convertir\n",
            "[144.60s -> 151.44s]  la transcripción del vídeo, pues de acuerdo a los tres vídeos que nos pasó, acá\n",
            "[151.44s -> 160.60s]  imprimí el primer vídeo. Entre esto, pues transcribí esto. Y acá en el ChagP3, pues hice la pregunta de\n",
            "[160.60s -> 167.00s]  qué eran los KPON, o bueno perdón, cuáles son los KPON de la transcripción. Y acá pues me dejó el primer vídeo.\n",
            "[167.00s -> 171.48s]  Así con los demás. Entonces acá se solucionaron dos puntos.\n",
            "[176.48s -> 177.48s]  Después...\n",
            "[177.48s -> 181.48s]  Camilo, déjame leer los KPON brevemente, por favor.\n",
            "[181.48s -> 188.48s]  No solamente ese, sino que quisiera ver. Dame dos segunditos.\n",
            "[191.48s -> 199.48s]  Ok. Ajá. Bueno, vale la pena decir... Listo. Perfecto. Chévere. Crítica.\n",
            "[201.48s -> 208.48s]  Crítica hacia la idea de que la educación es importante en la actualidad. No, bueno, hay que revisar lo que es los KPON, ¿no?\n",
            "[208.48s -> 220.48s]  Yo no dije eso. Relancia. Dije la... que la educación como está planteada hoy en día no es importante. No es lo importante.\n",
            "[221.36s -> 222.36s]  Bueno.\n",
            "[222.36s -> 236.36s]  ¿Sabes por qué pasó eso, profe? Porque resulta que en el modelo, cuando uno coloca el modelo, pues dependiendo también como la calidad, yo lo llamo así.\n",
            "[236.36s -> 246.36s]  Por ejemplo, acá había el modelo largis, creo que se llama. De este, se votaba la información, pero se demoraba muchísimo.\n",
            "[247.24s -> 251.24s]  Acá yo me acuerdo que dejé como cinco horas y no dejó.\n",
            "[251.24s -> 260.24s]  Entonces dentro del chat sí vi que muchos le cambiaron el tipo de modelo, pero teniendo en cuenta la calidad, pues ahí se baja.\n",
            "[260.24s -> 269.24s]  Yo me imagino que cuando se bajó la calidad, pues de acuerdo a esto, pues mostró de cierta manera, pues algunos textos que no eran coherentes.\n",
            "[269.24s -> 272.24s]  Es la intuición.\n",
            "[273.12s -> 279.12s]  Listo. Perfecto. Listo, Cristian, espérate un segundito.\n",
            "[279.12s -> 283.12s]  Desafíos, expectativas, proyectos relevantes.\n",
            "[283.12s -> 289.12s]  Ajá. Ok, pero esto habla... suena como si hablara solo de mí, ¿no?\n",
            "[289.12s -> 292.12s]  Listo, dale, sigue. Continúa con el segundo punto.\n",
            "[292.12s -> 295.12s]  Un segundo, pues acá están los KPON.\n",
            "[303.12s -> 306.12s]  Sí, dale, ya leí, dale, adelante.\n",
            "[306.12s -> 310.12s]  Y el tercero era este último.\n",
            "[317.12s -> 327.12s]  Ok, ok, súper interesante. Y te pusiste a averiguar, digamos, esos KPON, esas palabras como PCA, TSN, UMAP, BAGIN, BOOSTING,\n",
            "[327.12s -> 331.12s]  te pusiste a mirar de pronto qué significaban o simplemente hiciste...\n",
            "[332.00s -> 335.00s]  No, pero ahí sí, ahí sí me corché, no hay más alcance, perdón.\n",
            "[335.00s -> 338.00s]  Ahí sí, qué pena.\n",
            "[342.00s -> 345.00s]  Pero no, queda de tarea, lo voy a averiguar. Discúlpeme.\n",
            "[346.00s -> 352.00s]  Bueno, no, tranquilos, acá nada es obligatorio, solo es como para saber si entendiste lo que...\n",
            "[352.00s -> 356.00s]  Listo. No, excelente, excelente, me gusta, me gusta la iniciativa.\n",
            "[356.00s -> 359.00s]  Ah, bueno, y ahora nos vas a mostrar Dalí, o bueno, no sé.\n",
            "[359.88s -> 366.88s]  Dalí, pues yo lo que traté fue primero a ver si se podía generar un logo, pues de acuerdo acá en Python,\n",
            "[366.88s -> 374.88s]  pues este no, no, medio fue una imagen, no, no, una muy buena imagen, medio un circulito y un hombrecito ahí.\n",
            "[374.88s -> 381.88s]  Entonces yo sí busqué en Dalí los posibles logos, por ejemplo, de cada uno de los blogos.\n",
            "[382.76s -> 388.76s]  Por ejemplo, logos de lenguaje natural, pues acá cogí este, este y este.\n",
            "[389.76s -> 399.76s]  De los posibles logos de computer vision, cogí este, este, este y este.\n",
            "[400.76s -> 405.76s]  Y posibles logos de series de tiempo, este, este y este.\n",
            "[406.64s -> 412.64s]  Eso de acuerdo, pues, a un PROM, no sé si tenga por aquí el PROM.\n",
            "[413.64s -> 416.64s]  Acá dentro de los PROM, pues hice la pregunta.\n",
            "[418.64s -> 424.64s]  Antes de hacer la pregunta, pues lo primero que hice fue la consulta de qué posibles, por ejemplo, ideas,\n",
            "[424.64s -> 428.64s]  teniendo en cuenta esto, me podría ayudar para generar el PROM.\n",
            "[428.64s -> 435.64s]  Él me da cinco ideas, seis ideas, y posiblemente, pues cogí una de estas ideas y le solicité que me hiciera la imagen.\n",
            "[436.64s -> 441.64s]  Y acá, pues ya dejó el tema de la imagen, así para ser un poquito más preciso.\n",
            "[442.64s -> 446.64s]  Y ya la última pregunta era leer en el notebook los roles de ciencia de datos.\n",
            "[447.64s -> 452.64s]  Entonces acá en este caso, pues, habían como diez roles,\n",
            "[452.64s -> 457.64s]  desde el arquitecto de datos hasta el ingeniero MLOPS.\n",
            "[458.64s -> 461.64s]  Y pues acá una pequeña descripción de cada uno de ellos.\n",
            "[462.52s -> 465.52s]  Excelente. Listo. Gracias, Cristian.\n",
            "[466.52s -> 471.52s]  Muy bonito. Yo quiero que tomes solamente una imagen de cada grupo\n",
            "[472.52s -> 475.52s]  y la publiques en cada grupo. Sí.\n",
            "[476.52s -> 478.52s]  Y ahorita vamos a hacer la votación.\n",
            "[479.52s -> 482.52s]  Perfecto. Listo. Gracias, Cristian.\n",
            "[482.52s -> 490.52s]  Entonces, los que siguen, si tienen el, digamos, una solución muy similar a la de Cristian,\n",
            "[491.40s -> 494.40s]  que pueden exponer de aquí a los que quieran exponer.\n",
            "[495.40s -> 498.40s]  Si es lo mismo, tranquilos, lo exponen, dicen como tal,\n",
            "[498.40s -> 502.40s]  solo si hay alguna cosa diferente de lo que se expuso, lo enfatizan.\n",
            "[502.40s -> 506.40s]  ¿Listo? Los logos, pues, sí, muéstrenlos y hacen la misma tarea que Cristian.\n",
            "[506.40s -> 510.40s]  Si hicieron varios logos por grupo, elijan solo uno,\n",
            "[511.40s -> 515.40s]  que ustedes decidan o que alguien decida, una inteligencia artificial, no sé,\n",
            "[516.28s -> 518.28s]  y publiquenlo en cada uno de los grupos\n",
            "[519.28s -> 521.28s]  porque vamos a hacer la votación inmediatamente después.\n",
            "[522.28s -> 525.28s]  ¿Listo? Entonces, listo, adelante.\n",
            "[526.28s -> 528.28s]  Creo que está primero Fernando, ¿no?\n",
            "[529.28s -> 530.28s]  Entonces, dale, Fernando.\n",
            "[540.28s -> 542.28s]  Buenos días, compañeros.\n",
            "[543.16s -> 546.16s]  Ahí ya se ve la pantalla, ¿verdad?\n",
            "[547.16s -> 548.16s]  Sí, sí, señor.\n",
            "[549.16s -> 553.16s]  Listo. Entonces, lo primero que hice fue la instalación de las librerías.\n",
            "[555.16s -> 559.16s]  Pongo la recomendación de usar un entorno con T4\n",
            "[560.16s -> 562.16s]  para que, pues, sea rápido.\n",
            "[563.16s -> 566.16s]  No sé qué, ahí todos los esperando aquí, del resultado.\n",
            "[567.16s -> 569.16s]  La carga de librerías.\n",
            "[570.04s -> 575.04s]  Entonces, acá tenía inicialmente una para extraer el audio del video.\n",
            "[576.04s -> 579.04s]  Sin embargo, pues, los videos estaban bastante pesados.\n",
            "[580.04s -> 581.04s]  Llenó el drive.\n",
            "[582.04s -> 587.04s]  Entonces, dejé la función creada, pero finalmente monté los audios.\n",
            "[588.04s -> 592.04s]  Importamos Whisper, también Transformers y Pipeline.\n",
            "[593.04s -> 597.04s]  Transformers, básicamente es para echarle mano a los modelos que están en Hugging Face.\n",
            "[597.92s -> 600.92s]  Importé algunas como Spice y, bueno,\n",
            "[601.92s -> 604.92s]  fueron librerías para hacer algunos ensayos\n",
            "[605.92s -> 607.92s]  de qué tomaba como insight y todo eso.\n",
            "[608.92s -> 610.92s]  Finalmente me fui con la de OpenAI, claramente.\n",
            "[611.92s -> 612.92s]  También importé Picot\n",
            "[613.92s -> 616.92s]  para guardar algunos resultados que iba obteniendo.\n",
            "[619.92s -> 623.92s]  Importé TicTok, esta sirve para contar los tokens.\n",
            "[624.80s -> 628.80s]  Y en ciertas ocasiones, cuando la cadena era muy larga,\n",
            "[629.80s -> 631.80s]  pues, era para darle manejo a este tipo de cosas.\n",
            "[632.80s -> 634.80s]  Quiero saber cuántos tokens se estaban enviando\n",
            "[635.80s -> 637.80s]  y cuál era la capacidad de cada modelo,\n",
            "[638.80s -> 640.80s]  o de la API de OpenAI.\n",
            "[642.80s -> 646.80s]  Aquí, pues, se inserta la API key de OpenAI,\n",
            "[647.80s -> 648.80s]  la que cada uno tenga en su cuenta.\n",
            "[649.80s -> 651.80s]  Yo lo dejé así para mostrarlo.\n",
            "[652.68s -> 654.68s]  Creamos como las funciones.\n",
            "[655.68s -> 657.68s]  Entonces, una función de extraer audio,\n",
            "[658.68s -> 659.68s]  otra de speech to text.\n",
            "[660.68s -> 664.68s]  Aquí, pues, terminé usando el modelo que estaba en Hugging Face.\n",
            "[665.68s -> 667.68s]  Utilicé este WhisperLarge v2.\n",
            "[668.68s -> 670.68s]  Sin embargo, pues, ya está disponible el v3.\n",
            "[671.68s -> 676.68s]  Implementamos, pues, que utilice la GPU disponible.\n",
            "[679.68s -> 680.68s]  Otra función de contar tokens.\n",
            "[681.56s -> 685.56s]  Otra función para extraer los keypoints con GPT.\n",
            "[686.56s -> 688.56s]  Y, pues, utilicé el mejor modelo.\n",
            "[689.56s -> 690.56s]  Pues, probé con varios.\n",
            "[691.56s -> 694.56s]  Sin embargo, me fui con el mejor en este caso, el PROM.\n",
            "[697.56s -> 699.56s]  En donde, básicamente, se le piden temas principales,\n",
            "[700.56s -> 702.56s]  tareas y proyectos sugeridos, roles y contribuciones.\n",
            "[704.56s -> 707.56s]  Roles, contribuciones y las profesiones de los participantes.\n",
            "[708.44s -> 711.44s]  Y una función en donde se integran las funciones\n",
            "[712.44s -> 714.44s]  de extraer el audio y el video,\n",
            "[715.44s -> 718.44s]  que es este común comentario por lo que ya comenté previamente,\n",
            "[719.44s -> 722.44s]  convertir el audio al texto y extraer los insights.\n",
            "[723.44s -> 726.44s]  De modo que los resultados se dan como un diccionario\n",
            "[727.44s -> 729.44s]  en donde se devuelve la transcripción y ya el resumen.\n",
            "[731.44s -> 733.44s]  Montamos la unidad de...\n",
            "[734.32s -> 737.32s]  Montamos la unidad de Drive.\n",
            "[739.32s -> 743.32s]  Establecemos cuál es la lista de audios.\n",
            "[744.32s -> 747.32s]  Entonces, está el audio 1, audio 2, audio 3.\n",
            "[748.32s -> 751.32s]  Y, básicamente, ese listado lo que hace es que\n",
            "[752.32s -> 754.32s]  saca la transcripción de cada audio\n",
            "[755.32s -> 757.32s]  y la une con el audio anterior.\n",
            "[758.32s -> 761.32s]  Con eso quedamos con una sola cadena de texto continua.\n",
            "[762.20s -> 765.20s]  El resultado, pues, básicamente...\n",
            "[766.20s -> 767.20s]  Ah, bueno.\n",
            "[768.20s -> 770.20s]  Obtenemos el resultado, que son los key points,\n",
            "[771.20s -> 773.20s]  la transcripción y el resumen.\n",
            "[774.20s -> 776.20s]  Y los guardé como un objeto picol,\n",
            "[777.20s -> 780.20s]  por si después ya no tenía recursos gratis de la T4.\n",
            "[781.20s -> 783.20s]  Pero, pues, me alcanzaron.\n",
            "[784.20s -> 786.20s]  Los resultados, básicamente, fueron los siguientes.\n",
            "[787.20s -> 790.20s]  ¿Qué puedo destacar y qué me parece súper interesante?\n",
            "[791.08s -> 794.08s]  Que tuve en cuenta, cuando nombraste al profe Oscar,\n",
            "[795.08s -> 797.08s]  un colaborador en la planificación del curso,\n",
            "[798.08s -> 800.08s]  específicamente integración de contenidos relevantes en la industria,\n",
            "[801.08s -> 803.08s]  y a Paula como asistente en la clase.\n",
            "[804.08s -> 806.08s]  Ayuda con la organización, facilita la comunicación\n",
            "[807.08s -> 808.08s]  entre estudiantes y profesor.\n",
            "[809.08s -> 810.08s]  Me pareció súper chévere.\n",
            "[811.08s -> 815.08s]  También dio como los temas tocados de forma general,\n",
            "[816.08s -> 818.08s]  pero sin dejar nada por fuera.\n",
            "[818.96s -> 820.96s]  Nombrando aquí, por ejemplo, a Hugin Faze.\n",
            "[821.96s -> 823.96s]  Cuando nombraste a Whisper y a Florence.\n",
            "[824.96s -> 826.96s]  Como aplicaciones prácticas.\n",
            "[828.96s -> 831.96s]  Las tareas como utilizar Whisper para transcribir el audio de la clase,\n",
            "[832.96s -> 834.96s]  usar ChattyPT para resumir puntos claves de la transcripción.\n",
            "[835.96s -> 837.96s]  Diseñar logos para el grupo de WhatsApp usando Dalí.\n",
            "[838.96s -> 840.96s]  Explorar la creatividad y aplicar conceptos de diseño asistido\n",
            "[841.96s -> 842.96s]  por inteligencia artificial.\n",
            "[844.96s -> 847.96s]  También habla de la presentación de la clase.\n",
            "[848.96s -> 849.96s]  La aplicación sobre un modelo específico.\n",
            "[850.96s -> 852.96s]  La aplicabilidad en un contexto real.\n",
            "[855.96s -> 858.96s]  Habla del profe como que a pesar de ser educador,\n",
            "[859.96s -> 861.96s]  pues tiene experiencia en otros temas.\n",
            "[862.96s -> 864.96s]  Y destaca lo de pruebas estatales y proyectos en el TAN.\n",
            "[865.96s -> 867.96s]  Y un pequeño resumen de los estudiantes.\n",
            "[868.96s -> 871.96s]  Entonces, básicamente dice que son de diversas carreras.\n",
            "[872.96s -> 874.96s]  Que están buscando aplicar Deep Learning en sus campos.\n",
            "[875.96s -> 877.96s]  Con roles que van desde analistas de datos, desarrolladores\n",
            "[878.96s -> 879.96s]  y científicos de datos.\n",
            "[883.96s -> 886.96s]  Y pues es un resumen creo que bastante conciso.\n",
            "[887.96s -> 891.96s]  Como para tener una idea muy buena de las tres horas de clase.\n",
            "[894.96s -> 896.96s]  Ya en términos de los logos.\n",
            "[899.96s -> 900.96s]  Tengo los siguientes.\n",
            "[901.84s -> 903.84s]  Entonces, este para visión por computadora.\n",
            "[908.84s -> 910.84s]  Este para Natural Language Processing.\n",
            "[913.84s -> 915.84s]  Y este para Time Series.\n",
            "[920.84s -> 921.84s]  Superinteresante.\n",
            "[922.84s -> 925.84s]  ¿Cuál fue el problema que usaste para el primero?\n",
            "[926.72s -> 929.72s]  ¿Cuál fue el problema que usaste para el primero?\n",
            "[931.72s -> 933.72s]  Sí, se puede saber, tranquilo.\n",
            "[934.72s -> 936.72s]  Aquí en pro de la clase.\n",
            "[939.72s -> 941.72s]  Entonces, dice, eres un asistente experto en educación,\n",
            "[942.72s -> 945.72s]  escritura de documentos e inteligencia artificial.\n",
            "[946.72s -> 948.72s]  Tengo la transcripción de una clase introductoria sobre Deep Learning.\n",
            "[949.72s -> 951.72s]  Impartida como parte de una maestría en analítica de datos.\n",
            "[952.72s -> 954.72s]  Tu tarea es extraer los siguientes insights.\n",
            "[955.60s -> 956.60s]  Clave, temas principales.\n",
            "[957.60s -> 959.60s]  Resumen los temas más importantes tratados durante la clase.\n",
            "[960.60s -> 962.60s]  Destacando los conceptos y técnicas clave explicados.\n",
            "[963.60s -> 964.60s]  Tareas y proyectos sugeridos.\n",
            "[965.60s -> 967.60s]  Identifica cualquier tarea, ejercicio o proyecto mencionado\n",
            "[968.60s -> 969.60s]  que los estudiantes deban realizar,\n",
            "[970.60s -> 972.60s]  incluyendo detalles sobre los objetivos de estas tareas.\n",
            "[973.60s -> 975.60s]  Roles, contribuciones y profesiones.\n",
            "[976.60s -> 979.60s]  Describe los roles específicos de las personas mencionadas en la transcripción.\n",
            "[980.60s -> 982.60s]  Por ejemplo, profesores, asistentes de enseñanza, estudiantes.\n",
            "[983.48s -> 985.48s]  Y sus respectivas contribuciones durante la clase.\n",
            "[986.48s -> 988.48s]  Adicionalmente, resume las profesiones y cargos laborales\n",
            "[989.48s -> 990.48s]  de las personas que intervienen en la clase,\n",
            "[991.48s -> 992.48s]  indicando si son roles analíticos.\n",
            "[993.48s -> 996.48s]  La transcripción, y básicamente acá se inserta el texto\n",
            "[997.48s -> 1000.48s]  que ya se había extraído de cada uno de los audios\n",
            "[1001.48s -> 1002.48s]  y unido posteriormente,\n",
            "[1003.48s -> 1005.48s]  proporciona una lista organizada de los insights mencionados.\n",
            "[1006.48s -> 1009.48s]  Separa las tareas asignadas a los estudiantes en una lista par.\n",
            "[1010.48s -> 1011.48s]  La respuesta que proporciones debe tener\n",
            "[1012.36s -> 1014.36s]  entre 3.500 y 4.000 tokens.\n",
            "[1015.36s -> 1019.36s]  Buscando aprovechar al máximo que las respuestas de 4.096 de ese modelo.\n",
            "[1022.36s -> 1026.36s]  Excelente. Muy bien. Muy buen prompt. Me gusta.\n",
            "[1027.36s -> 1031.36s]  Listo. Pero, ¿y el del logo?\n",
            "[1032.36s -> 1033.36s]  ¿Cómo fue el del logo?\n",
            "[1034.36s -> 1035.36s]  Voy para allá.\n",
            "[1036.36s -> 1039.36s]  Muy breve, porque tus compañeros también están que se hablen.\n",
            "[1040.24s -> 1041.24s]  Sí.\n",
            "[1046.24s -> 1048.24s]  Pues, el del logo, básicamente,\n",
            "[1051.24s -> 1055.24s]  hice un pequeño como prompt principal\n",
            "[1056.24s -> 1060.24s]  en donde le decía la tarea general y que me ayudara con tres prompts.\n",
            "[1061.24s -> 1063.24s]  Me dio tres prompts que no son exactamente estos,\n",
            "[1065.24s -> 1068.24s]  pero de ahí les hice algunas modificaciones.\n",
            "[1069.12s -> 1072.12s]  Para lograr estos resultados.\n",
            "[1073.12s -> 1077.12s]  Excelente. Listo. Me encanta tu aproximación.\n",
            "[1078.12s -> 1079.12s]  Gracias.\n",
            "[1080.12s -> 1082.12s]  No me acuerdo. Fernando. Gracias, Fernando.\n",
            "[1083.12s -> 1084.12s]  Gracias.\n",
            "[1085.12s -> 1086.12s]  Tomo con los apunticos.\n",
            "[1087.12s -> 1089.12s]  Listo. Adelante. Siguiente.\n",
            "[1090.12s -> 1092.12s]  Yo he visto que hay una chica, pero no sé si...\n",
            "[1093.12s -> 1095.12s]  Creo que bajó la mano. Se asustó.\n",
            "[1096.12s -> 1097.12s]  Tranquilo. Es para compartir.\n",
            "[1098.00s -> 1101.00s]  Era yo, profe, pero rápidamente bajé la mano\n",
            "[1102.00s -> 1105.00s]  porque la solución es muy similar a la de los compañeros anteriores.\n",
            "[1106.00s -> 1108.00s]  Creo que lo único fue que no hice por medio de código\n",
            "[1109.00s -> 1113.00s]  el cambio de video a audio, sino que, pues, nada,\n",
            "[1114.00s -> 1116.00s]  usé una solución en internet y cargué el video,\n",
            "[1117.00s -> 1120.00s]  saqué el audio y eso fue lo que cargué mediante código.\n",
            "[1121.00s -> 1123.00s]  No lo hice y ya.\n",
            "[1123.88s -> 1126.88s]  Y utilicé el generador de imágenes de Copilot,\n",
            "[1127.88s -> 1130.88s]  pero creo que se está unido a, de algún modo, a Dale.\n",
            "[1131.88s -> 1135.88s]  Entonces creo que son las únicas diferencias que tengo en mis respuestas.\n",
            "[1137.88s -> 1140.88s]  Está chévere. Yo también hago lo mismo.\n",
            "[1141.88s -> 1145.88s]  Pues cuando necesito rapidez y no tengo el código hecho,\n",
            "[1146.88s -> 1149.88s]  simplemente hay una aplicación en mi Mac\n",
            "[1150.76s -> 1154.76s]  y yo simplemente pongo el video y le quito el audio.\n",
            "[1155.76s -> 1157.76s]  Claro, cuando son mil audios, no hice.\n",
            "[1158.76s -> 1161.76s]  Bueno, ahora sí me pegó la pela, pero es una solución, de hecho, muy práctica.\n",
            "[1162.76s -> 1164.76s]  Hubiera sido interesante que lo mostraras, ya nos lo comentaste.\n",
            "[1165.76s -> 1167.76s]  Entonces, super bacano.\n",
            "[1168.76s -> 1170.76s]  Pásalos los logos, si quieres, al grupo.\n",
            "[1171.76s -> 1172.76s]  Vamos a votar.\n",
            "[1173.76s -> 1174.76s]  Listo.\n",
            "[1175.76s -> 1176.76s]  Daniel Leonardo, adelante.\n",
            "[1177.64s -> 1178.64s]  Muy cortito, todos por fin,\n",
            "[1179.64s -> 1183.64s]  para que podamos tener la segunda y tercera hora\n",
            "[1184.64s -> 1186.64s]  provechosas para la clase que les quiero mostrar varias cositas.\n",
            "[1188.64s -> 1189.64s]  Buenos días, Proge.\n",
            "[1190.64s -> 1192.64s]  No sé si ya está viendo mi pantalla. Sí.\n",
            "[1193.64s -> 1194.64s]  Sí, ya se ve.\n",
            "[1195.64s -> 1196.64s]  Ok.\n",
            "[1197.64s -> 1199.64s]  Yo utilicé una solución para desearle al compañero.\n",
            "[1200.64s -> 1203.64s]  Entonces, pues como los videos estaban tan pesados,\n",
            "[1204.52s -> 1208.52s]  lo que hice fue cargarlos directamente desde el drive.\n",
            "[1211.52s -> 1213.52s]  Entonces, lo que hice fue...\n",
            "[1214.52s -> 1215.52s]  No sé si ahí es, pero un momentito.\n",
            "[1216.52s -> 1218.52s]  Ahí se están viendo, sí.\n",
            "[1220.52s -> 1223.52s]  Entonces, lo que hice fue utilizar una librería de Mopi\n",
            "[1224.52s -> 1226.52s]  para poder extraer el audio del video.\n",
            "[1227.52s -> 1230.52s]  Entonces, pues cargué las librerías, cargué el de Moviepi,\n",
            "[1231.40s -> 1235.40s]  de Videoclip, cargué la de OpenAI de Whisper directamente de Github\n",
            "[1236.40s -> 1237.40s]  y pues cargué Whisper.\n",
            "[1238.40s -> 1240.40s]  Entonces, en la línea aquí, pues lo que hice fue cargar,\n",
            "[1241.40s -> 1243.40s]  llamarle, pues para poder cargarlo del drive.\n",
            "[1244.40s -> 1246.40s]  Aquí, pues para estar seguro cómo había nombrado los videos.\n",
            "[1247.40s -> 1252.40s]  Y aquí creé la función para extraer el audio del video.\n",
            "[1253.40s -> 1256.40s]  Aquí lo que hace entonces es llamar dónde está el video\n",
            "[1257.40s -> 1258.40s]  y dónde lo voy a guardar.\n",
            "[1259.40s -> 1260.40s]  Lo guardaba directamente, pues aquí.\n",
            "[1261.40s -> 1265.40s]  Igual, usé una T4 para que fuera rápido.\n",
            "[1266.40s -> 1268.40s]  Y se puso en el video 1, video 2 y video 3.\n",
            "[1269.40s -> 1271.40s]  Luego hice la transcripción con el modelo de Whisper.\n",
            "[1272.40s -> 1274.40s]  Utilicé el Medium\n",
            "[1275.40s -> 1278.40s]  porque no quería sacrificar la calidad.\n",
            "[1279.40s -> 1281.40s]  Si usaba un Small, sabía que no iba a ser tan preciso\n",
            "[1282.40s -> 1283.40s]  y un Large consumía demasiada máquina.\n",
            "[1284.40s -> 1285.40s]  Entonces, utilicé un Medium.\n",
            "[1286.40s -> 1289.40s]  Normalmente se demoró por cada video 8 minutos procesando\n",
            "[1290.28s -> 1291.28s]  para extraer el audio.\n",
            "[1292.28s -> 1294.28s]  Entonces, aquí lo que hago es que los guardo en un texto,\n",
            "[1295.28s -> 1296.28s]  tanto el primero, el segundo y el tercero.\n",
            "[1297.28s -> 1298.28s]  Para...\n",
            "[1299.28s -> 1304.28s]  No entendí o no supe cómo hacer el extraer los key points.\n",
            "[1305.28s -> 1307.28s]  Entonces, busqué en ChatGPT, pues me va a...\n",
            "[1308.28s -> 1309.28s]  Busqué ayuda en ChatGPT,\n",
            "[1310.28s -> 1315.28s]  diciéndoles si había algún modelo ya entrenado en Hugging Face.\n",
            "[1316.28s -> 1318.28s]  Entonces, probé con varios.\n",
            "[1319.16s -> 1320.16s]  No fueron tan precisos.\n",
            "[1321.16s -> 1322.16s]  Utilicé este último que era T5.\n",
            "[1323.16s -> 1324.16s]  Lo mismo, tocaba tokenizar\n",
            "[1325.16s -> 1326.16s]  porque como el texto era tan extenso,\n",
            "[1327.16s -> 1330.16s]  entonces no corría bien.\n",
            "[1331.16s -> 1332.16s]  Intenté con...\n",
            "[1333.16s -> 1334.16s]  Se me olvidó el nombre del otro.\n",
            "[1335.16s -> 1336.16s]  Bueno, intenté con tres modelos diferentes\n",
            "[1337.16s -> 1338.16s]  y este fue el que mejor dio,\n",
            "[1339.16s -> 1341.16s]  pero de todas maneras no me funcionó bien.\n",
            "[1342.16s -> 1343.16s]  Entonces, aquí lo que hice fue lo mismo.\n",
            "[1344.16s -> 1345.16s]  Crear unas funciones para cargar los textos,\n",
            "[1346.16s -> 1347.16s]  dividir los textos,\n",
            "[1348.04s -> 1349.04s]  tokenizarlos\n",
            "[1351.04s -> 1354.04s]  y pues guardarlos en un punto TXT.\n",
            "[1355.04s -> 1358.04s]  Pero pues esta parte realmente no me funcionó bien.\n",
            "[1359.04s -> 1361.04s]  Entonces, yo creo que voy a corregir la tarea\n",
            "[1362.04s -> 1363.04s]  y lo voy a hacer directamente con ChatGPT\n",
            "[1364.04s -> 1367.04s]  ya con los textos que ya había sacado\n",
            "[1368.04s -> 1371.04s]  para poder buscar los key points.\n",
            "[1374.04s -> 1376.04s]  Bueno, tú lideraste un tema interesante\n",
            "[1376.92s -> 1377.92s]  que vamos a ver casi al final.\n",
            "[1378.92s -> 1380.92s]  No utilices ChatGPT.\n",
            "[1381.92s -> 1382.92s]  Te recomiendo que uses\n",
            "[1383.92s -> 1384.92s]  y se los va a liberar a todos.\n",
            "[1385.92s -> 1388.92s]  Es un modelo que salió hace como una semana, tal vez.\n",
            "[1389.92s -> 1392.92s]  El Gema 2, 2 billones.\n",
            "[1393.92s -> 1395.92s]  Esperen a ver si lo encuentro acá.\n",
            "[1396.92s -> 1397.92s]  2 billones de parámetros.\n",
            "[1398.92s -> 1399.92s]  Es un modelo gratuito.\n",
            "[1400.92s -> 1401.92s]  Es un modelo open source\n",
            "[1402.92s -> 1403.92s]  y que cabe en su celular.\n",
            "[1404.80s -> 1405.80s]  Interesante.\n",
            "[1406.80s -> 1407.80s]  Puedes hacer tu tarea con ese.\n",
            "[1408.80s -> 1409.80s]  Ese es un modelo generativo tipo ChatGPT\n",
            "[1410.80s -> 1411.80s]  pero no te toca pagar.\n",
            "[1412.80s -> 1413.80s]  Es libre. Es gratis.\n",
            "[1414.80s -> 1415.80s]  Inténtelo a ver cómo le va y me cuenta.\n",
            "[1416.80s -> 1417.80s]  Ok.\n",
            "[1418.80s -> 1419.80s]  Sí, porque lo que yo intenté aquí\n",
            "[1420.80s -> 1422.80s]  con los key points con modelo de Hugging Face.\n",
            "[1423.80s -> 1424.80s]  Hay demasiados.\n",
            "[1425.80s -> 1426.80s]  Este decía que era como el que no tenía que\n",
            "[1427.80s -> 1429.80s]  pues tenía más tokens para poder traer\n",
            "[1430.80s -> 1431.80s]  porque eran 8 mil, por lo menos de primer texto\n",
            "[1432.80s -> 1433.80s]  eran como 8 mil 900.\n",
            "[1434.80s -> 1435.80s]  Así pico de palabras.\n",
            "[1436.80s -> 1438.80s]  Entonces, claro, toca tokenizar el texto como tal.\n",
            "[1439.80s -> 1440.80s]  Entonces, por ese lado, grave.\n",
            "[1441.80s -> 1443.80s]  Y lo hice así, pero no me resultó bien.\n",
            "[1444.80s -> 1445.80s]  Entonces, creo que voy a hacer la corrección.\n",
            "[1446.80s -> 1447.80s]  ¿Cómo se llama, profe?\n",
            "[1448.80s -> 1449.80s]  Para ver si lo puedo hacer por ese lado.\n",
            "[1450.80s -> 1451.80s]  Lo dejé en el chat.\n",
            "[1452.80s -> 1453.80s]  Gema 2, 2 billones.\n",
            "[1454.80s -> 1455.80s]  Ese te puede correr en colab.\n",
            "[1456.80s -> 1457.80s]  Creo que incluso hay un Lama 3\n",
            "[1458.80s -> 1460.80s]  de 8 billones que te podría correr ahí también.\n",
            "[1461.80s -> 1462.80s]  Pero bueno, ahí lo dejé en el chat\n",
            "[1463.68s -> 1464.68s]  para todos y para todas\n",
            "[1465.68s -> 1467.68s]  que investiguen cómo poder usar ese modelo.\n",
            "[1468.68s -> 1469.68s]  Está Hugging Face.\n",
            "[1470.68s -> 1471.68s]  Obviamente en el curso vamos a ver\n",
            "[1472.68s -> 1474.68s]  ya que, bueno, el compromiso de para hoy era el PDA.\n",
            "[1475.68s -> 1476.68s]  Todavía no lo tengo listo por unas cosas\n",
            "[1477.68s -> 1478.68s]  que se leyeron esta semana\n",
            "[1479.68s -> 1480.68s]  de nuevos avances en Nia.\n",
            "[1481.68s -> 1483.68s]  Pero yo creo que, pues igual les compartí el clásico,\n",
            "[1484.68s -> 1485.68s]  pero los cambios sí.\n",
            "[1486.68s -> 1488.68s]  Si algo ahorita los hacemos con ChatGPT\n",
            "[1489.68s -> 1490.68s]  en la idea que tengo.\n",
            "[1491.56s -> 1492.56s]  Pero intenten usar este modelo\n",
            "[1493.56s -> 1494.56s]  que les acabo de mandar\n",
            "[1495.56s -> 1496.56s]  y me dicen qué resultados tienen,\n",
            "[1497.56s -> 1498.56s]  si lo lograron doblegar.\n",
            "[1499.56s -> 1500.56s]  ¿Doblegar saben qué significa?\n",
            "[1501.56s -> 1502.56s]  Que haga lo que usted quiere que haga.\n",
            "[1503.56s -> 1504.56s]  Pero me parece muy interesante tu solución\n",
            "[1505.56s -> 1506.56s]  porque es diferente de las demás.\n",
            "[1507.56s -> 1508.56s]  Te pegaste la pela de\n",
            "[1509.56s -> 1510.56s]  ir a Hugging Face, cargar el modelo,\n",
            "[1511.56s -> 1512.56s]  cargar el tokenizador,\n",
            "[1513.56s -> 1514.56s]  usar la inferencia.\n",
            "[1515.56s -> 1516.56s]  Eso es súper importante.\n",
            "[1517.56s -> 1518.56s]  Por una cuestión de costos.\n",
            "[1519.56s -> 1520.56s]  No es nada más que,\n",
            "[1521.56s -> 1522.56s]  no quiere decir que los que ya no usan\n",
            "[1523.56s -> 1524.56s]  ChatGPT está mal.\n",
            "[1525.56s -> 1526.56s]  De hecho, esa es la solución,\n",
            "[1527.56s -> 1528.56s]  digamos, más directa,\n",
            "[1529.56s -> 1530.56s]  más óptima, pero que cuesta.\n",
            "[1531.56s -> 1532.56s]  Entonces, espero que estas opciones\n",
            "[1533.56s -> 1534.56s]  están muy bien.\n",
            "[1535.56s -> 1536.56s]  Sí, dale, corrégelo y no hay problema.\n",
            "[1537.56s -> 1538.56s]  Me lo mandas otra vez o me dices\n",
            "[1539.56s -> 1540.56s]  en el mismo correo, profe,\n",
            "[1541.56s -> 1542.56s]  es la versión corregida.\n",
            "[1543.56s -> 1544.56s]  ¿Listo?\n",
            "[1545.56s -> 1546.56s]  Sí, lo que me falta es la última parte,\n",
            "[1547.56s -> 1548.56s]  los key points.\n",
            "[1549.56s -> 1550.56s]  Entonces voy a corregirlo y te lo envío de nuevo, profe.\n",
            "[1551.56s -> 1552.56s]  Lo hago al mismo momento.\n",
            "[1560.56s -> 1562.56s]  Entonces, de las imágenes tengo\n",
            "[1563.56s -> 1564.56s]  este para el de\n",
            "[1565.56s -> 1566.56s]  Procesamiento de Lenguaje Natural,\n",
            "[1567.56s -> 1568.56s]  lo hice con Dalí.\n",
            "[1569.56s -> 1570.56s]  Este para Computer Vision\n",
            "[1573.56s -> 1574.56s]  y este para\n",
            "[1576.56s -> 1577.56s]  para Deep Learning.\n",
            "[1578.56s -> 1579.56s]  Y por aquí tengo el de Series de Tiempo.\n",
            "[1581.56s -> 1582.56s]  Ok, listo.\n",
            "[1583.56s -> 1584.56s]  Públicalos y miramos a ver\n",
            "[1585.56s -> 1586.56s]  cómo va esa votación.\n",
            "[1587.56s -> 1588.56s]  Gracias.\n",
            "[1589.56s -> 1590.56s]  Listo, profe.\n",
            "[1591.56s -> 1592.56s]  Daniel Leonardo.\n",
            "[1593.56s -> 1594.56s]  Dale, Juan.\n",
            "[1595.56s -> 1596.56s]  Gracias.\n",
            "[1597.56s -> 1598.56s]  Buenos días para todos.\n",
            "[1599.56s -> 1600.56s]  Ya les comparto mi pantalla.\n",
            "[1607.56s -> 1608.56s]  Listo.\n",
            "[1609.56s -> 1610.56s]  Ahí lo están viendo, ¿sí?\n",
            "[1611.56s -> 1612.56s]  Ya se ve.\n",
            "[1614.56s -> 1615.56s]  Listo.\n",
            "[1616.56s -> 1617.56s]  Bueno, pues la solución al final, digamos,\n",
            "[1618.56s -> 1619.56s]  que también pues vamos a enfocar\n",
            "[1620.56s -> 1621.56s]  lo que ya los compañeros han presentado\n",
            "[1622.56s -> 1623.56s]  para el tema de\n",
            "[1624.56s -> 1625.56s]  extraer el audio de los videos.\n",
            "[1626.56s -> 1627.56s]  Aquí hice uso pues de\n",
            "[1628.56s -> 1629.56s]  de Audio Segment,\n",
            "[1630.56s -> 1631.56s]  que pues es un módulo de la librería PyTool,\n",
            "[1632.56s -> 1633.56s]  que pues va muy enfocado a todo el tema\n",
            "[1634.56s -> 1637.56s]  de manipulación de archivos tipo audio.\n",
            "[1638.56s -> 1639.56s]  Y también hice uso de la librería OZ,\n",
            "[1640.44s -> 1641.44s]  para pues poder hacer uso de la carpeta\n",
            "[1642.44s -> 1643.44s]  que el profe nos había compartido.\n",
            "[1644.44s -> 1645.44s]  Y pues así, pues poder leer\n",
            "[1646.44s -> 1647.44s]  como la carpeta y cada archivo\n",
            "[1648.44s -> 1649.44s]  que estaba contenido allí.\n",
            "[1650.44s -> 1651.44s]  Pues yo lo que hice básicamente\n",
            "[1652.44s -> 1653.44s]  fue como crear un acceso directo\n",
            "[1654.44s -> 1655.44s]  desde esa carpeta a mi drive\n",
            "[1656.44s -> 1657.44s]  y pues nada, a partir de la librería OZ\n",
            "[1658.44s -> 1659.44s]  acceder a la misma y listar\n",
            "[1660.44s -> 1661.44s]  pues todos los archivos con extensión .mp4\n",
            "[1662.44s -> 1663.44s]  que vienen siendo los videos.\n",
            "[1664.44s -> 1665.44s]  Y posteriormente, pues a partir\n",
            "[1666.44s -> 1667.44s]  de un loop para cada uno de esos videos,\n",
            "[1668.44s -> 1669.44s]  pues en un archivo,\n",
            "[1670.44s -> 1671.44s]  que ya estaban almacenados en la carpeta,\n",
            "[1672.44s -> 1673.44s]  aplicarle el tema de transformación\n",
            "[1674.44s -> 1675.44s]  del formato a cada uno de los videos,\n",
            "[1676.44s -> 1677.44s]  basándolo de un formato .mp4\n",
            "[1678.44s -> 1679.44s]  a un formato .mp3.\n",
            "[1680.44s -> 1681.44s]  Con eso pues ya teníamos\n",
            "[1682.44s -> 1683.44s]  simplemente la extracción del audio\n",
            "[1684.44s -> 1685.44s]  y finalmente pues lo exporté\n",
            "[1686.44s -> 1687.44s]  pues al directorio que ya había\n",
            "[1688.44s -> 1689.44s]  especificado en la parte superior\n",
            "[1690.44s -> 1691.44s]  de Audio Folder Path.\n",
            "[1692.44s -> 1693.44s]  Entonces pues con ese loop\n",
            "[1694.44s -> 1695.44s]  que ya tenía el formato .mp4\n",
            "[1696.44s -> 1697.44s]  y que ya tenía el formato .mp3\n",
            "[1698.32s -> 1699.32s]  entonces pues con ese loop\n",
            "[1700.32s -> 1701.32s]  un poquito sencillito\n",
            "[1702.32s -> 1703.32s]  pues se pudo generar todo el tema\n",
            "[1704.32s -> 1705.32s]  de la extracción de audio.\n",
            "[1706.32s -> 1707.32s]  Y posteriormente, una vez que pues\n",
            "[1708.32s -> 1709.32s]  ya hice toda la extracción del audio\n",
            "[1710.32s -> 1711.32s]  generé algo muy similar\n",
            "[1712.32s -> 1713.32s]  pero para pues todo el tema\n",
            "[1714.32s -> 1715.32s]  de pasarlo a texto.\n",
            "[1716.32s -> 1717.32s]  Entonces aquí pues hice uso\n",
            "[1718.32s -> 1719.32s]  del modelo base,\n",
            "[1720.32s -> 1721.32s]  pues porque es como el más\n",
            "[1722.32s -> 1723.32s]  quizás es pues más óptimo\n",
            "[1724.32s -> 1725.32s]  teniendo en cuenta pues la cantidad\n",
            "[1726.32s -> 1727.32s]  de parámetros que tiene\n",
            "[1728.32s -> 1729.32s]  no es tan pesado\n",
            "[1730.32s -> 1731.32s]  y pues también manejé un enfoque\n",
            "[1732.32s -> 1733.32s]  similar a lo que había hecho\n",
            "[1734.32s -> 1735.32s]  para todo el tema de pues\n",
            "[1736.32s -> 1737.32s]  extraer el audio.\n",
            "[1738.32s -> 1739.32s]  Entonces listé pues todos los\n",
            "[1740.32s -> 1741.32s]  archivos de audio en la carpeta\n",
            "[1742.32s -> 1743.32s]  que previamente había creado\n",
            "[1744.32s -> 1745.32s]  y le apliqué pues el respectivo\n",
            "[1746.32s -> 1747.32s]  modelo de Whisper\n",
            "[1748.32s -> 1749.32s]  especificándole pues que fueran\n",
            "[1750.32s -> 1751.32s]  el lenguaje en español\n",
            "[1752.32s -> 1753.32s]  y pues finalmente guardé\n",
            "[1754.32s -> 1755.32s]  cada uno de los archivos\n",
            "[1756.32s -> 1757.32s]  en un formato .txt.\n",
            "[1758.20s -> 1759.20s]  Y decidí generar pues un archivo\n",
            "[1760.20s -> 1761.20s]  .txt para cada uno pues\n",
            "[1762.20s -> 1763.20s]  porque mi idea era pues\n",
            "[1764.20s -> 1765.20s]  pasarle el .txt al .shgpt\n",
            "[1766.20s -> 1767.20s]  pues para generar\n",
            "[1768.20s -> 1769.20s]  el respectivo resumen.\n",
            "[1770.20s -> 1771.20s]  No quería como generar uno solo\n",
            "[1772.20s -> 1773.20s]  pues quizás como por temas de\n",
            "[1774.20s -> 1775.20s]  pesos, entonces por eso decidí\n",
            "[1776.20s -> 1777.20s]  manejarlo así, particionado.\n",
            "[1778.20s -> 1779.20s]  Entonces el output pues vienen\n",
            "[1780.20s -> 1781.20s]  siendo los tres archivos .txt\n",
            "[1782.20s -> 1783.20s]  que son los que tenemos acá\n",
            "[1784.20s -> 1785.20s]  en esta carpetica.\n",
            "[1786.20s -> 1787.20s]  Y por último lo que hice pues\n",
            "[1788.20s -> 1789.20s]  fue ponerle .txt a .shgpt\n",
            "[1790.20s -> 1791.20s]  con este prompt que tenemos acá.\n",
            "[1792.20s -> 1793.20s]  Entonces le comenté todo esto\n",
            "[1794.20s -> 1795.20s]  pues que dándole el contexto\n",
            "[1796.20s -> 1797.20s]  que fuera un experto con\n",
            "[1798.20s -> 1799.20s]  más de 10 años de experiencia\n",
            "[1800.20s -> 1801.20s]  y capacidad de sintetizar\n",
            "[1802.20s -> 1803.20s]  y pues también conocimiento\n",
            "[1804.20s -> 1805.20s]  de los conceptos.\n",
            "[1806.20s -> 1807.20s]  Y finalmente pues en cuanto\n",
            "[1808.20s -> 1809.20s]  a los puntos clave que me vino\n",
            "[1810.20s -> 1811.20s]  pues fueron en total seis\n",
            "[1812.20s -> 1813.20s]  donde está pues diferencia\n",
            "[1814.20s -> 1815.20s]  entre aprendizaje automático\n",
            "[1816.20s -> 1817.20s]  y aprendizaje profundo,\n",
            "[1818.08s -> 1819.08s]  aplicaciones del aprendizaje\n",
            "[1820.08s -> 1821.08s]  profundo, regularización\n",
            "[1822.08s -> 1823.08s]  y optimización en redes\n",
            "[1824.08s -> 1825.08s]  neuronales, la interpretabilidad\n",
            "[1826.08s -> 1827.08s]  de los modelos y por último\n",
            "[1828.08s -> 1829.08s]  las aplicaciones y avances\n",
            "[1830.08s -> 1831.08s]  recientes en el CAM.\n",
            "[1832.08s -> 1833.08s]  Ahí quizás fue como muy\n",
            "[1834.08s -> 1835.08s]  extenso en cada uno,\n",
            "[1836.08s -> 1837.08s]  pero pues digamos que en\n",
            "[1838.08s -> 1839.08s]  términos generales pues fue\n",
            "[1840.08s -> 1841.08s]  como un buen resumen.\n",
            "[1842.08s -> 1843.08s]  Y por último pues generé\n",
            "[1844.08s -> 1845.08s]  también pues un archivo\n",
            "[1846.08s -> 1847.08s]  main.py pues de aquí\n",
            "[1848.08s -> 1849.08s]  a la parte de las palabras\n",
            "[1850.08s -> 1851.08s]  que se genera como el\n",
            "[1852.08s -> 1853.08s]  pipeline completo pues de\n",
            "[1854.08s -> 1855.08s]  toda la parte de extracción\n",
            "[1856.08s -> 1857.08s]  de audio y posteriormente\n",
            "[1858.08s -> 1859.08s]  la extracción de pues la\n",
            "[1860.08s -> 1861.08s]  generación de los archivos\n",
            "[1862.08s -> 1863.08s]  txt.\n",
            "[1864.08s -> 1865.08s]  Súper bien, me encanta.\n",
            "[1866.08s -> 1867.08s]  Siento que cada vez va\n",
            "[1868.08s -> 1869.08s]  subiendo como el nivel\n",
            "[1870.08s -> 1871.08s]  sin desmeritar lo que ya\n",
            "[1872.08s -> 1873.08s]  se ha hecho.\n",
            "[1873.08s -> 1874.08s]  Miren muchachos, iba a\n",
            "[1875.08s -> 1876.08s]  poner una encuesta diferente\n",
            "[1876.96s -> 1877.96s]  y aquí no hay vergüenza\n",
            "[1878.96s -> 1879.96s]  de nada.\n",
            "[1879.96s -> 1880.96s]  Ah, pero me vas a mostrar\n",
            "[1881.96s -> 1882.96s]  los logos, no?\n",
            "[1882.96s -> 1883.96s]  Espérate.\n",
            "[1883.96s -> 1884.96s]  Y chicos, fíjense que\n",
            "[1885.96s -> 1886.96s]  lo importante de estas\n",
            "[1887.96s -> 1888.96s]  tareas no es quién lo hizo\n",
            "[1889.96s -> 1890.96s]  mejor, no.\n",
            "[1891.96s -> 1892.96s]  Lo importante es usted que\n",
            "[1893.96s -> 1894.96s]  está aprendiendo de lo otro.\n",
            "[1895.96s -> 1896.96s]  Entonces fíjense que era\n",
            "[1897.96s -> 1898.96s]  algo que iba a decir al final.\n",
            "[1899.96s -> 1900.96s]  Ah, todavía hay alguien.\n",
            "[1901.96s -> 1902.96s]  No, entonces más bien lo\n",
            "[1903.96s -> 1904.96s]  digo al final, lo digo al\n",
            "[1905.84s -> 1906.84s]  final, pero me gusta mucho\n",
            "[1907.84s -> 1908.84s]  tu idea, Juan, porque\n",
            "[1909.84s -> 1910.84s]  algo fundamental es que\n",
            "[1911.84s -> 1912.84s]  definiste los parámetros\n",
            "[1913.84s -> 1914.84s]  importantes de Whisper.\n",
            "[1915.84s -> 1916.84s]  Bueno, uno por lo menos\n",
            "[1917.84s -> 1918.84s]  vi el lenguaje, el idioma.\n",
            "[1919.84s -> 1920.84s]  Sí es cierto que Whisper\n",
            "[1921.84s -> 1922.84s]  detecta el idioma con un\n",
            "[1923.84s -> 1924.84s]  modelo por dentro, pero\n",
            "[1925.84s -> 1926.84s]  se puede equivocar.\n",
            "[1927.84s -> 1928.84s]  Entonces la forma de\n",
            "[1929.84s -> 1930.84s]  asegurarnos, de asegurarnos\n",
            "[1931.84s -> 1932.84s]  de que controlamos el modelo\n",
            "[1933.84s -> 1934.84s]  a la perfección es\n",
            "[1935.84s -> 1936.84s]  diciéndole cuál es el idioma\n",
            "[1937.84s -> 1938.84s]  y eso hace una serie\n",
            "[1939.84s -> 1940.84s]  de operaciones por dentro que\n",
            "[1941.84s -> 1942.84s]  nos permite asegurar que eso\n",
            "[1943.84s -> 1944.84s]  va a salir en español.\n",
            "[1945.84s -> 1946.84s]  Porque puede ser que en ese\n",
            "[1947.84s -> 1948.84s]  momento I want to speak in\n",
            "[1949.84s -> 1950.84s]  English and I don't care if\n",
            "[1951.84s -> 1952.84s]  you don't understand.\n",
            "[1953.84s -> 1954.84s]  Y entonces en esa transcripción\n",
            "[1955.84s -> 1956.84s]  puede ser que eso quede mal,\n",
            "[1957.84s -> 1958.84s]  ¿cierto?\n",
            "[1958.84s -> 1959.84s]  Entonces muy importante\n",
            "[1960.84s -> 1961.84s]  ese control de esos parámetros.\n",
            "[1962.84s -> 1963.84s]  Entonces súper, súper chévere\n",
            "[1964.72s -> 1965.72s]  que es un main PNG\n",
            "[1966.72s -> 1967.72s]  te permite reproducir\n",
            "[1968.72s -> 1969.72s]  el experimento.\n",
            "[1970.72s -> 1971.72s]  En Notebooks también se puede\n",
            "[1971.72s -> 1972.72s]  hacer y uno puede subir\n",
            "[1973.72s -> 1974.72s]  Notebooks a la nube, pero\n",
            "[1975.72s -> 1976.72s]  en cuestiones de deployment\n",
            "[1977.72s -> 1978.72s]  obviamente el hecho de que\n",
            "[1979.72s -> 1980.72s]  lo tengas ya en un punto PNG\n",
            "[1981.72s -> 1982.72s]  va a hacer que la compilación\n",
            "[1983.72s -> 1984.72s]  por consola sea muchísimo más\n",
            "[1985.72s -> 1986.72s]  rápido.\n",
            "[1986.72s -> 1987.72s]  Entonces te felicito,\n",
            "[1988.72s -> 1989.72s]  está súper completo.\n",
            "[1990.72s -> 1991.72s]  Nos dejaste compartir,\n",
            "[1992.72s -> 1993.72s]  entonces asumo que no quieres\n",
            "[1994.60s -> 1995.60s]  compartir más.\n",
            "[1996.60s -> 1997.60s]  Adelante.\n",
            "[1998.60s -> 1999.60s]  Profe, sí los tengo en mi\n",
            "[2000.60s -> 2001.60s]  computador personal porque\n",
            "[2002.60s -> 2003.60s]  estoy desde otro, pero ya los\n",
            "[2004.60s -> 2005.60s]  comparto por el grupo.\n",
            "[2006.60s -> 2007.60s]  Dale, listo, sin lío,\n",
            "[2008.60s -> 2009.60s]  no hay problema.\n",
            "[2010.60s -> 2011.60s]  Vale, vale, gracias.\n",
            "[2012.60s -> 2013.60s]  Buenos días para todos.\n",
            "[2014.60s -> 2015.60s]  Bueno, fue mi trabajo, no\n",
            "[2016.60s -> 2017.60s]  estaba con buen deseo, pero\n",
            "[2018.60s -> 2019.60s]  no, yo hice básicamente,\n",
            "[2020.60s -> 2021.60s]  también utilice Python para\n",
            "[2022.60s -> 2023.60s]  hacer la conversión, o sea,\n",
            "[2024.48s -> 2025.48s]  yo hice la descripción que\n",
            "[2026.48s -> 2027.48s]  con un archivo plano para\n",
            "[2028.48s -> 2029.48s]  cada uno de los tres y lo que\n",
            "[2030.48s -> 2031.48s]  yo hice fue al final que me\n",
            "[2032.48s -> 2033.48s]  dejara un TXT unificado de\n",
            "[2034.48s -> 2035.48s]  cada uno de los tres audios.\n",
            "[2036.48s -> 2037.48s]  Y pues mis puntos que estaba\n",
            "[2038.48s -> 2039.48s]  mirando en comparación con\n",
            "[2040.48s -> 2041.48s]  todos mis compañeros, ya mis\n",
            "[2042.48s -> 2043.48s]  puntos fueron más de\n",
            "[2044.48s -> 2045.48s]  conclusión y de tareas.\n",
            "[2046.48s -> 2047.48s]  Entonces como que teníamos que\n",
            "[2048.48s -> 2049.48s]  generar logos, que teníamos\n",
            "[2050.48s -> 2051.48s]  que utilizar aplicaciones,\n",
            "[2052.48s -> 2053.48s]  crear la comunidad en los\n",
            "[2054.36s -> 2055.36s]  que yo estaba, las\n",
            "[2056.36s -> 2057.36s]  grabaciones de las clases, la\n",
            "[2058.36s -> 2059.36s]  preparación para las próximas\n",
            "[2060.36s -> 2061.36s]  clases, las evaluaciones y\n",
            "[2062.36s -> 2063.36s]  conclusiones fueron mis puntos\n",
            "[2064.36s -> 2065.36s]  claves en comparación con los\n",
            "[2066.36s -> 2067.36s]  compañeros que vi.\n",
            "[2068.36s -> 2069.36s]  Y para los logos yo rápidamente\n",
            "[2070.36s -> 2071.36s]  utilice Microsoft Bean y pues\n",
            "[2072.36s -> 2073.36s]  los logos la verdad no, mis\n",
            "[2074.36s -> 2075.36s]  descripciones como que eran muy\n",
            "[2076.36s -> 2077.36s]  simples, me daba cuenta cada vez\n",
            "[2078.36s -> 2079.36s]  que me gustaba, pero cada vez\n",
            "[2080.36s -> 2081.36s]  que me iba gustando más ya me\n",
            "[2082.36s -> 2083.36s]  daba cuenta que ya no tenía\n",
            "[2084.36s -> 2085.36s]  más.\n",
            "[2086.36s -> 2087.36s]  Entonces estos fueron los que\n",
            "[2088.36s -> 2089.36s]  conseguí.\n",
            "[2090.36s -> 2091.36s]  Ya los compartí por el chat.\n",
            "[2092.36s -> 2093.36s]  Perfecto.\n",
            "[2094.36s -> 2095.36s]  Bueno, pues yo puedo darles\n",
            "[2096.36s -> 2097.36s]  acceso a mi cuenta.\n",
            "[2098.36s -> 2099.36s]  El problema es que si todos la\n",
            "[2100.36s -> 2101.36s]  usan al tiempo me van a quebrar,\n",
            "[2102.36s -> 2103.36s]  o sea me van a quitar el\n",
            "[2104.36s -> 2105.36s]  acceso.\n",
            "[2106.36s -> 2107.36s]  Pero si alguien, alguno de\n",
            "[2108.36s -> 2109.36s]  ustedes necesita usar GTP, 4.0\n",
            "[2110.36s -> 2111.36s]  o esas cosas, no tengo lío en\n",
            "[2112.36s -> 2113.36s]  darle las credenciales.\n",
            "[2114.36s -> 2115.36s]  Si alguien lo está usando, una\n",
            "[2116.36s -> 2117.36s]  gente en la fiscalía, entonces\n",
            "[2118.36s -> 2119.36s]  no tengo lío.\n",
            "[2120.36s -> 2121.36s]  Y me parece chévere, varias\n",
            "[2122.36s -> 2123.36s]  personas lo pueden usar al tiempo\n",
            "[2124.36s -> 2125.36s]  y pues Dalí en ese ámbito de\n",
            "[2126.36s -> 2127.36s]  esos 20 dólares que uno paga al\n",
            "[2128.36s -> 2129.36s]  mes, creo que vale la pena.\n",
            "[2130.36s -> 2131.36s]  No les estoy vendiendo el\n",
            "[2132.36s -> 2133.36s]  producto, les estoy diciendo vale\n",
            "[2134.36s -> 2135.36s]  la pena porque uno puede usar en\n",
            "[2136.36s -> 2137.36s]  eso audio, video, pero en video\n",
            "[2138.36s -> 2139.36s]  todavía no, puede usar audio,\n",
            "[2140.36s -> 2141.36s]  puede usar Dalí, puede usar\n",
            "[2142.36s -> 2143.36s]  GPT's personalizados.\n",
            "[2144.24s -> 2145.24s]  80 mil pesos al mes, creo que\n",
            "[2146.24s -> 2147.24s]  ahora está más barato.\n",
            "[2148.24s -> 2149.24s]  Pero si tienen alguna, o sea,\n",
            "[2150.24s -> 2151.24s]  claro, si necesitan yo les puedo\n",
            "[2152.24s -> 2153.24s]  dar el acceso para que lo hagan.\n",
            "[2154.24s -> 2155.24s]  Si veo que pues me bloquean por\n",
            "[2156.24s -> 2157.24s]  alguna cuestión, pues les digo\n",
            "[2158.24s -> 2159.24s]  venga, lo voy a usar en mi\n",
            "[2160.24s -> 2161.24s]  trabajo, pero no se preocupen por\n",
            "[2162.24s -> 2163.24s]  eso.\n",
            "[2163.24s -> 2164.24s]  Si no tienen créditos, escribanme\n",
            "[2165.24s -> 2166.24s]  profes, que no tengo créditos, no\n",
            "[2167.24s -> 2168.24s]  sé cómo hacer listo mi toma,\n",
            "[2169.24s -> 2170.24s]  porque esa es la idea.\n",
            "[2171.24s -> 2172.24s]  Me gustaría pagarles a todos en\n",
            "[2173.12s -> 2174.12s]  el momento, a todos los profesores,\n",
            "[2175.12s -> 2176.12s]  entonces no sé si ese rubro vaya\n",
            "[2177.12s -> 2178.12s]  a estar, entonces pues les ofrezco\n",
            "[2179.12s -> 2180.12s]  más bien eso, como miren, este es\n",
            "[2181.12s -> 2182.12s]  y hágale y siga haciendo cositas.\n",
            "[2183.12s -> 2184.12s]  ¿Listo?\n",
            "[2185.12s -> 2186.12s]  Súper chévere, Marilyn, me gusta\n",
            "[2187.12s -> 2188.12s]  sobre todo la estructura que\n",
            "[2189.12s -> 2190.12s]  pusiste el logo de la universidad,\n",
            "[2191.12s -> 2192.12s]  creo que fuiste de las unicas.\n",
            "[2193.12s -> 2194.12s]  Es muy importante, entonces\n",
            "[2195.12s -> 2196.12s]  fíjense.\n",
            "[2197.12s -> 2198.12s]  Entonces les voy a dar la\n",
            "[2199.12s -> 2200.12s]  retroalimentación en estos cuatro\n",
            "[2201.12s -> 2202.12s]  minuticos, porque pues tengo que\n",
            "[2203.12s -> 2204.12s]  decirles, es moda, creo que yo fui\n",
            "[2205.12s -> 2206.12s]  los primeros que introduje los\n",
            "[2207.12s -> 2208.12s]  notebooks en la universidad central,\n",
            "[2209.12s -> 2210.12s]  me parece súper chévere que esa\n",
            "[2211.12s -> 2212.12s]  cultura siga.\n",
            "[2213.12s -> 2214.12s]  Hay que diferenciar, ¿no?\n",
            "[2215.12s -> 2216.12s]  Entre notebook y digamos en la\n",
            "[2217.12s -> 2218.12s]  tarea yo pedía un pipeline, con\n",
            "[2219.12s -> 2220.12s]  eso me refería un producto\n",
            "[2221.12s -> 2222.12s]  profesional, si lo podía lograr.\n",
            "[2223.12s -> 2224.12s]  Los notebooks no están mal, están\n",
            "[2225.12s -> 2226.12s]  chéveres, es la forma de\n",
            "[2227.12s -> 2228.12s]  prototipar que todos aprendemos\n",
            "[2229.12s -> 2230.12s]  así, ¿cierto?\n",
            "[2231.12s -> 2232.12s]  Los notebooks en colap y la goma,\n",
            "[2233.12s -> 2234.12s]  pero a nivel profesional, si\n",
            "[2235.12s -> 2236.12s]  usted va a ser desarrollador,\n",
            "[2237.12s -> 2238.12s]  aquí puede haber personas que\n",
            "[2239.12s -> 2240.12s]  van a ser tomadores de\n",
            "[2241.12s -> 2242.12s]  decisiones o personas que se van\n",
            "[2243.12s -> 2244.12s]  a encargar del desarrollo, eso\n",
            "[2245.12s -> 2246.12s]  no tiene ningún problema, ¿cierto?\n",
            "[2247.12s -> 2248.12s]  En MIT incluso en los cursos que\n",
            "[2249.12s -> 2250.12s]  certifican oficialmente, por lo\n",
            "[2251.12s -> 2252.12s]  menos uno que yo hice, era,\n",
            "[2253.12s -> 2254.12s]  usted tiene dos opciones de hacer\n",
            "[2255.12s -> 2256.12s]  tarea, puede hacerlo a nivel de\n",
            "[2257.12s -> 2258.12s]  toma de decisiones o puede\n",
            "[2259.12s -> 2260.12s]  hacerlo full code.\n",
            "[2261.12s -> 2262.12s]  Yo no les puse restricción en\n",
            "[2263.12s -> 2264.12s]  los notebooks, porque yo creo\n",
            "[2265.12s -> 2266.12s]  que es más código, explican\n",
            "[2267.12s -> 2268.12s]  también, pero se van mucho al\n",
            "[2269.12s -> 2270.12s]  tecnicismo, eso es súper\n",
            "[2271.12s -> 2272.12s]  importante.\n",
            "[2273.12s -> 2274.12s]  Entonces notebooks bien, algo\n",
            "[2275.12s -> 2276.12s]  que no vi, que sería interesante\n",
            "[2277.12s -> 2278.12s]  que lo revisáramos de una vez,\n",
            "[2279.12s -> 2280.12s]  sobre todo aprovechando que la\n",
            "[2281.12s -> 2282.12s]  guía del DANE va a salir muy\n",
            "[2283.12s -> 2284.12s]  pronto, la guía del ciclo de\n",
            "[2285.12s -> 2286.12s]  vida de modelos de ciencia de\n",
            "[2287.12s -> 2288.12s]  datos, no sé si les ha comentado\n",
            "[2289.12s -> 2290.12s]  por ahí, desafortunadamente va a\n",
            "[2291.00s -> 2292.00s]  estar en el país, pero pues,\n",
            "[2293.00s -> 2294.00s]  la verdad yo aporte, no es bien\n",
            "[2295.00s -> 2296.00s]  desde mi criterio experto, pero\n",
            "[2297.00s -> 2298.00s]  no he escrito una sola línea de\n",
            "[2299.00s -> 2300.00s]  ese documento hasta el momento,\n",
            "[2301.00s -> 2302.00s]  entonces pues no, pues digo que\n",
            "[2303.00s -> 2304.00s]  es desafortunado, pero algo que\n",
            "[2305.00s -> 2306.00s]  es muy importante tener en\n",
            "[2307.00s -> 2308.00s]  cuenta que se dijo y se propuso\n",
            "[2309.00s -> 2310.00s]  es la estandarización de los\n",
            "[2311.00s -> 2312.00s]  proyectos que ya van a estar muy\n",
            "[2313.00s -> 2314.00s]  cerca a la producción.\n",
            "[2315.00s -> 2316.00s]  Entonces fíjense algo muy\n",
            "[2317.00s -> 2318.00s]  importante, esto es algo que me\n",
            "[2318.88s -> 2319.88s]  ustedes deberían adoptar, los\n",
            "[2320.88s -> 2321.88s]  invito, si no lo, ay perdón, los\n",
            "[2322.88s -> 2323.88s]  invito si no lo, si no lo han\n",
            "[2324.88s -> 2325.88s]  hecho, que lo piensen en esta\n",
            "[2326.88s -> 2327.88s]  tarea, no en la siguiente sí,\n",
            "[2328.88s -> 2329.88s]  listo.\n",
            "[2330.88s -> 2331.88s]  Recuerden algo muy importante,\n",
            "[2332.88s -> 2333.88s]  esto no me lo estoy inventando\n",
            "[2334.88s -> 2335.88s]  yo, esto es algo que debe estar\n",
            "[2336.88s -> 2337.88s]  en su cabecita como\n",
            "[2338.88s -> 2339.88s]  estructuradores de proyectos,\n",
            "[2340.88s -> 2341.88s]  listo.\n",
            "[2342.88s -> 2343.88s]  Entonces, vamos a empezar con\n",
            "[2344.88s -> 2345.88s]  la estrategia de la estandarización\n",
            "[2346.76s -> 2347.76s]  de los proyectos, listo.\n",
            "[2348.76s -> 2349.76s]  Entonces, si, ¿tenes alguna\n",
            "[2350.76s -> 2351.76s]  pregunta?\n",
            "[2352.76s -> 2353.76s]  Roger creo que se le posó la\n",
            "[2354.76s -> 2355.76s]  pantalla, no, pensé que era yo\n",
            "[2356.76s -> 2357.76s]  pero no, no sé si está\n",
            "[2358.76s -> 2359.76s]  interactuando y.\n",
            "[2360.76s -> 2361.76s]  Ya, es que este, sí, sí, es que\n",
            "[2362.76s -> 2363.76s]  este, este Google, ¿ya están\n",
            "[2364.76s -> 2365.76s]  viendo? Sí.\n",
            "[2366.76s -> 2367.76s]  No.\n",
            "[2368.76s -> 2369.76s]  Sí, pero.\n",
            "[2370.76s -> 2371.76s]  ¿Listo? Me avisan.\n",
            "[2372.64s -> 2373.64s]  Sí, el escrito y el blog,\n",
            "[2374.64s -> 2375.64s]  sí.\n",
            "[2376.64s -> 2377.64s]  Sí, sí, sí, sí, esa es la idea.\n",
            "[2378.64s -> 2379.64s]  Bueno, para que se vea más,\n",
            "[2380.64s -> 2381.64s]  listo.\n",
            "[2382.64s -> 2383.64s]  Entonces, a lo que me refiero es\n",
            "[2384.64s -> 2385.64s]  esta estructura, el documento es\n",
            "[2386.64s -> 2387.64s]  que no se los puedo mostrar, pero\n",
            "[2388.64s -> 2389.64s]  en ese documento va a estar esto\n",
            "[2390.64s -> 2391.64s]  que les voy a explicar.\n",
            "[2392.64s -> 2393.64s]  Que tienen que tener una\n",
            "[2394.64s -> 2395.64s]  estructura en su, en su, en su\n",
            "[2396.64s -> 2397.64s]  proyecto, siempre.\n",
            "[2398.64s -> 2399.64s]  Estoy hablando del proyecto de la\n",
            "[2400.64s -> 2401.64s]  propuesta que ustedes me van a\n",
            "[2402.52s -> 2403.52s]  pedir, ¿no?\n",
            "[2404.52s -> 2405.52s]  De los videos.\n",
            "[2406.52s -> 2407.52s]  No, ahorita miramos el PDA.\n",
            "[2408.52s -> 2409.52s]  Entonces, es muy importante que\n",
            "[2410.52s -> 2411.52s]  usted siga esta estructura.\n",
            "[2412.52s -> 2413.52s]  Ese no me lo inventé yo, como\n",
            "[2414.52s -> 2415.52s]  les digo, eso es algo que se\n",
            "[2416.52s -> 2417.52s]  sigue.\n",
            "[2417.52s -> 2418.52s]  Yo de pronto me inventé, a que\n",
            "[2419.52s -> 2420.52s]  quiero poner documentos porque\n",
            "[2421.52s -> 2422.52s]  soy gomelo para tener las\n",
            "[2423.52s -> 2424.52s]  referencias a la mano, sí, docs.\n",
            "[2425.52s -> 2426.52s]  Pero algo que está sí o sí en\n",
            "[2427.52s -> 2428.52s]  los proyectos es una carpeta\n",
            "[2429.52s -> 2430.52s]  input, ¿cierto? Entonces, ¿qué\n",
            "[2431.40s -> 2432.40s]  es?\n",
            "[2432.40s -> 2433.40s]  Si usan APIs, pues no, ¿cierto?\n",
            "[2434.40s -> 2435.40s]  Si de pronto el resultado de la\n",
            "[2436.40s -> 2437.40s]  actividad es generar un modelo,\n",
            "[2438.40s -> 2439.40s]  pues sí, ¿cierto?\n",
            "[2440.40s -> 2441.40s]  Entonces, este, este modelo puede\n",
            "[2442.40s -> 2443.40s]  que sí, puede que no.\n",
            "[2444.40s -> 2445.40s]  Notebooks, muy importante para\n",
            "[2446.40s -> 2447.40s]  prototipar, para hacer pruebas de\n",
            "[2448.40s -> 2449.40s]  concepto.\n",
            "[2449.40s -> 2450.40s]  Está súper bien, porque lo hayan\n",
            "[2451.40s -> 2452.40s]  hecho ahí no, no está mal, es\n",
            "[2453.40s -> 2454.40s]  decir, si yo hago notebooks todo\n",
            "[2455.40s -> 2456.40s]  el tiempo.\n",
            "[2457.40s -> 2458.40s]  Una carpeta output, donde están\n",
            "[2459.40s -> 2460.40s]  los resultados.\n",
            "[2461.28s -> 2462.28s]  Yo no sé si fue, que lo hizo, no\n",
            "[2463.28s -> 2464.28s]  sé si fue Leonardo, fue Juan,\n",
            "[2465.28s -> 2466.28s]  pero donde ponían el output los\n",
            "[2467.28s -> 2468.28s]  de XT, de respuesta.\n",
            "[2470.28s -> 2471.28s]  Entonces, importantísimo\n",
            "[2472.28s -> 2473.28s]  separarlo.\n",
            "[2474.28s -> 2475.28s]  ¿Por qué?\n",
            "[2476.28s -> 2477.28s]  Porque la carpeta más importante\n",
            "[2478.28s -> 2479.28s]  de todas es la carpeta source,\n",
            "[2480.28s -> 2481.28s]  SRC, que es donde uno consigna\n",
            "[2482.28s -> 2483.28s]  el código fuente.\n",
            "[2484.28s -> 2485.28s]  Entonces, por eso decía la\n",
            "[2486.28s -> 2487.28s]  pregunta en el grupo, ¿qué tan\n",
            "[2488.28s -> 2489.28s]  relacionado está con programación\n",
            "[2490.16s -> 2491.16s]  de la idea de su proyecto?\n",
            "[2492.16s -> 2493.16s]  Ojalá lleguemos ahí, es que\n",
            "[2494.16s -> 2495.16s]  usted pueda hacer un producto que\n",
            "[2496.16s -> 2497.16s]  pueda venderlo, ¿sí?\n",
            "[2498.16s -> 2499.16s]  O sea, que ustedes se vuelvan\n",
            "[2500.16s -> 2501.16s]  empresarios.\n",
            "[2501.16s -> 2502.16s]  Entonces, lo que están haciendo\n",
            "[2503.16s -> 2504.16s]  ahorita, que nos dedicamos a una\n",
            "[2505.16s -> 2506.16s]  obra, básicamente, que nos va a\n",
            "[2507.16s -> 2508.16s]  tocar tomar diez minúticos ahorita\n",
            "[2509.16s -> 2510.16s]  a las ocho.\n",
            "[2510.16s -> 2511.16s]  Si ustedes me están vendiendo una\n",
            "[2512.16s -> 2513.16s]  idea y yo decidí comprar o hacerles\n",
            "[2514.16s -> 2515.16s]  algunas anotaciones.\n",
            "[2516.16s -> 2517.16s]  La idea es que ustedes vendan una\n",
            "[2518.16s -> 2519.16s]  idea el día de su proyecto final.\n",
            "[2520.16s -> 2521.16s]  Y usted se va a volver loco.\n",
            "[2522.16s -> 2523.16s]  O loca.\n",
            "[2524.16s -> 2525.16s]  En el sentido que cada cosita\n",
            "[2526.16s -> 2527.16s]  nueva que yo le pida, la va a\n",
            "[2528.16s -> 2529.16s]  tocar volver a estructurar.\n",
            "[2530.16s -> 2531.16s]  Por ejemplo, si yo le digo en un\n",
            "[2532.16s -> 2533.16s]  notebook, yo le digo, no, póngame\n",
            "[2534.16s -> 2535.16s]  un proceso intermedio de\n",
            "[2536.16s -> 2537.16s]  transformación de datos.\n",
            "[2538.16s -> 2539.16s]  Y usted ya tiene sus celditas así\n",
            "[2540.16s -> 2541.16s]  todas bonitas, uf, se vuelve\n",
            "[2542.16s -> 2543.16s]  añico, ¿cierto?\n",
            "[2544.16s -> 2545.16s]  Entonces, es muy importante ahorita,\n",
            "[2546.16s -> 2547.16s]  si no sabe, no importa, podemos\n",
            "[2548.16s -> 2549.16s]  repasarlo, la programación orientada,\n",
            "[2550.16s -> 2551.16s]  la programación de proyectos\n",
            "[2552.16s -> 2553.16s]  digitales también.\n",
            "[2554.16s -> 2555.16s]  Importante el source porque ahí\n",
            "[2556.16s -> 2557.16s]  podemos consignar todo el código\n",
            "[2558.16s -> 2559.16s]  fuente, pero de manera modular\n",
            "[2560.16s -> 2561.16s]  para que pueda responder a ciertas\n",
            "[2562.16s -> 2563.16s]  necesidades.\n",
            "[2563.16s -> 2564.16s]  Y pues voy a terminar con un\n",
            "[2565.16s -> 2566.16s]  producto que se los voy a mostrar.\n",
            "[2567.16s -> 2568.16s]  Fue algo que presentamos ayer a la\n",
            "[2569.16s -> 2570.16s]  alcaldía de Tunja.\n",
            "[2571.16s -> 2572.16s]  Está en pañales, fue un prototipo,\n",
            "[2573.16s -> 2574.16s]  fase es menos diez, pero es algo\n",
            "[2575.16s -> 2576.16s]  que yo creo que ustedes ya, con lo\n",
            "[2577.16s -> 2578.16s]  que me mostraron hoy, podrían estar\n",
            "[2579.04s -> 2580.04s]  a construir, ¿listo?\n",
            "[2581.04s -> 2582.04s]  Entonces, muy importante, ya les\n",
            "[2583.04s -> 2584.04s]  muestro, muy importante esas\n",
            "[2585.04s -> 2586.04s]  carpetas, input, output, notebooks,\n",
            "[2587.04s -> 2588.04s]  source, donde tiene todo su código\n",
            "[2589.04s -> 2590.04s]  fuente.\n",
            "[2591.04s -> 2592.04s]  Si no sabe cómo hacerlo, pues\n",
            "[2593.04s -> 2594.04s]  fácil, pregúntele al CGPT,\n",
            "[2595.04s -> 2596.04s]  pregúntele al profe, eso acá no nos\n",
            "[2597.04s -> 2598.04s]  varamos, pero sí muy importante que\n",
            "[2599.04s -> 2600.04s]  esta estructura quede.\n",
            "[2601.04s -> 2602.04s]  ¿Por qué?\n",
            "[2603.04s -> 2604.04s]  Porque es importante, porque es que,\n",
            "[2605.04s -> 2606.04s]  por ejemplo, cuando uno tiene esta\n",
            "[2607.04s -> 2608.04s]  estructura, uno siempre puede\n",
            "[2609.04s -> 2610.04s]  hacer un proyecto a caminos\n",
            "[2611.04s -> 2612.04s]  relativos, es decir, si yo tengo\n",
            "[2613.04s -> 2614.04s]  este cuaderno acá y yo quiero\n",
            "[2615.04s -> 2616.04s]  referirme a datos que están en el\n",
            "[2617.04s -> 2618.04s]  input, no tengo que quemar las\n",
            "[2619.04s -> 2620.04s]  rutas como C, dos puntos,\n",
            "[2621.04s -> 2622.04s]  Jaimito Duque, dos puntos,\n",
            "[2623.04s -> 2624.04s]  Universidad Central 2024, no.\n",
            "[2625.04s -> 2626.04s]  Yo puedo desde acá decir, si yo\n",
            "[2627.04s -> 2628.04s]  pongo punto, punto, slash, me\n",
            "[2629.04s -> 2630.04s]  devuelve una carpeta, punto,\n",
            "[2631.04s -> 2632.04s]  punto, slash, me devuelve una\n",
            "[2633.04s -> 2634.04s]  carpeta, estoy en el input y\n",
            "[2635.04s -> 2636.04s]  entonces puedo comenzar a hacer un\n",
            "[2637.04s -> 2638.04s]  proyecto autocontenido, de tal\n",
            "[2638.92s -> 2639.92s]  manera que nadie más es capaz de\n",
            "[2640.92s -> 2641.92s]  reproducir los resultados en su\n",
            "[2642.92s -> 2643.92s]  computador, eso.\n",
            "[2644.92s -> 2645.92s]  Por eso que les digo que es muy\n",
            "[2646.92s -> 2647.92s]  importante, entonces, los que\n",
            "[2648.92s -> 2649.92s]  hicieron notebook, está súper\n",
            "[2650.92s -> 2651.92s]  bien, no les va a bajar nota,\n",
            "[2652.92s -> 2653.92s]  pero váyanse acostumbrando un\n",
            "[2654.92s -> 2655.92s]  poquito a este tipo de\n",
            "[2656.92s -> 2657.92s]  organización, ¿cierto?\n",
            "[2658.92s -> 2659.92s]  Entonces, algunos estarán\n",
            "[2660.92s -> 2661.92s]  pensando, uy, hoy pucha, me faltó\n",
            "[2662.92s -> 2663.92s]  más de la mitad, seguramente,\n",
            "[2664.92s -> 2665.92s]  pero pues la idea es que a medida\n",
            "[2666.92s -> 2667.92s]  que vayamos avanzando en las\n",
            "[2668.80s -> 2669.80s]  cosas, en la guía nacional del\n",
            "[2670.80s -> 2671.80s]  DANE, del ciclo de modelos de\n",
            "[2672.80s -> 2673.80s]  datos, ¿sí?\n",
            "[2674.80s -> 2675.80s]  Muy importante.\n",
            "[2676.80s -> 2677.80s]  Entonces, también los\n",
            "[2678.80s -> 2679.80s]  requirements, de tal manera que\n",
            "[2680.80s -> 2681.80s]  si el profe quiere reproducir su\n",
            "[2682.80s -> 2683.80s]  tarea, a ver si está bien, pues\n",
            "[2684.80s -> 2685.80s]  pueda instalar los paquetes que\n",
            "[2686.80s -> 2687.80s]  usted utilizó, sin tener que\n",
            "[2688.80s -> 2689.80s]  buscarlos, instalarlos solo, y\n",
            "[2690.80s -> 2691.80s]  entonces, que ahí estén\n",
            "[2692.80s -> 2693.80s]  consignadas todas las librerías\n",
            "[2694.80s -> 2695.80s]  que se le requieran usar.\n",
            "[2696.80s -> 2697.80s]  Entonces, ¿cómo así que es un\n",
            "[2698.80s -> 2699.80s]  proceso de redes neuronales, de\n",
            "[2700.80s -> 2701.80s]  Deep Learning?\n",
            "[2702.80s -> 2703.80s]  Bueno, eso es básicamente, les\n",
            "[2704.80s -> 2705.80s]  voy a mostrar, yo creo que vamos\n",
            "[2706.80s -> 2707.80s]  a tomar 8 y 10, y de 8 y 10, 8\n",
            "[2708.80s -> 2709.80s]  y 20, para poderles mostrar las\n",
            "[2710.80s -> 2711.80s]  dos cosas.\n",
            "[2711.80s -> 2712.80s]  Entonces, primero la solución de\n",
            "[2713.80s -> 2714.80s]  la tarea.\n",
            "[2715.80s -> 2716.80s]  Entonces, había muchas formas de\n",
            "[2717.80s -> 2718.80s]  solucionar la tarea, como se dan\n",
            "[2719.80s -> 2720.80s]  cuenta, y qué pena mostrar las\n",
            "[2721.80s -> 2722.80s]  cosas del trabajo, hay muchas\n",
            "[2723.80s -> 2724.80s]  formas de solucionar la tarea.\n",
            "[2725.80s -> 2726.80s]  Una en particular, es, por\n",
            "[2727.68s -> 2728.68s]  ejemplo, la de una de las\n",
            "[2729.68s -> 2730.68s]  chicas, que era, no, pues, yo\n",
            "[2731.68s -> 2732.68s]  tengo solo 3 videos, pues, qué\n",
            "[2733.68s -> 2734.68s]  más, pues, voy a hacer una\n",
            "[2735.68s -> 2736.68s]  aplicación, en este caso yo hice\n",
            "[2737.68s -> 2738.68s]  una aplicación, y yo en esa\n",
            "[2739.68s -> 2740.68s]  aplicación puedo colocar, por\n",
            "[2741.68s -> 2742.68s]  ejemplo, un video, ¿no?\n",
            "[2743.68s -> 2744.68s]  Este creo que era uno de los\n",
            "[2745.68s -> 2746.68s]  videos, ah, sí, este es la clase\n",
            "[2747.68s -> 2748.68s]  anterior, uno de las horas.\n",
            "[2749.68s -> 2750.68s]  Y yo acá le puedo decir,\n",
            "[2751.68s -> 2752.68s]  Detach Audio, elimino esto, y\n",
            "[2753.68s -> 2754.68s]  simplemente, ah, aquí, aquí,\n",
            "[2755.56s -> 2756.56s]  elimino esto, y simplemente,\n",
            "[2757.56s -> 2758.56s]  ah, otra vez, Detach Audio,\n",
            "[2759.56s -> 2760.56s]  claro, Rof, no tengo Mac, sí,\n",
            "[2761.56s -> 2762.56s]  es cierto, ¿no?\n",
            "[2763.56s -> 2764.56s]  Entonces, cada uno, fíjense,\n",
            "[2765.56s -> 2766.56s]  usa los recursos que tienen\n",
            "[2767.56s -> 2768.56s]  disponibles para analizar un\n",
            "[2769.56s -> 2770.56s]  problema, siempre.\n",
            "[2771.56s -> 2772.56s]  ¿Listo?\n",
            "[2773.56s -> 2774.56s]  Entonces, esta es una de las\n",
            "[2775.56s -> 2776.56s]  soluciones, no es la solución\n",
            "[2777.56s -> 2778.56s]  modular que muchos mostraron, si\n",
            "[2779.56s -> 2780.56s]  hay librerías, ya como saben que\n",
            "[2781.56s -> 2782.56s]  las tienen, entonces, yo entre\n",
            "[2783.56s -> 2784.56s]  hoy y mañana voy a subir una\n",
            "[2785.44s -> 2786.44s]  de esas, pero esta es una\n",
            "[2787.44s -> 2788.44s]  solución creativa, entonces, uno\n",
            "[2789.44s -> 2790.44s]  dice como, bueno, pues, solo\n",
            "[2791.44s -> 2792.44s]  tengo tres videos, no importa,\n",
            "[2793.44s -> 2794.44s]  ¿qué me demoro más?\n",
            "[2795.44s -> 2796.44s]  Escribiendo el código, buscando\n",
            "[2797.44s -> 2798.44s]  el internet, o simplemente\n",
            "[2799.44s -> 2800.44s]  diciendo, bueno, pues, hagamos\n",
            "[2801.44s -> 2802.44s]  un share, acá simplemente le\n",
            "[2803.44s -> 2804.44s]  voy a decir solo audio, listo,\n",
            "[2805.44s -> 2806.44s]  next, y lo pongo en el\n",
            "[2807.44s -> 2808.44s]  escritorio, listo, se puso acá,\n",
            "[2809.44s -> 2810.44s]  se puso a exportar, entonces\n",
            "[2811.44s -> 2812.44s]  espero un poquito, y luego ese\n",
            "[2813.44s -> 2814.44s]  audio sí se lo introduzco a\n",
            "[2815.32s -> 2816.32s]  este lado, Whisper solo recibe\n",
            "[2817.32s -> 2818.32s]  audios, por ahí vi una tarea\n",
            "[2819.32s -> 2820.32s]  que decía que están insertando\n",
            "[2821.32s -> 2822.32s]  directamente el video, entonces,\n",
            "[2823.32s -> 2824.32s]  toca saber muy bien las\n",
            "[2825.32s -> 2826.32s]  herramientas que estoy usando\n",
            "[2827.32s -> 2828.32s]  para lo que lo estoy usando,\n",
            "[2829.32s -> 2830.32s]  entonces, ojo ahí, lo otro que\n",
            "[2831.32s -> 2832.32s]  vi es que, bueno, hasta donde\n",
            "[2833.32s -> 2834.32s]  tenía entendido el Whisper, es\n",
            "[2835.32s -> 2836.32s]  un modelo que solo recibe 30\n",
            "[2837.32s -> 2838.32s]  segundos de audio, entonces,\n",
            "[2839.32s -> 2840.32s]  ¿cómo hicieron?\n",
            "[2841.32s -> 2842.32s]  De pronto la API la mejoraron,\n",
            "[2843.32s -> 2844.32s]  ya recibe, bueno, el modelo\n",
            "[2845.32s -> 2846.32s]  de pronto ya lo mejoraron y\n",
            "[2847.32s -> 2848.32s]  recibe más de 30 segundos,\n",
            "[2849.32s -> 2850.32s]  puede ser, la pregunta que no\n",
            "[2851.32s -> 2852.32s]  les hice y que quisiera que\n",
            "[2853.32s -> 2854.32s]  pusieran en el chat es cuánto\n",
            "[2855.32s -> 2856.32s]  se demoraron en transcribir uno\n",
            "[2857.32s -> 2858.32s]  de los videos, la solución que\n",
            "[2859.32s -> 2860.32s]  yo les propongo y que les puse\n",
            "[2861.32s -> 2862.32s]  en una pista en la semana fue,\n",
            "[2863.32s -> 2864.32s]  ¿será que hay otra librería que\n",
            "[2865.32s -> 2866.32s]  nos permita, primero, agilizar\n",
            "[2867.32s -> 2868.32s]  tiempos y segundo, poner audios\n",
            "[2869.32s -> 2870.32s]  más largos?\n",
            "[2871.32s -> 2872.32s]  Y por ahí una librería que se\n",
            "[2873.32s -> 2874.32s]  llama Facebook, que es una\n",
            "[2875.32s -> 2876.32s]  herramienta que se llama\n",
            "[2877.32s -> 2878.32s]  Faster Whisper, esto se los\n",
            "[2879.32s -> 2880.32s]  digo, no porque les diga, oye,\n",
            "[2881.32s -> 2882.32s]  lo que hicieron está mal, no,\n",
            "[2883.32s -> 2884.32s]  esto es una herramienta que yo\n",
            "[2885.32s -> 2886.32s]  les digo, venga, se los pongo a\n",
            "[2887.32s -> 2888.32s]  su consideración, en este curso\n",
            "[2889.32s -> 2890.32s]  pues, me da el privilegio de\n",
            "[2891.32s -> 2892.32s]  estar con un experto en\n",
            "[2893.32s -> 2894.32s]  inteligencia artificial que les\n",
            "[2895.32s -> 2896.32s]  puede ayudar a encontrar esas\n",
            "[2897.32s -> 2898.32s]  herramientas profesionales que\n",
            "[2899.32s -> 2900.32s]  pueden potenciar, entonces, la\n",
            "[2901.32s -> 2902.32s]  base es Whisper, uso Whisper,\n",
            "[2903.32s -> 2904.32s]  claro, le dije Whisper, OpenAI\n",
            "[2905.32s -> 2906.32s]  no, si era abierto, pues, lo\n",
            "[2907.32s -> 2908.32s]  que pasa es que si usas el\n",
            "[2909.32s -> 2910.32s]  abierto, necesitas tener\n",
            "[2911.32s -> 2912.32s]  recursos computacionales,\n",
            "[2913.32s -> 2914.32s]  ¿cierto? Que Colab da gratis,\n",
            "[2915.32s -> 2916.32s]  si usas la API, te toca pagar\n",
            "[2917.32s -> 2918.32s]  de pronto un centavo de dólar,\n",
            "[2919.32s -> 2920.32s]  pero tienes el resultado en muy\n",
            "[2921.32s -> 2922.32s]  poco tiempo, con la misma\n",
            "[2923.32s -> 2924.32s]  calidad y estás usando los\n",
            "[2925.32s -> 2926.32s]  computadores de OpenAI,\n",
            "[2927.32s -> 2928.32s]  entonces, hay dos cositas, ¿no?\n",
            "[2929.32s -> 2930.32s]  La solución que yo propongo es\n",
            "[2931.32s -> 2932.32s]  una intermedia y es, no vamos a\n",
            "[2933.32s -> 2934.32s]  pagar y Faster Whisper,\n",
            "[2935.32s -> 2936.32s]  para los que sepan de\n",
            "[2937.32s -> 2938.32s]  programación, para los que no\n",
            "[2939.32s -> 2940.32s]  importa, Faster Whisper es una\n",
            "[2941.32s -> 2942.32s]  tecnología, es una iniciativa\n",
            "[2943.32s -> 2944.32s]  que se creó a partir del hecho\n",
            "[2945.32s -> 2946.32s]  de que se quiere que todos los\n",
            "[2947.32s -> 2948.32s]  modelos de inteligencia\n",
            "[2949.32s -> 2950.32s]  artificial sean capaces de ser\n",
            "[2951.32s -> 2952.32s]  ejecutados en un celular, en un\n",
            "[2953.32s -> 2954.32s]  Tamagotchi, en cualquier\n",
            "[2955.32s -> 2956.32s]  dispositivo, porque hacia allá\n",
            "[2957.32s -> 2958.32s]  es donde vamos, si ustedes se\n",
            "[2959.32s -> 2960.32s]  vieron la película Hair, si no\n",
            "[2961.32s -> 2962.32s]  se la han visto, véansela, es\n",
            "[2963.32s -> 2964.32s]  excelente, y se creó en el\n",
            "[2965.32s -> 2966.32s]  momento, ahí se pone de\n",
            "[2967.32s -> 2968.32s]  entrevista que nosotros en un\n",
            "[2969.32s -> 2970.32s]  futuro muy cercano, y yo lo veo\n",
            "[2971.32s -> 2972.32s]  de pronto el otro año, vamos\n",
            "[2973.32s -> 2974.32s]  a tener nuestros asistentes\n",
            "[2975.32s -> 2976.32s]  personales, como Jarvis de\n",
            "[2977.32s -> 2978.32s]  Aeronaut, en nuestros\n",
            "[2979.32s -> 2980.32s]  computadores, en nuestros\n",
            "[2981.32s -> 2982.32s]  dispositivos, y que nos estén\n",
            "[2983.32s -> 2984.32s]  todo el tiempo diciendo y\n",
            "[2984.32s -> 2985.32s]  recomendando cosas, entonces\n",
            "[2986.32s -> 2987.32s]  esta era una iniciativa a\n",
            "[2988.32s -> 2989.32s]  través de una librería que se\n",
            "[2990.32s -> 2991.32s]  llama CTranslate 2, que lo que\n",
            "[2992.32s -> 2993.32s]  hace es transformar el código\n",
            "[2994.20s -> 2995.20s]  del software, en este caso\n",
            "[2996.20s -> 2997.20s]  Whisper, en código C++, lo\n",
            "[2998.20s -> 2999.20s]  cual hace que, para los que no\n",
            "[3000.20s -> 3001.20s]  sepan, el C++ es un lenguaje\n",
            "[3002.20s -> 3003.20s]  de programación que es muy\n",
            "[3004.20s -> 3005.20s]  cercano al lenguaje de máquina,\n",
            "[3006.20s -> 3007.20s]  por lo tanto es muy rápido, y\n",
            "[3008.20s -> 3009.20s]  además, cuantizaron, que\n",
            "[3010.20s -> 3011.20s]  significará cuantizar, cierto,\n",
            "[3012.20s -> 3013.20s]  el modelo, de tal manera que se\n",
            "[3014.20s -> 3015.20s]  podía tener la misma precisión\n",
            "[3016.20s -> 3017.20s]  que usar el grandote de FLOAT\n",
            "[3018.20s -> 3021.20s]  16, con solamente INT8 o INT16\n",
            "[3022.08s -> 3023.08s]  o una precisión mixta, una\n",
            "[3024.08s -> 3025.08s]  cuantización de 8 bits, y eso\n",
            "[3026.08s -> 3027.08s]  mejora los tiempos, entonces,\n",
            "[3028.08s -> 3029.08s]  por ejemplo, acá está una\n",
            "[3030.08s -> 3031.08s]  comparación, que es Faster\n",
            "[3032.08s -> 3033.08s]  Whisper, con una muestra que\n",
            "[3034.08s -> 3035.08s]  se tenía de 13 minutos de\n",
            "[3036.08s -> 3037.08s]  audio, usando cada una de las\n",
            "[3038.08s -> 3039.08s]  especificaciones, con el\n",
            "[3040.08s -> 3041.08s]  OpenAI Whisper, que la mayoría\n",
            "[3042.08s -> 3043.08s]  de los usos, se demora 4\n",
            "[3044.08s -> 3045.08s]  minutos en transcribirlos, 13\n",
            "[3046.08s -> 3047.08s]  minutos, con Faster Whisper se\n",
            "[3048.08s -> 3049.08s]  demora 4 minutos en transcribirlos,\n",
            "[3049.96s -> 3050.96s]  y con el OpenAI Whisper, que\n",
            "[3051.96s -> 3052.96s]  se demora 4 minutos, con\n",
            "[3053.96s -> 3054.96s]  Faster Whisper se demora 59\n",
            "[3055.96s -> 3056.96s]  segundos, entonces, interesante\n",
            "[3057.96s -> 3058.96s]  ¿no?\n",
            "[3059.96s -> 3060.96s]  Use Faster Whisper, este, se\n",
            "[3061.96s -> 3062.96s]  los comento porque, el año\n",
            "[3063.96s -> 3064.96s]  pasado, a la registraduría\n",
            "[3065.96s -> 3066.96s]  nacional del estado civil, le\n",
            "[3067.96s -> 3068.96s]  hicimos un tablero de alertas\n",
            "[3069.96s -> 3070.96s]  tempranas de delitos\n",
            "[3071.96s -> 3072.96s]  preelectorales, y, nos dimos\n",
            "[3073.96s -> 3074.96s]  cuenta, que pues, ¿qué pasa?\n",
            "[3075.96s -> 3076.96s]  Pues, están los territoriales,\n",
            "[3077.96s -> 3078.96s]  en Chocó, en todo el mundo, en\n",
            "[3079.96s -> 3080.96s]  Gran melting, ahí, en las\n",
            "[3081.96s -> 3082.96s]  atacas, debían describir cada\n",
            "[3083.96s -> 3084.96s]  uno, público, el mayor\n",
            "[3085.96s -> 3086.96s]  hermano, el ¨guía de\n",
            "[3087.96s -> 3088.64s]  prysis¨, de vehiculo,\n",
            "[3089.64s -> 3090.64s]  cincuenta en el carro, para\n",
            "[3091.64s -> 3092.64s]  instalar todos eso, porque\n",
            "[3093.64s -> 3106.64s]  amanece la tabla, el\n",
            "[3106.64s -> 3108.04s]  usuario lo hasta\n",
            "[3108.04s -> 3111.18s]  necesita tanto recurso computacional lo cual hace que muchas personas puedan\n",
            "[3111.18s -> 3116.14s]  acceder al tiempo. FastTrackWisper es una solución muy interesante. Entonces se los\n",
            "[3116.14s -> 3119.44s]  voy a dejar acá también yo creo que es importante que ustedes tengan en sus\n",
            "[3119.44s -> 3123.60s]  herramientas sobre todo porque esta tarea no fue una tarea inocente es una\n",
            "[3123.60s -> 3127.50s]  una tarea a propósito y fue una tarea a propósito porque lo primero que se tiene\n",
            "[3127.50s -> 3131.40s]  que solucionar en su vida es que se quite todo el trabajo manual que tiene\n",
            "[3131.40s -> 3135.64s]  si yo odio las tareas manuales y siempre\n",
            "[3135.64s -> 3139.84s]  que hay una herramienta que nos ayude yo la voy a difundir por ejemplo en mi\n",
            "[3139.84s -> 3144.28s]  empresa nosotros usamos FastTrackWisper para hacer generación automática de\n",
            "[3144.28s -> 3147.98s]  actas no para reemplazar el trabajo de la persona la persona sigue haciendo las\n",
            "[3147.98s -> 3151.48s]  actas lo que pasa es que ya se demora menos tiempo y puede poner key points\n",
            "[3151.48s -> 3157.84s]  este key points era una cáscara porque la compañera que expuso tiene\n",
            "[3157.84s -> 3162.16s]  mucha razón una de las compañías que expuso tiene mucha razón que es un key point\n",
            "[3162.16s -> 3167.04s]  un key point puede significar muchas cosas puede ser a nivel cierto mucho de\n",
            "[3167.04s -> 3171.00s]  muchas observaciones uno puede tener yo esperaba que quizás alguien pero pues no\n",
            "[3171.00s -> 3174.84s]  pueden corregirlo de todas maneras que pusiera aquí pues dependiendo de\n",
            "[3174.84s -> 3178.64s]  temáticas si a nivel de conclusiones pero a nivel de lo que se habló de los\n",
            "[3178.64s -> 3181.84s]  perfiles estos son los key points pero a nivel de lo que habló de inteligencia\n",
            "[3181.84s -> 3185.60s]  artificial estos son los key points pero a nivel de las tareas cierto entonces\n",
            "[3185.60s -> 3189.88s]  depende de lo que esté preguntando si me pareció curioso que sólo no mostraba\n",
            "[3189.88s -> 3193.04s]  el dánez pues también estoy en fiscalía por favor no lo difunda mucho\n",
            "[3193.04s -> 3200.00s]  entonces lo interesante el prompt que si siempre les va a ser la tarea de lo\n",
            "[3200.00s -> 3206.28s]  general es encontrar los detalles específicos que generan valor entonces\n",
            "[3206.28s -> 3209.64s]  por eso es que uno se demora tanto prompteando porque uno quiere extraer\n",
            "[3209.64s -> 3214.84s]  esa pequeña palabra que hizo cambiar el humor de esta persona que hizo cambiar\n",
            "[3214.84s -> 3218.96s]  todo el rumbo de la conversación o de la charla cierto eso es lo que quiero que\n",
            "[3218.96s -> 3223.04s]  vayan apuntando entonces está perfecto todo lo que hicieron me parece fabuloso\n",
            "[3223.04s -> 3226.88s]  lo podemos seguir potenciando entonces faster whisper era la herramienta\n",
            "[3226.88s -> 3232.68s]  digamos que yo que yo digo listo el que la encontró súper y lo iba a eximir\n",
            "[3232.68s -> 3239.14s]  del primer parcial digamos parcial que vamos a planear pero pues obviamente es\n",
            "[3239.14s -> 3244.44s]  para la casa pero no pasa nada cierto como encontré\n",
            "[3244.44s -> 3249.32s]  el faster whisper cacharreando mirando oiga necesito más menos recursos más con\n",
            "[3249.32s -> 3253.76s]  menos y así entonces que lo hayan encontrado no pues que es una mina de\n",
            "[3253.76s -> 3257.84s]  oro entonces se los comparto para que lo usen en su vida profesional hay incluso\n",
            "[3257.84s -> 3261.80s]  uno mejor pero que yo le tengo un poquito menos de fiabilidad\n",
            "[3261.80s -> 3268.44s]  pero le he usado y se llama incendio y fast whisper y ese sí ya se va al extremo\n",
            "[3268.44s -> 3274.12s]  en que pueden con ciertas características de gpu transcribir 150\n",
            "[3274.12s -> 3281.14s]  minutos en 98 segundos bueno es otra herramienta usen la con precaución con\n",
            "[3281.14s -> 3286.48s]  responsabilidad pero lo más importante es muchachos por favor\n",
            "[3286.48s -> 3291.52s]  evalúen sus resultados no solamente es keepoins y mire profe tome sus keepoins\n",
            "[3291.52s -> 3297.24s]  yo no sé qué transcribió pero lo transcribí cierto no lo importante es\n",
            "[3297.24s -> 3302.88s]  como ustedes tomen la clase por lo menos tener un muestre o unos chunks unos\n",
            "[3302.88s -> 3307.84s]  pedacitos decir oiga si esto lo dijo el profe realmente cierto ninguno me mostró\n",
            "[3307.84s -> 3312.68s]  ninguna me mostró la transcripción partes que usted diga profe recuerda que\n",
            "[3312.68s -> 3316.36s]  usted dijo esto esto y esto y mire aquí está la transcripción\n",
            "[3316.36s -> 3322.72s]  eso es algo clave porque es que ocurre en algunas entidades en algunas que\n",
            "[3322.72s -> 3328.04s]  trabajo que si usan la tecnología por ejemplo no sé\n",
            "[3328.04s -> 3332.12s]  vamos a hacer un lago de datos y entonces hacen el lago de datos y usan\n",
            "[3332.12s -> 3337.20s]  whisper entonces no vamos a transcribir todas las entrevistas que hagamos en la\n",
            "[3337.20s -> 3341.44s]  empresa listo y entonces le pagan a una empresa y entonces no si hicimos whisper\n",
            "[3341.44s -> 3345.40s]  y lo estamos consignando en el lago de datos de la entidad\n",
            "[3345.40s -> 3349.56s]  y uno les pregunta oiga usted está verificando que esa información si está\n",
            "[3349.56s -> 3352.68s]  acorde por eso este equipo en que cambio un poquito donde sea que yo sea que\n",
            "[3352.68s -> 3358.92s]  porque la educación es importante pero importante el tema cierto si\n",
            "[3358.96s -> 3364.44s]  usted cambia la transcripción real o sea imagínense yo en el DANE entonces si\n",
            "[3364.44s -> 3370.84s]  viene equipo yo hago transcripción de encuestas agropecuarias que vienen de\n",
            "[3370.84s -> 3375.24s]  entrevistas a hablar entonces a cuánto está la papa señor no si 8 lucas a\n",
            "[3375.24s -> 3378.88s]  cuántos y cuánto trajo de papa de dónde viene cuál es su plan\n",
            "[3378.88s -> 3383.20s]  imagínense si yo no si ustedes digamos ustedes son del DANE ustedes están\n",
            "[3383.20s -> 3387.16s]  trabajando conmigo en el equipo si nosotros no validáramos lo que\n",
            "[3387.16s -> 3392.88s]  transcribe whisper un solo valor que se cambie\n",
            "[3392.88s -> 3400.08s]  del precio de la papa sistemáticamente mueve la economía del país\n",
            "[3400.08s -> 3404.20s]  de harto entonces por eso es que hay que regular la\n",
            "[3404.20s -> 3407.24s]  inteligencia artificial por eso es que estos modelos de deep\n",
            "[3407.24s -> 3412.40s]  learning toca supervisarlos el humano no va a quedar por fuera de hecho tiene una\n",
            "[3412.40s -> 3416.44s]  tarea mucho más importante ahorita que es que esos resultados verificar que esos\n",
            "[3416.44s -> 3420.76s]  resultados tengan sentido entonces eso yo sé que no lo hicieron muchos o\n",
            "[3420.76s -> 3424.84s]  muchas porfa revisen revisen partes de la transcripción yo sé que son tres\n",
            "[3424.84s -> 3429.28s]  horas y de hecho parte de la tarea de la próxima es a vuelvanlo a hacer cierto\n",
            "[3429.28s -> 3434.24s]  pero ya como tienen su código cierto entonces ya lo pueden hacer muy sencillo\n",
            "[3434.24s -> 3438.52s]  ejecuto si ya los que tienen transcripción debido no más es que\n",
            "[3438.52s -> 3442.20s]  pongan play a todas las celdas o si lo hacen como les estoy diciendo orientado\n",
            "[3442.20s -> 3447.16s]  a los objetos o el chico que tenía el men no más es ejecutar entran los vídeos\n",
            "[3447.16s -> 3453.56s]  salen los key points pero tienen que haya alguna manera y la tarea en una de\n",
            "[3453.56s -> 3459.64s]  las partes de la tarea que va a poner es y esto es una pregunta abierta\n",
            "[3459.64s -> 3464.88s]  cómo va a medir cuál va a ser su métrica para medir\n",
            "[3465.40s -> 3470.76s]  que la transcripción en de alguna manera está bien hecho\n",
            "[3470.76s -> 3476.32s]  suena complicado piénsenlo y la otra sesión me vienen con propuestas no\n",
            "[3476.32s -> 3481.96s]  profanamente me ocurre que medir de esta manera porque ta ta ta o no es que yo me\n",
            "[3481.96s -> 3486.76s]  puse a mirar un vídeo de usted y yo transcribía a mano y luego vi esa parte\n",
            "[3486.76s -> 3491.28s]  y comparé y listo bueno pero traten de hacerlo matemáticamente hablando cierto\n",
            "[3491.84s -> 3498.24s]  alguna fórmula o alguna metodología cierto no sé que el profe yo cogí en el\n",
            "[3498.24s -> 3503.84s]  tk spice y quita los tokens lo toque ni sé y entonces compare los tokens una\n",
            "[3503.84s -> 3508.72s]  una y medio esto porque recuerden que whisper eso lo vamos a ver más adelante\n",
            "[3508.72s -> 3513.48s]  whisper tiene una métrica cuando se entrenó que se llama el wear el word\n",
            "[3513.48s -> 3520.68s]  error ratio o el error del radio de palabra y ahí se encuentran tres\n",
            "[3520.68s -> 3525.60s]  métricas importantísimas el número de sustituciones\n",
            "[3525.60s -> 3532.84s]  ya que no dijo transformers pero en esa no me sirve que no dijo por ejemplo audio\n",
            "[3532.84s -> 3535.96s]  sino que dijo\n",
            "[3535.96s -> 3542.16s]  music cierto sustituyó la segunda es\n",
            "[3542.16s -> 3551.16s]  omitió es decir lo quitó y la tercera es añadió lo que se conoce actualmente\n",
            "[3551.16s -> 3556.68s]  como alucinación entonces whisper por más large que sea o\n",
            "[3556.68s -> 3562.76s]  small tiene unas métricas de precisión de ese\n",
            "[3562.76s -> 3567.60s]  wear entonces interesante tampoco lo vi tampoco\n",
            "[3567.60s -> 3571.32s]  es que no tranquilos no les vaya bien la tarea pero hubiera sido muy interesante\n",
            "[3571.68s -> 3576.60s]  que alguien me dice profe listo yo me puse a leer el paper de whisper\n",
            "[3576.60s -> 3581.12s]  el paper de whisper es un paper científico que salió hace mucho tiempo\n",
            "[3581.12s -> 3588.28s]  en el 2016 17 por ahí va la cuestión perdón en el 2020 por ahí va la cuestión\n",
            "[3588.28s -> 3593.08s]  y entonces yo me puse a leer el paper de whisper para saber por qué es que yo\n",
            "[3593.08s -> 3598.24s]  transcribe y yo no sé o sea dato importante profes se entrenó\n",
            "[3598.24s -> 3605.32s]  con 680 mil horas ahí está 680 mil horas de audios transcritos es decir\n",
            "[3605.32s -> 3609.12s]  contrataron un ejército de gente para transcribir audios y luego hicieron una\n",
            "[3609.12s -> 3614.92s]  red neuronal que en la segunda hora ahorita vamos a ver los principios\n",
            "[3614.92s -> 3618.76s]  básicos porque la votación dijo vamos a ver redes neuronales no tanto desde cero\n",
            "[3618.76s -> 3625.20s]  pero sí pues vamos a hacer un pequeño quiz de no calificable a ver cómo están\n",
            "[3625.20s -> 3629.56s]  en eso a ver si realmente entendemos lo que es una red neuronal y entonces uno\n",
            "[3629.56s -> 3634.44s]  dice no vea profe a mire es que dónde están los puntos mire a profes que mire\n",
            "[3634.44s -> 3640.16s]  que el whisper es un transformer y hace unos espectrogramas es una parte\n",
            "[3640.16s -> 3645.72s]  visual y luego entonces me muestran a mire profe mire es que en estos datasets\n",
            "[3645.72s -> 3652.00s]  se tiene un wear de tanto cierto acá también hay otra métrica el\n",
            "[3652.52s -> 3657.92s]  rr entonces me decir profe de transcripción yo estoy esperando que en\n",
            "[3657.92s -> 3662.24s]  promedio whisper se haya equivocado tanto por lo tanto en los equipos pueda\n",
            "[3662.24s -> 3666.64s]  confiar tanto una decisión importante cierto a su\n",
            "[3666.64s -> 3671.20s]  gerente o si usted es gerente de que tomar esa decisión y no puede decir no\n",
            "[3671.20s -> 3675.12s]  pues es que yo transcribí ya entonces no no lo tengo como un regaño sino como una\n",
            "[3675.12s -> 3679.72s]  una oportunidad de mejor cierto importante entonces si yo leo el paper\n",
            "[3679.72s -> 3684.28s]  no tiene que leerlo todo mándeselo a chepití digale oiga digame los equipos de\n",
            "[3684.28s -> 3688.80s]  este paper porque entiendo cierto todo pantallacito que significa esa vaina que\n",
            "[3688.80s -> 3693.48s]  es un flores que es un flor cierto ah whisper también puede traducir no\n",
            "[3693.48s -> 3698.60s]  sabía que interesante que otras tareas podría hacer whisper por nosotros\n",
            "[3698.60s -> 3705.32s]  entonces es más como invitarlos a que si está excelente lo que hicieron me\n",
            "[3705.32s -> 3708.76s]  parece fabuloso quiere decir que la maestría en analítica está funcionando\n",
            "[3708.76s -> 3712.56s]  porque los ponen a hacer código y lo están haciendo seguramente soportado\n",
            "[3712.56s -> 3715.88s]  por chepití pero están entendiendo lo que están haciendo yo creería que en un\n",
            "[3715.88s -> 3721.36s]  85% pero qué interesante es ir más allá cierto que es whisper que se sabe\n",
            "[3721.36s -> 3725.80s]  nada o sea si listo es un modelo pero y cómo funciona cómo se creó porque lo\n",
            "[3725.80s -> 3730.32s]  hicieron cierto les digo este whisper es la herramienta que está incluida en\n",
            "[3730.32s -> 3733.04s]  casi todos los sistemas de reconocimiento es el que está detrás de\n",
            "[3733.04s -> 3738.48s]  teams cuando usted teams los que trajan acá sus empresas tienen teams el modelo\n",
            "[3738.48s -> 3743.16s]  que transcribe el audio es whisper es una versión de\n",
            "[3743.16s -> 3747.84s]  whisper que afinaron en microsoft entonces qué interesante pues todo esto\n",
            "[3747.84s -> 3751.56s]  cierto y por qué están todos esos idiomas y por qué es tan bueno en español\n",
            "[3751.56s -> 3754.88s]  eso será una cosa interesante de saber bueno\n",
            "[3754.88s -> 3759.64s]  anécdotas para el futuro y les va a mostrar pues les va a motivar antes de\n",
            "[3759.64s -> 3762.96s]  que nos vayamos al descansito\n",
            "[3763.76s -> 3768.44s]  lo que pueden lograr si se ponen las pilas\n",
            "[3768.44s -> 3773.32s]  y comienzan a estructurar sus proyectos o sus tareas\n",
            "[3773.32s -> 3777.36s]  como les dije que se estructuran con esa organización de carpetas ahorita les\n",
            "[3777.36s -> 3783.60s]  mando digamos un ejemplo pero que podemos lograr entonces\n",
            "[3783.60s -> 3788.88s]  disculpen yo busco la aplicación la tenía por ahí\n",
            "[3788.88s -> 3796.84s]  pero creo que ya no la tengo que tener toca iniciarla me toca ir al puerto\n",
            "[3796.84s -> 3801.04s]  entonces disculpenme yo acá me llevo este collito\n",
            "[3801.04s -> 3806.92s]  y bueno esto fue algo que le presentamos a la alcaldía de Tunján como ayer para\n",
            "[3806.92s -> 3812.88s]  que las personas se motiven a que deben disminuir su trabajo manual para hacer\n",
            "[3812.88s -> 3818.56s]  cosas más interesantes no hacer cosas... solución de pqr's cierto\n",
            "[3818.56s -> 3823.32s]  aplicación de whisper interesante esto es un producto que ya se puede vender si\n",
            "[3823.32s -> 3828.48s]  alguien en esta clase quisiera hacer un proyecto de esta índole\n",
            "[3828.48s -> 3833.48s]  yo le garantizo pues no le puedo garantizar nada pero yo tengo mucha fe\n",
            "[3833.48s -> 3840.00s]  en que este tipo de productos primero le va a dar mucho dinero y segundo le va a\n",
            "[3840.00s -> 3845.52s]  abrir muchas puertas en todo el estado y en toda la parte privada entonces para\n",
            "[3845.52s -> 3849.16s]  qué sirve whisper al final para qué sirve faster whisper es que yo pueda\n",
            "[3849.16s -> 3855.00s]  hacer cosas como estas cierto es como hola me llamo daniel montenegro\n",
            "[3855.00s -> 3865.40s]  mi celular es 10 32 4 38 34 mi celular es 3 13 2 0 0 86 46 y quisiera\n",
            "[3865.40s -> 3873.68s]  denunciar a el alcalde del municipio de tenjo porque le da contratos a sus\n",
            "[3873.68s -> 3877.88s]  amigos a mí no me da nada y está cometiendo posible peculado por favor\n",
            "[3877.88s -> 3882.20s]  ayuden entonces una aplicación real de whisper\n",
            "[3882.20s -> 3887.28s]  es bien interesante es podemos transcribir en tiempo real sí y luego\n",
            "[3887.28s -> 3891.48s]  mandarlo entonces fíjense que algo en común de su tarea y esto es que\n",
            "[3891.48s -> 3896.04s]  justamente es usar whisper y algún modelo generativo chat gpt si quiere\n",
            "[3896.04s -> 3903.76s]  pero hay muchos cierto entonces con muy poco esfuerzo pero esfuerzo\n",
            "[3903.76s -> 3908.32s]  muy guiado usted puede llegar a hacer cosas como esta donde ya no necesitamos\n",
            "[3908.32s -> 3912.28s]  que la persona vaya y radica ese pqr por allá\n",
            "[3912.28s -> 3917.92s]  de manera manual sino que incluso en un enfoque inclusivo de la redundancia\n",
            "[3917.92s -> 3923.56s]  diferencial incluso personas que no saben leer y escribir podrían acceder al\n",
            "[3923.56s -> 3928.96s]  sistema de peticiones quejas reclamos y denuncias cierto entonces bien\n",
            "[3928.96s -> 3934.68s]  interesante esto porque justamente fíjense que su tarea les abre las\n",
            "[3934.68s -> 3938.80s]  puertas a hacer este tipo de aplicaciones y solamente dos tecnologías involucradas\n",
            "[3938.80s -> 3946.84s]  hay whisper chat gpt cierto entonces piensen si les gusta este proyecto o\n",
            "[3946.92s -> 3952.00s]  similares les estoy votando ideas para sus propuestas esto es algo que se puede\n",
            "[3952.00s -> 3956.92s]  hacer en una semana pues es decir luego hay que uno ya pues si cierto comienza a\n",
            "[3956.92s -> 3961.00s]  aprender con chat gpt cositas que es un front que es un back como hago tal como\n",
            "[3961.00s -> 3965.12s]  pongo aquí este loguito pero fíjense que con dos tecnologías súper básicas que\n",
            "[3965.12s -> 3969.72s]  ustedes ya hicieron en su tarea pueden lograr algo como esto cierto\n",
            "[3969.72s -> 3975.36s]  y además pues para los que les gusta el back pues obviamente\n",
            "[3975.36s -> 3978.92s]  esto parece magia pero esto es lo que son cosas de las que vamos a aprender en\n",
            "[3978.92s -> 3985.08s]  esta asignatura y es cómo efectivamente usar los modelos de lenguaje para que a\n",
            "[3985.08s -> 3989.40s]  partir de los textos yo pueda obtener a partir de un json que realmente es un\n",
            "[3989.40s -> 3993.52s]  registro eventualmente de una base de datos los datos de manera automática\n",
            "[3993.52s -> 4000.16s]  cierto entidad que responde una explicación qué tipo de qr es cierto y\n",
            "[4000.56s -> 4005.80s]  si es una fac o no es una fac ese tipo de cositas listo entonces para que se\n",
            "[4005.80s -> 4011.84s]  animen entonces yo creo que tomémonos a bueno y el código no pues sí porque\n",
            "[4011.84s -> 4015.88s]  pero usted nos muestra esto pero y el juego que como así no le creo que sea\n",
            "[4015.88s -> 4020.96s]  modular fíjense la estructura\n",
            "[4023.12s -> 4027.36s]  input notebooks source\n",
            "[4028.24s -> 4032.40s]  audios misma cosa y la única manera de\n",
            "[4032.40s -> 4037.28s]  trabajar en equipo que estoy trabajando acá con un compañero caleño es eso que\n",
            "[4037.28s -> 4042.24s]  esté organizado que se destruyan tareas pero que se conserve esa estructura\n",
            "[4042.24s -> 4048.72s]  entonces muy interesante y que además sea pues obviamente pues muy modular o\n",
            "[4048.72s -> 4051.92s]  sea que yo tengo una clase cita que tengo\n",
            "[4051.92s -> 4056.32s]  un objeto que tengo un constructor que tengo unos atributos que tengo unas\n",
            "[4056.32s -> 4059.08s]  funcionalidades de tal manera que si yo tomo ese código y me lo descargo en mi\n",
            "[4059.08s -> 4066.08s]  computador lo puedo ejecutar sin tener que tener nada pues adicional porque por\n",
            "[4066.08s -> 4071.84s]  ahí también deben estar esos requirements que bueno dependiendo cierto\n",
            "[4071.84s -> 4076.16s]  hay personas que los hacen de una manera o de otra que dependen si necesitamos\n",
            "[4076.16s -> 4083.64s]  me toca regañar al caleño por haberlo puesto así tal vez entonces fíjense\n",
            "[4083.64s -> 4086.88s]  con muy poquito ustedes están a muy poquito\n",
            "[4086.88s -> 4092.08s]  de hacer este tipo de cosas entonces bueno vámonos a un descansito acá como\n",
            "[4092.08s -> 4097.08s]  no es por hora entonces podemos quedarnos las tres horas lo que\n",
            "[4097.08s -> 4101.52s]  va a hacer es que voy a parar la grabación listo y nos vemos en 10\n",
            "[4101.52s -> 4107.52s]  minutitos para así comenzar la clase entonces\n",
            "[4107.52s -> 4114.56s]  no sé cómo parar acá pero creo que si yo le pongo como\n",
            "[4114.56s -> 4120.64s]  de tener grabación y creería que se para de tener grabación y luego hacemos la\n",
            "[4120.64s -> 4123.92s]  segunda parte la tercera parte\n",
            "[4124.36s -> 4128.52s]  sí sí sí este como ya de la universidad central ya podemos grabar ya podemos\n",
            "[4128.52s -> 4133.84s]  tener las tres horas pero para la grabación para que no sea sólo un vídeo\n",
            "[4133.84s -> 4140.96s]  te pongo el cronómetro porfa gracias paulín hay compa\n",
            "time: 4min 57s (started: 2024-08-24 11:41:15 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total time to Transcribe: 9 min 23 sec"
      ],
      "metadata": {
        "id": "VU6CkfmyV8d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segments_2, info_2 = model.transcribe(\"/content/drive/MyDrive/04 SEMESTRE MAD/Copia de Universidad Central - Deep Learning - Clase 2_ Regularización de Redes Neuronales (2024-08-17 08_36 GMT-5).mp3\", beam_size=1)\n",
        "\n",
        "print(\"Detected language '%s' with probability %f\" % (info_2.language, info_2.language_probability))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxhMCnN_NjHD",
        "outputId": "20ef1f53-1b51-4971-8198-501d2ca28d5a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language 'es' with probability 0.993652\n",
            "time: 16.9 s (started: 2024-08-24 11:56:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for segment in segments_2:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GqPJKCyNr5d",
        "outputId": "1b2a9fd3-78ed-4e60-dd8a-5cc80081acd6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00s -> 9.60s]  Iniciamos. Ah, pero un momento, si la grabación queda en inglés. Tocaría ver cómo se está\n",
            "[9.60s -> 15.40s]  transcribiendo, no? Miremos en algún, en algún lado. Yo no sé si se vea la transcripción.\n",
            "[16.72s -> 20.48s]  Bueno, veamos cómo se ve al final. Pues porque estoy hablando en español, me imagino que la\n",
            "[20.48s -> 24.52s]  transcripción no va a tener sentido, pero bueno, miremos a ver qué pues el team si es interesante\n",
            "[24.52s -> 32.84s]  que se tiene. Bueno, listo. Bienvenidos y bienvenidas a este segunda sesión de la\n",
            "[32.84s -> 39.64s]  maestría en analítica de datos, Deep Learning. Me excuso nuevamente que soy un poquito malito de la\n",
            "[39.64s -> 52.08s]  voz. Pero entonces lo primero que quiero preguntarles es si alguien quiere exponer su\n",
            "[52.08s -> 59.84s]  tarea. Actualmente recibí. Listo, Cristian, dale adelante. Pero antes de que comiences. Entonces\n",
            "[59.84s -> 69.80s]  actualmente recibí una, dos, tres, cuatro. Te faltan varias tareas. Entonces, varias personas.\n",
            "[69.80s -> 76.60s]  Entonces tienen de qué a las 10 para mandarla. Y la idea es que en este espacio socialicemos\n",
            "[76.60s -> 86.88s]  entre todos. ¿Qué hicieron? ¿Cómo les fue? Y pues vamos a dar la solución. Mi solución. Listo.\n",
            "[86.88s -> 95.60s]  Entonces dale adelante, Cristian. Cuéntanos cómo te fue. Perdón, si Cristian. Yo creo que era Camilo.\n",
            "[95.60s -> 104.80s]  No, sí. Camilo. Listo, ya estamos viendo. Adelante. Listo, pues de acuerdo a la tarea,\n",
            "[105.36s -> 110.48s]  transcribir la sesión completa de tres horas usando el modelo de Whisper. Usar\n",
            "[110.48s -> 117.40s]  ChagP3 para extraer los KPON de la sesión usando la transcripción Whisper. Crear un\n",
            "[117.40s -> 123.12s]  logo usando el modelo de generación de imágenes para cada uno de los grupos de la comunidad y\n",
            "[123.12s -> 129.12s]  pues leer los roles de la ciencia de datos. Pues acá lo que hice fue primero pues instalar\n",
            "[130.12s -> 137.28s]  OpenAI Whisper. Posteriormente importé Whisper. El modelo de Whisper lo hace. Pues acá hay varios\n",
            "[137.28s -> 146.32s]  modelos. Uno que es largo, medio y pequeño que es Small. Acá convertir la transcripción del\n",
            "[146.32s -> 153.72s]  vídeo. Pues de acuerdo a los tres vídeos que nos pasó. Acá imprimí el primer vídeo. Entre esto\n",
            "[153.88s -> 162.52s]  transcribió esto. Y acá en el ChagP3 hice la pregunta de qué eran los KPON, perdón,\n",
            "[162.52s -> 168.60s]  cuáles son los KPON de la transcripción. Y acá me dejó el primer vídeo. Así con los demás.\n",
            "[168.60s -> 180.40s]  Entonces acá se solucionaron dos puntos. Después. Porfa, Camilo, déjame leer los KPON brevemente.\n",
            "[181.40s -> 188.40s]  No solamente es ese. Si no, quisiera ver. Dame dos segunditos.\n",
            "[194.60s -> 197.08s]  Bueno, vale la pena decir. Listo. Perfecto.\n",
            "[200.68s -> 204.76s]  Crítica hacia la idea de que la educación es importante en la actualidad. No, bueno,\n",
            "[204.92s -> 216.04s]  hay que revisar lo que es lo que es los KPON. Yo no dije eso. La educación como está planteada\n",
            "[216.04s -> 227.04s]  hoy en día no es lo importante. Bueno. Sí, ¿sabes por qué pasó eso, profe? Porque resulta\n",
            "[227.04s -> 234.36s]  que en el modelo, cuando uno coloca el modelo, pues dependiendo también como la calidad,\n",
            "[234.36s -> 242.44s]  yo lo llamo así. Por ejemplo, acá había el modelo largis, creo que se llama, de este,\n",
            "[242.44s -> 248.00s]  si votaba la información, pero se demoraba muchísimo. Acá yo me acuerdo que dejé como\n",
            "[248.00s -> 255.76s]  cinco horas y no dejó. Entonces dentro del chat sí vi que muchos le cambiaron el tipo de modelo,\n",
            "[255.76s -> 262.48s]  pero teniendo en cuenta la calidad, pues ahí se baja. Yo me imagino que cuando se bajó la calidad,\n",
            "[262.48s -> 268.04s]  pues de acuerdo a esto, pues mostró de cierta manera pues algunos textos que no eran coherentes.\n",
            "[270.16s -> 279.64s]  Es la intuición. Listo. Perfecto. Listo, Christian, espérate un segundito. Desafíos,\n",
            "[279.64s -> 288.04s]  expectativas, proyectos relevantes. Ajá. Ok, pero esto suena como si hablaras solo de mí, ¿no?\n",
            "[288.04s -> 294.44s]  Listo. Dale, sigue. Continúa con el segundo punto. Segundo, pues acá están los queipos.\n",
            "[294.44s -> 309.12s]  Sí, dale. Ya leí. Dale, adelante. Y el tercero era este último.\n",
            "[316.24s -> 323.44s]  Ok, ok. Súper interesante. Y te pusiste a averiguar, digamos, esos queipos, esas palabras,\n",
            "[323.44s -> 330.44s]  como PCA, TSN, UMAP, BAG, INDUSTRY, te pusiste a mirar de pronto qué significaban o simplemente hiciste...\n",
            "[330.44s -> 336.16s]  No, profe, ahí sí, ahí sí me corché, no fui, no hay más alcance, perdón. Ahí sí, qué pena.\n",
            "[341.96s -> 349.32s]  Pero no, queda de tarea, lo voy a averiguar. Discúlpeme. Bueno, no, tranquilos. Acá nada es obligatorio,\n",
            "[349.32s -> 356.60s]  solo es como para saber si entendiste lo que... Listo. No, excelente, excelente, me gusta, me gusta la iniciativa.\n",
            "[356.60s -> 363.16s]  Ah, bueno, y ahora nos vas a mostrar, Dalí, o bueno, no sé. Dalí, pues yo lo que traté fue primero a ver si se podía\n",
            "[363.16s -> 372.80s]  generar un logo, pues de acuerdo a acá en Python, pues este no, medio fue una imagen, una muy buena imagen,\n",
            "[372.80s -> 382.16s]  medio un círculo y un hombrecito ahí. Entonces yo sí busqué en Dalí los posibles logos, por ejemplo, de cada uno de los grupos.\n",
            "[382.16s -> 394.80s]  Por ejemplo, logos de lenguaje natural, pues acá cogí este, este y este. De los posibles logos de computer vision, cogí este.\n",
            "[395.12s -> 405.56s]  Aquí es que no se ve muy bien. Este, este y este. Y posibles logros de series de tiempo, este, este y este.\n",
            "[405.56s -> 416.68s]  Eso de acuerdo a un prong, no sé si tenga por aquí el prong. Acá dentro de los prongs hice la pregunta.\n",
            "[417.68s -> 426.68s]  Antes de hacer la pregunta, pues lo primero que hice fue la consulta de qué posibles, por ejemplo, ideas, teniendo en cuenta esto,\n",
            "[426.68s -> 435.68s]  me podría ayudar para generar el prong. Él me da cinco ideas, seis ideas y posiblemente cogí una de estas ideas y le solicité que me hiciera la imagen.\n",
            "[435.68s -> 441.68s]  Y acá pues ya dejó el tema de la imagen, así para ser un poquito más preciso.\n",
            "[441.68s -> 446.68s]  Y ya la última pregunta era leer en el notebook los roles de ciencia de datos.\n",
            "[446.68s -> 457.68s]  Entonces acá en este caso pues habían como diez roles, desde el arquitecto de datos hasta el ingeniero MLOPS.\n",
            "[457.68s -> 461.68s]  Y pues acá una pequeña descripción de cada uno de ellos.\n",
            "[462.68s -> 474.68s]  Excelente. Listo. Gracias Cristian. Muy bonito. Yo quiero que tomes solamente una imagen de cada grupo y la publiques en cada grupo.\n",
            "[474.68s -> 481.68s]  Y ahorita vamos a hacer la votación. Perfecto. Listo. Gracias Cristian.\n",
            "[481.68s -> 493.68s]  Entonces los que siguen, si tienen una solución muy similar a la de Cristian o los que vayan a exponer de aquí a los que quieran exponer,\n",
            "[493.68s -> 501.68s]  si es lo mismo, tranquilos, lo exponen, dicen como tal, solo si hay alguna cosa diferente de lo que se expuso, lo enfatizan.\n",
            "[501.68s -> 505.68s]  ¿Listo? Los logos pues sí muéstrenlos y hacen la misma tarea que Cristian.\n",
            "[505.68s -> 509.68s]  Si hicieron varios logos por grupo, elijan solo uno.\n",
            "[510.68s -> 514.68s]  Que ustedes decidan o que alguien decida, una inteligencia artificial, no sé.\n",
            "[514.68s -> 521.68s]  Y publiquenlo en cada uno de los grupos porque vamos a hacer la votación inmediatamente después. ¿Listo?\n",
            "[521.68s -> 529.68s]  Entonces, listo, adelante. Creo que está primero Fernando. Entonces dale Fernando.\n",
            "[536.68s -> 546.68s]  Buenos días compañeros. Ahí ya se ve la pantalla, ¿verdad?\n",
            "[546.68s -> 548.68s]  Sí, sí señor.\n",
            "[548.68s -> 553.68s]  Listo. Entonces lo primero que hice fue la instalación de las librerías.\n",
            "[553.68s -> 562.68s]  Pongo la recomendación de usar un entorno con T4 para que sea rápido.\n",
            "[562.68s -> 566.68s]  No sé qué, esperando que den el resultado.\n",
            "[566.68s -> 569.68s]  La carga de librerías.\n",
            "[569.68s -> 575.68s]  Entonces acá tenía inicialmente una para extraer el audio del video.\n",
            "[575.68s -> 579.68s]  Sin embargo, pues los videos estaban bastante pesados.\n",
            "[579.68s -> 581.68s]  Llenó el drive.\n",
            "[581.68s -> 586.68s]  Entonces dejé la función creada, pero finalmente monté los audios.\n",
            "[587.68s -> 591.68s]  Importamos Whisper, también Transformers y Pipeline.\n",
            "[591.68s -> 596.68s]  Transformers básicamente es para echarle mano a los modelos que están en Hugging Face.\n",
            "[596.68s -> 600.68s]  Importé algunas como Spicy y bueno.\n",
            "[600.68s -> 607.68s]  Fueron librerías para hacer algunos ensayos de qué tomaba como insight y todo eso.\n",
            "[607.68s -> 610.68s]  Finalmente me fui con la de OpenAI claramente.\n",
            "[611.68s -> 617.68s]  También importé Picot para guardar algunos resultados que iba obteniendo.\n",
            "[619.68s -> 621.68s]  Importé TicToken.\n",
            "[621.68s -> 624.68s]  Esto sirve para contar los tokens.\n",
            "[624.68s -> 632.68s]  Y en ciertas ocasiones cuando la cadena era muy larga, pues era para darle manejo a este tipo de cosas.\n",
            "[632.68s -> 637.68s]  Y saber cuántos tokens se estaban enviando y cuál era la capacidad de cada modelo.\n",
            "[637.68s -> 641.68s]  O de la API de OpenAI.\n",
            "[641.68s -> 647.68s]  Aquí se inserta la API de OpenAI.\n",
            "[647.68s -> 649.68s]  La que cada uno tenga en su cuenta.\n",
            "[649.68s -> 652.68s]  Yo lo dejé así para mostrarlo.\n",
            "[652.68s -> 655.68s]  Creamos como las funciones.\n",
            "[655.68s -> 658.68s]  Entonces una función de extraer audio.\n",
            "[658.68s -> 661.68s]  Otra de speech to text.\n",
            "[661.68s -> 665.68s]  Aquí terminé usando el modelo que estaba en Hugging Face.\n",
            "[665.68s -> 668.68s]  Utilicé este WhisperLarge v2.\n",
            "[668.68s -> 671.68s]  Sin embargo ya está disponible el v3.\n",
            "[671.68s -> 677.68s]  Implementamos que utilice la GPU disponible.\n",
            "[679.68s -> 682.68s]  Otra función de contar tokens.\n",
            "[682.68s -> 686.68s]  Otra función para extraer los keypoints con GPT.\n",
            "[686.68s -> 689.68s]  Y utilicé el mejor modelo.\n",
            "[689.68s -> 695.68s]  El modelo que usé fue el Prom.\n",
            "[695.68s -> 703.68s]  En donde básicamente se le piden temas principales, tareas y proyectos sugeridos, roles y contribuciones.\n",
            "[703.68s -> 708.68s]  Roles, contribuciones y las profesiones de los participantes.\n",
            "[709.68s -> 718.68s]  Y una función en donde se integran las funciones de extraer el audio y el video.\n",
            "[718.68s -> 721.68s]  Que es este comentario por lo que ya comenté previamente.\n",
            "[721.68s -> 724.68s]  Convertir el audio al texto.\n",
            "[724.68s -> 727.68s]  Y extraer los insights.\n",
            "[727.68s -> 733.68s]  De modo que los resultados se dan como un diccionario en donde se devuelve la transcripción y ya el resumen.\n",
            "[734.68s -> 738.68s]  Montamos la unidad de Drive.\n",
            "[739.68s -> 743.68s]  Establecemos cual es el listado de audios.\n",
            "[743.68s -> 744.68s]  De Drive.\n",
            "[744.68s -> 747.68s]  Entonces está el audio 1, audio 2, audio 3.\n",
            "[747.68s -> 754.68s]  Y básicamente ese listado lo que hace es que saca la transcripción de cada audio.\n",
            "[754.68s -> 757.68s]  Y la une con el audio anterior.\n",
            "[757.68s -> 762.68s]  Con eso quedamos con una sola cadena de texto continua.\n",
            "[763.68s -> 767.68s]  El resultado pues básicamente...\n",
            "[767.68s -> 771.68s]  Ah bueno, obtenemos el resultado que son los keypoints.\n",
            "[771.68s -> 774.68s]  La transcripción y el resumen.\n",
            "[774.68s -> 777.68s]  Y los guardé como un objeto picol.\n",
            "[777.68s -> 781.68s]  Por si después ya no tenía recursos gratis de la T4.\n",
            "[781.68s -> 783.68s]  Pero pues me alcanzaron.\n",
            "[783.68s -> 786.68s]  Los resultados básicamente fueron los siguientes.\n",
            "[786.68s -> 791.68s]  ¿Qué puedo destacar y qué me parece súper interesante?\n",
            "[791.68s -> 794.68s]  Que tuve en cuenta cuando nombraste al profe Oscar.\n",
            "[794.68s -> 796.68s]  Un colaborador en la planificación del curso.\n",
            "[796.68s -> 799.68s]  Específicamente integración de contenido relevantes en la industria.\n",
            "[799.68s -> 802.68s]  Y a Paula como asistente en la clase.\n",
            "[802.68s -> 804.68s]  Ayuda con la organización.\n",
            "[804.68s -> 807.68s]  Facilita la comunicación entre estudiantes y profesor.\n",
            "[807.68s -> 809.68s]  Me pareció súper chévere.\n",
            "[811.68s -> 816.68s]  También dio como los temas tocados de forma general.\n",
            "[816.68s -> 818.68s]  Pero sin dejar nada por fuera.\n",
            "[819.68s -> 821.68s]  Nombrando aquí por ejemplo a Hugin Faze.\n",
            "[821.68s -> 824.68s]  Cuando nombraste a Whisper y a Florence.\n",
            "[824.68s -> 827.68s]  Como aplicaciones prácticas.\n",
            "[829.68s -> 832.68s]  Las tareas como utilizar Whisper para transcribir el audio de la clase.\n",
            "[832.68s -> 835.68s]  Usar ChattyPT para resumir puntos claves de la transcripción.\n",
            "[835.68s -> 838.68s]  Diseñar logos para el grupo de WhatsApp usando Dalí.\n",
            "[838.68s -> 843.68s]  Explorar la creatividad y aplicar conceptos de diseño asistido por inteligencia artificial.\n",
            "[844.68s -> 848.68s]  También habla de la presentación sobre un modelo específico.\n",
            "[848.68s -> 851.68s]  Y la aplicabilidad en un contexto real.\n",
            "[854.68s -> 858.68s]  Habla del profe como que a pesar de ser educador.\n",
            "[858.68s -> 861.68s]  Tiene experiencia en otros temas.\n",
            "[861.68s -> 864.68s]  Y destaca lo de pruebas estatales y proyectos en el TAN.\n",
            "[864.68s -> 867.68s]  Y un pequeño resumen de los estudiantes.\n",
            "[867.68s -> 870.68s]  Entonces básicamente dice que son de diversas carreras.\n",
            "[871.68s -> 874.68s]  Estamos buscando aplicar DeepLearning en sus campos.\n",
            "[874.68s -> 878.68s]  Con roles que van desde analistas de datos, desarrolladores y científicos de datos.\n",
            "[882.68s -> 886.68s]  Y pues es un resumen creo que bastante conciso.\n",
            "[886.68s -> 891.68s]  Como para tener una idea muy buena de las tres horas de clase.\n",
            "[893.68s -> 896.68s]  Ya en términos de los logos.\n",
            "[897.68s -> 900.68s]  Tengo los siguientes.\n",
            "[903.68s -> 908.68s]  Entonces este para visión por computadora.\n",
            "[910.68s -> 913.68s]  Este para Natural Language Processing.\n",
            "[916.68s -> 919.68s]  Y este para Time Series.\n",
            "[920.68s -> 923.68s]  Super interesante.\n",
            "[923.68s -> 926.68s]  ¿Cuál fue el prompt que usaste para el primero?\n",
            "[930.68s -> 933.68s]  Si se puede saber, tranquilo.\n",
            "[933.68s -> 936.68s]  Aquí en pro de la clase.\n",
            "[938.68s -> 943.68s]  Entonces dice eres un asistente experto en educación, escritura de documentos e inteligencia artificial.\n",
            "[943.68s -> 946.68s]  Tengo la transcripción de una clase introductoria sobre Deep Learning.\n",
            "[947.68s -> 950.68s]  Y impartida como parte de una maestría analítica de datos.\n",
            "[950.68s -> 953.68s]  Su tarea es extraer los siguientes insights.\n",
            "[953.68s -> 956.68s]  Resumen los temas más importantes tratados durante la clase.\n",
            "[956.68s -> 959.68s]  Destacando los conceptos y técnicas clave explicados.\n",
            "[959.68s -> 962.68s]  Tareas y proyectos sugeridos.\n",
            "[962.68s -> 966.68s]  Identifica cualquier tarea, ejercicio o proyecto mencionado que los estudiantes deban realizar.\n",
            "[966.68s -> 969.68s]  Incluyendo detalles sobre los objetivos de estas tareas.\n",
            "[969.68s -> 972.68s]  Roles, contribuciones y profesiones.\n",
            "[973.68s -> 976.68s]  Describe los roles específicos de las personas mencionadas en la transcripción.\n",
            "[976.68s -> 979.68s]  Por ejemplo, profesores, asistentes de enseñanza, estudiantes.\n",
            "[979.68s -> 982.68s]  Y sus respectivas contribuciones durante la clase.\n",
            "[982.68s -> 985.68s]  Adicionalmente, resume las profesiones y cargos laborales de las personas que intervienen en la clase.\n",
            "[985.68s -> 988.68s]  Indicando si son roles analíticos.\n",
            "[988.68s -> 991.68s]  La transcripción.\n",
            "[991.68s -> 994.68s]  Y básicamente acá se inserta el texto.\n",
            "[994.68s -> 997.68s]  Que ya se había extraído de cada uno de los audios.\n",
            "[997.68s -> 1000.68s]  Y unido posteriormente a la transcripción.\n",
            "[1001.68s -> 1004.68s]  Proporciona una lista organizada de los insights mencionados.\n",
            "[1004.68s -> 1007.68s]  Separa las tareas asignadas a los estudiantes en una lista aparte.\n",
            "[1007.68s -> 1010.68s]  La respuesta que proporciones debe tener entre 3,500 y 4,000 tokens.\n",
            "[1010.68s -> 1013.68s]  Buscando aprovechar al máximo las respuestas de 4,096 de ese modelo.\n",
            "[1013.68s -> 1016.68s]  Excelente.\n",
            "[1016.68s -> 1019.68s]  Muy bien.\n",
            "[1019.68s -> 1022.68s]  Muy buen prompt.\n",
            "[1022.68s -> 1025.68s]  Me gusta.\n",
            "[1025.68s -> 1028.68s]  Listo.\n",
            "[1029.68s -> 1032.68s]  El del logo.\n",
            "[1032.68s -> 1035.68s]  Muy breve.\n",
            "[1035.68s -> 1038.68s]  Porque tus compañeros también están.\n",
            "[1038.68s -> 1041.68s]  Que se hablen.\n",
            "[1041.68s -> 1044.68s]  Sí.\n",
            "[1044.68s -> 1047.68s]  El del logo.\n",
            "[1047.68s -> 1050.68s]  Básicamente.\n",
            "[1050.68s -> 1053.68s]  Hice un pequeño.\n",
            "[1053.68s -> 1056.68s]  Como prompt principal.\n",
            "[1056.68s -> 1059.68s]  Donde decía la tarea general.\n",
            "[1059.68s -> 1062.68s]  Y que me ayudara con 3 prompt.\n",
            "[1062.68s -> 1065.68s]  Me dio 3 prompt.\n",
            "[1065.68s -> 1068.68s]  Pero de ahí les hice algunas modificaciones.\n",
            "[1068.68s -> 1071.68s]  Para lograr estos resultados.\n",
            "[1071.68s -> 1074.68s]  Excelente.\n",
            "[1074.68s -> 1077.68s]  Listo.\n",
            "[1077.68s -> 1080.68s]  Me encantaba tu aproximación.\n",
            "[1080.68s -> 1083.68s]  Gracias.\n",
            "[1083.68s -> 1086.68s]  Ahora.\n",
            "[1086.68s -> 1089.68s]  Listo.\n",
            "[1089.68s -> 1092.68s]  Siguiente.\n",
            "[1092.68s -> 1095.68s]  Una chica.\n",
            "[1095.68s -> 1098.68s]  Se asustó.\n",
            "[1098.68s -> 1101.68s]  Es para compartir.\n",
            "[1101.68s -> 1104.68s]  Bajé la mano rápidamente.\n",
            "[1104.68s -> 1107.68s]  Porque la solución es similar a la de los compañeros anteriores.\n",
            "[1107.68s -> 1110.68s]  No hice de código.\n",
            "[1110.68s -> 1117.24s]  audio sino que pues nada, usé una solución en internet y cargué el vídeo, saqué el audio y\n",
            "[1117.24s -> 1127.52s]  eso fue lo que cargué mediante código, no lo hice y ya, y utilicé el generador de imágenes de Copilot\n",
            "[1127.52s -> 1133.68s]  pero creo que se está unido a de algún modo a Ale, entonces creo que son las únicas diferencias\n",
            "[1133.68s -> 1143.16s]  que tengo en mis respuestas. Pues está chévere, yo también hago lo mismo, pues cuando necesito\n",
            "[1143.16s -> 1151.64s]  rapidez, no tengo el código hecho, simplemente hay una aplicación en mi Mac y yo simplemente\n",
            "[1151.64s -> 1159.04s]  pongo el vídeo y le quito el audio, claro cuando son mil audios uno dice, bueno ahora sí me pegó la\n",
            "[1159.04s -> 1163.40s]  pela, no, pero es una solución de hecho muy práctica, hubiera sido interesante que lo\n",
            "[1163.40s -> 1171.28s]  mostrara, ya nos lo comentaste, entonces eso, bacano, pásalos los logos si quieres al grupo,\n",
            "[1171.28s -> 1180.28s]  vamos a votar, listo, Daniel Leonardo, adelante, muy cortico, todos por fin para que podamos\n",
            "[1181.64s -> 1186.92s]  tener la segunda y tercera hora, provechosas para la clase que les quiero mostrar varias cositas.\n",
            "[1189.04s -> 1198.16s]  Buenos días, no sé si ya están viendo mi pantalla, si, si ya se ve, ok, yo utilicé una\n",
            "[1198.16s -> 1204.28s]  solución para desearle al compañero, entonces pues como los vídeos estaban tan pesados lo que hice fue\n",
            "[1204.28s -> 1214.84s]  cargarlos directamente desde el drive, entonces lo que hice fue, no sé si ahí es, pero un momentico,\n",
            "[1215.08s -> 1225.36s]  ahí se están viendo, sí, entonces lo que hice fue utilizar una librería de Mopi para poder extraer\n",
            "[1225.36s -> 1231.76s]  el audio del vídeo, entonces pues cargué las librerías, cargué el de MobiEpi de videoclip,\n",
            "[1231.76s -> 1238.24s]  cargué la de OpenAI de Whisper directamente de Github y pues cargué Whisper, entonces en la línea\n",
            "[1238.24s -> 1244.84s]  de aquí lo que hice fue cargar, llamarle, pues para poder cargarlo del drive, aquí pues para estar\n",
            "[1244.84s -> 1253.20s]  seguro como había nombrado los vídeos y aquí creé la función para extraer el audio del vídeo, aquí\n",
            "[1253.20s -> 1259.48s]  lo que hace entonces es llamar donde está el vídeo y donde lo voy a guardar, lo guardaba directamente\n",
            "[1259.48s -> 1268.04s]  pues aquí, igual usé una T4 para que no, para que fuera rápido, hice pues en el vídeo 1, vídeo 2 y\n",
            "[1268.32s -> 1277.20s]  vídeo 3, luego hice la transcripción con el modelo de Whisper, utilicé el Medium porque pues no quería\n",
            "[1277.20s -> 1282.60s]  sacrificar la calidad, si usaba un Small sabía que no iba a ser tan preciso y un Large consumía\n",
            "[1282.60s -> 1288.60s]  demasiada máquina, entonces utilicé un Medium, normalmente se demoró por cada vídeo 8 minutos\n",
            "[1288.60s -> 1295.60s]  procesando para extraer el audio, entonces aquí lo que hago es que los guardo en un texto, tanto\n",
            "[1295.60s -> 1303.96s]  el primero, el segundo y el tercero, para no entendí o no supe cómo hacer el extraer los\n",
            "[1303.96s -> 1311.28s]  Keypoints, entonces busqué en ChatGPT, pues me va de, busqué ayuda en ChatGPT diciéndoles si había algún\n",
            "[1311.28s -> 1322.40s]  algún modelo ya entrenado en Hugging Face, entonces probé con varios, no fueron tan precisos, utilicé\n",
            "[1322.40s -> 1328.28s]  este último que era el T5, lo mismo tocaba hacer un, tocaba tokenizar porque como el texto era tan\n",
            "[1328.28s -> 1336.72s]  extenso entonces no, no corría bien, intenté con, se me olvidó el nombre del otro, bueno intenté con otro, con tres\n",
            "[1336.72s -> 1343.48s]  modelos diferentes y pues este fue el que mejor dio, pero de todas maneras no me funcionó bien, entonces\n",
            "[1343.48s -> 1349.24s]  aquí lo que hice fue lo mismo, que crear unas funciones para cargar los textos, dividir los textos, tokenizarlos\n",
            "[1349.24s -> 1359.04s]  y pues guardarlos en un, en un punto TXT, pero pues esta parte no, realmente no, no me funcionó bien,\n",
            "[1359.04s -> 1364.96s]  entonces yo creo que voy a corregir la tarea y lo voy a hacer directamente con ChatGPT, ya con los\n",
            "[1364.96s -> 1371.88s]  textos, los textos que ya había sacado para poder buscar los, los Keypoints.\n",
            "[1371.88s -> 1401.84s]  Bueno, tú liberaste un tema interesante que vamos a ver casi al final, no utilices ChatGPT, te recomiendo que uses y se los va a liberar a todos, es un modelo que salió hace como una semana tal vez, el GMA2, 2 billones, esperen a ver si lo encuentro acá, 2 billones de parámetros, es un modelo gratuito, es un modelo\n",
            "[1401.84s -> 1417.04s]  open source y que cabe en su celular, interesante, puedes hacer tu tarea con ese, ese es un modelo generativo tipo ChatGPT, pero no te toca pagar, es libre, es gratis, inténtelo a ver cómo le va y me cuenta.\n",
            "[1417.04s -> 1446.52s]  Ok, sí porque lo que yo intenté aquí con, o sea, extraer los Keypoints con, con un modelo de Hugging Face, hay demasiados, este decía que era como el que no tenía que, pues tenía más Tokens para poder extraer porque eran 8 mil, por lo menos de primer texto eran como 8 mil 900 y pico de palabras, entonces claro, tocaba tokenizar el texto como tal, entonces por ese lado grave y lo hice así, pero no, no me resultó bien, entonces creo que voy a hacer la corrección.\n",
            "[1446.52s -> 1449.52s]  ¿Cómo se llama Profe para tenerlo, para ver si lo puedo hacer por ese lado?\n",
            "[1476.52s -> 1505.52s]  Bueno, pues yo creo que ya les comenté todas las cosas que se leyeron esta semana de nuevos avances en Nia, pero yo creo que, pues igual les compartí el clásico, pero los cambios sí, si algo ahorita los hacemos con ChatGPT, bien la idea que tengo, pero intenten usar este modelo que les acabo de mandar y me dicen que resultados tienen, si lo lograron doblegar, doblegar saben qué significa, que haga lo que usted quiere que haga, entonces, pero me parece muy interesante tu solución porque es diferente de las demás,\n",
            "[1506.52s -> 1530.52s]  te pegaste la pela de ir a HuginFace, cargar el modelo, cargar el tokenizador, usar la inferencia, eso es súper importante, por una cuestión de costos, no quiere decir que los que hayan usado ChatGPT está mal, de hecho esa es la solución, digamos más directa, más óptima, pero que cuesta, entonces, espero que estas opciones están muy bien,\n",
            "[1530.52s -> 1538.52s]  y si no, pues ya le corrígelo y no hay problema, me lo mandas otra vez o me dices en el mismo correo, este es la versión corregida, listo.\n",
            "[1560.52s -> 1580.52s]  Entonces, de las imágenes tengo este para el de procesamiento de lenguaje natural, lo hice con Dalí, este para computer vision, y este para deep learning, y por aquí tengo el de serie de tiempo.\n",
            "[1581.52s -> 1594.52s]  Ok, listo, publicalos y miramos a ver cómo va esta votación, gracias. Daniel Leonardo, dale Juan.\n",
            "[1594.52s -> 1601.52s]  Buenos días para todos, ya les comparto mi pantalla.\n",
            "[1607.52s -> 1609.52s]  Listo, ahí lo están viendo, ¿sí?\n",
            "[1611.52s -> 1612.52s]  Ya se ve.\n",
            "[1613.52s -> 1636.52s]  Listo, bueno pues la solución al final digamos que también van a enfocar lo que ya los compañeros han presentado, para el tema de extraer el audio de los videos, aquí hice uso de audio segment, que es un módulo de la librería PyTub, que va muy enfocado a todo el tema de manipulación de archivos tipo audio.\n",
            "[1637.52s -> 1649.52s]  Y también hice uso de la librería OZ para poder hacer uso de la carpeta que el profe nos había compartido, y pues así poder leer como la carpeta y cada archivo que estaba contenido allí.\n",
            "[1650.52s -> 1664.52s]  Pues yo lo que hice básicamente fue crear un acceso directo desde esa carpeta a mi drive, y pues nada, a partir de la librería OZ, acceder a la misma y listar todos los archivos con extensión .mp4 que vienen siendo los videos.\n",
            "[1665.52s -> 1683.52s]  Y posteriormente pues a partir de un loop para cada uno de esos videos pues almacenados en la carpeta, aplicarle el tema de transformación del formato a cada uno de los videos, basándolo de un formato .mp4 a un formato .mp3.\n",
            "[1684.52s -> 1696.52s]  Con eso pues ya teníamos simplemente la extracción del audio, y finalmente pues lo exporté al directorio que ya había especificado en la parte superior de audio folder path.\n",
            "[1696.52s -> 1703.52s]  Entonces pues con ese loop, un poquito sencillito, pues se pudo generar todo el tema de la extracción de audio.\n",
            "[1704.52s -> 1713.52s]  Y posteriormente una vez que pues ya hice toda la extracción del audio, generé algo muy similar, pero para pues todo el tema de pasarlo a texto.\n",
            "[1713.52s -> 1728.52s]  Entonces aquí pues hice uso del modelo base, del modelo base pues porque es como el más, quizás es más óptimo, teniendo en cuenta pues la cantidad de parámetros que tiene, no es tan pesado.\n",
            "[1728.52s -> 1734.52s]  Y pues también manejé un enfoque similar a lo que había hecho para todo el tema de pues extraer el audio.\n",
            "[1734.52s -> 1747.52s]  Entonces listé pues todos los archivos de audio en la carpeta que previamente había creado, y le apliqué pues el respectivo modelo de Whisper, especificándole pues que fueran el lenguaje en español.\n",
            "[1747.52s -> 1752.52s]  Y pues finalmente guardé cada uno de los archivos en un formato .txt.\n",
            "[1752.52s -> 1767.52s]  Entonces teniendo en cuenta que eran tres videos, pues se generaron tres archivos de audio, y decidí generar pues un archivo .txt para cada uno, pues porque mi idea era pues pasarle el .txt al .shgpt para generar el respectivo resumen.\n",
            "[1767.52s -> 1775.52s]  No quería como generar uno solo pues quizás como por temas de pesos, entonces por eso decidí manejarlo así, particionado.\n",
            "[1775.52s -> 1782.52s]  Entonces el output pues vienen siendo los tres archivos .txt, que son los que tenemos acá en esta carpetica.\n",
            "[1782.52s -> 1791.52s]  Y por último lo que hice pues fue pasarle esos tres archivos .txt al .shgpt con este prompt que tenemos acá.\n",
            "[1791.52s -> 1802.52s]  Entonces le comenté todo esto pues dándole el contexto, que fue un experto con más de 10 años de experiencia y capacidad de sintetizar y pues también conocimiento de los conceptos.\n",
            "[1802.52s -> 1809.52s]  Y finalmente pues en cuanto a los puntos clave que me brindó pues fueron en total seis.\n",
            "[1809.52s -> 1827.52s]  Donde está pues diferencia entre aprendizaje automático y aprendizaje profundo, multiplicación de matrices como base del aprendizaje profundo, las aplicaciones del aprendizaje profundo, regularización y optimización en redes neuronales, la interpretabilidad de los modelos y por último las aplicaciones y avances recientes en el CAM.\n",
            "[1827.52s -> 1835.52s]  Ahí quizás fue como muy extenso en cada uno, pero pues digamos que en términos generales pues fue como un buen resumen.\n",
            "[1835.52s -> 1855.52s]  Y por último pues generé también pues un archivo main.py, pues de aquí consolidando pues cada uno de los pasos que hice en los notebooks anteriores, pues para que se genere como el pipeline completo pues de toda la parte de extracción de audio y posteriormente la extracción de, pues la generación de los archivos .txt.\n",
            "[1885.52s -> 1890.52s]  Y bueno, pues lo que yo creo que es muy importante de estas tareas no es quién lo hizo mejor, no.\n",
            "[1890.52s -> 1893.52s]  Lo importante es usted qué está aprendiendo del otro.\n",
            "[1893.52s -> 1897.52s]  Entonces fíjense que era algo que iba a decir al final.\n",
            "[1897.52s -> 1900.52s]  Ah, todavía hay alguien.\n",
            "[1900.52s -> 1903.52s]  No, entonces más bien lo digo al final, lo digo al final.\n",
            "[1903.52s -> 1914.52s]  Pero me gustó mucho tu idea, Juan, porque algo fundamental es que definiste los parámetros importantes de Whisper.\n",
            "[1914.52s -> 1918.52s]  Bueno, uno por lo menos vi el lenguaje, el idioma.\n",
            "[1918.52s -> 1926.52s]  Sí es cierto que Whisper detecta el idioma con un modelo por dentro, pero se puede equivocar.\n",
            "[1926.52s -> 1936.52s]  Entonces la forma de asegurarnos, de asegurarnos de que controlamos el modelo a la perfección es diciéndole cuál es el idioma.\n",
            "[1936.52s -> 1943.52s]  Y eso hace una serie de operaciones por dentro que nos permite asegurar que eso va a salir en español.\n",
            "[1944.52s -> 1950.52s]  Porque puede ser que en ese momento I want to speak in English and I don't care if you don't understand.\n",
            "[1950.52s -> 1954.52s]  Y entonces en esa transcription puede ser que eso quede mal, ¿cierto?\n",
            "[1954.52s -> 1959.52s]  Entonces muy importante esa control de esos parámetros.\n",
            "[1959.52s -> 1961.52s]  Entonces súper, súper chévere eso.\n",
            "[1961.52s -> 1968.52s]  Y también obviamente que lo hayas puesto en un main.py te permite reproducir el experimento.\n",
            "[1969.52s -> 1974.52s]  En notebooks también se puede hacer y uno puede subir notebooks a la nube, pero en cuestiones de deployment,\n",
            "[1974.52s -> 1982.52s]  obviamente el hecho de que lo tengas ya en un .py va a hacer que la compilación por consola sea muchísimo más rápida.\n",
            "[1982.52s -> 1986.52s]  Entonces te felicito, está súper completo.\n",
            "[1987.52s -> 1991.52s]  Nos dejaste compartir, entonces asumo que no quieres mostrarnos los logos.\n",
            "[1991.52s -> 1993.52s]  Entonces vamos con Marilyn.\n",
            "[1993.52s -> 1994.52s]  Adelante.\n",
            "[1995.52s -> 2002.52s]  Profe, sí los tengo en mi computador personal porque estoy desde otro, pero ya los comparto por el grupo.\n",
            "[2003.52s -> 2005.52s]  Dale, listo. Sin lío. No hay problema.\n",
            "[2005.52s -> 2007.52s]  Vale, vale. Gracias.\n",
            "[2008.52s -> 2015.52s]  Buenos días para todos. Bueno, fue mi trabajo. No estaba con buen deseo, pero yo hice básicamente,\n",
            "[2015.52s -> 2018.52s]  también utilice Python para hacer la conversión.\n",
            "[2019.52s -> 2027.52s]  O sea, extraje el audio de los tres videos, hice la transcripción, quedé con un archivo plano para cada uno de los tres.\n",
            "[2027.52s -> 2034.52s]  Y lo que yo hice fue al final, fue que me dejara un txt unificado de cada uno de los tres audios.\n",
            "[2035.52s -> 2042.52s]  Y pues mis puntos que estaba mirando en comparación con todos mis compañeros, ya mis puntos fueron más de conclusión y de tareas.\n",
            "[2043.52s -> 2050.52s]  Entonces como que teníamos que generar logos, que teníamos que utilizar aplicaciones, crear la comunidad en los chat,\n",
            "[2050.52s -> 2057.52s]  que teníamos ya una carpeta, las grabaciones de las clases, la preparación para las próximas clases,\n",
            "[2057.52s -> 2063.52s]  las evaluaciones y conclusiones fueron mis puntos claves en comparación con los compañeros que vi.\n",
            "[2064.52s -> 2074.52s]  Y para los logos, yo rápidamente utilice Microsoft Beam y pues los logos la verdad, mis descripciones como que eran muy simples,\n",
            "[2074.52s -> 2082.52s]  me daba cuenta cada vez que me gustaba, pero cada vez que me iba gustando más, ya me daba cuenta que ya no tenía créditos.\n",
            "[2083.52s -> 2088.52s]  Entonces estos fueron los que conseguí, ya los compartí por el chat.\n",
            "[2088.52s -> 2101.52s]  Perfecto. Bueno, pues yo puedo darles acceso a mi cuenta. El problema es que si todos la usan al tiempo, me van a quebrar, o sea, me van a quitar el acceso.\n",
            "[2101.52s -> 2110.52s]  Pero si alguien, alguno de ustedes necesita usar chat GPT, 4.0 o esas cosas, no tengo lío en darle las credenciales.\n",
            "[2110.52s -> 2115.52s]  Como cinco personal del DANE lo están usando, una gente de la fiscalía, entonces no tengo lío.\n",
            "[2115.52s -> 2124.52s]  Y me parece chévere, varias personas lo pueden usar al tiempo y pues DALI es en ese ámbito, esos 20 dólares que uno paga al mes, creo que vale la pena.\n",
            "[2124.52s -> 2132.52s]  No les estoy vendiendo el producto, les estoy diciendo vale la pena, porque uno puede usar en eso audio, video, pero en video todavía no.\n",
            "[2132.52s -> 2138.52s]  Puede usar audio, puede usar DALI, puede usar GPT personalizados, entonces me parece interesante.\n",
            "[2139.52s -> 2145.52s]  Creo que puede ser una herramienta de trabajo profesional muy interesante por 80 mil pesos al mes, creo que ahora está más barato.\n",
            "[2145.52s -> 2151.52s]  Pero si tienen alguna, o sea, claro, si necesitan, yo les puedo dar el acceso para que lo hagan.\n",
            "[2151.52s -> 2157.52s]  Si veo que me bloquean por alguna cuestión, pues les digo, venga, lo voy a usar en mi trabajo.\n",
            "[2157.52s -> 2164.52s]  Pero no se preocupen por eso, si no tienen créditos, inscríbanme, profes, que no tengo créditos, no sé cómo hacer listo mi tom.\n",
            "[2165.52s -> 2174.52s]  Porque esa es la idea. Me gustaría pagarles a todos en esta clase, pero pues no lo he presentado a los profesores, entonces no sé si ese rubro vaya a estar.\n",
            "[2174.52s -> 2181.52s]  Entonces pues les ofrezco más bien eso, como miren, este es, y hágale, y siga haciendo cositas.\n",
            "[2181.52s -> 2188.52s]  ¿Listo? Súper chévere Marilyn, me gusta, sobre todo la estructura, que pusiste el logo de la universidad, creo que fuiste de las únicas.\n",
            "[2189.52s -> 2198.52s]  Es muy importante, entonces, fíjense. Entonces les voy a dar la retroalimentación en estos cuatro minuticos, porque pues tengo que ir a por un cafecito.\n",
            "[2198.52s -> 2205.52s]  Miren que cada uno de ustedes, no sé si de pronto es moda, creo que yo fui de los primeros que introduje los notebooks en la universidad central.\n",
            "[2205.52s -> 2208.52s]  Me parece súper chévere que esa cultura siga.\n",
            "[2209.52s -> 2221.52s]  Hay que diferenciar, ¿no? Entre notebook y digamos en la tarea yo pedía un pipeline, con eso me refería un producto profesional, si lo podía lograr.\n",
            "[2221.52s -> 2231.52s]  Los notebooks no están mal, están chéveres, es la forma de prototipar que todos aprendemos así, ¿cierto? Los notebooks en colap y la goma.\n",
            "[2231.52s -> 2240.52s]  Pero a nivel profesional, si usted va a ser desarrollador, aquí puede haber personas que van a ser tomadores de decisiones o personas que se van a encargar del desarrollo.\n",
            "[2240.52s -> 2250.52s]  Eso no tiene ningún problema, ¿cierto? En MIT, incluso en los cursos que certifican oficialmente, por lo menos uno que yo hice, era usted tiene dos opciones de hacer tarea.\n",
            "[2250.52s -> 2254.52s]  Puede hacerlo a nivel de toma de decisiones o puede hacerlo full code.\n",
            "[2255.52s -> 2266.52s]  Yo no les puse restricción en esta porque quería saber con qué salían y me di cuenta que algunos un poquito más pequeños explican más, otros muy más código, explican también, pero se van mucho al tecnicismo.\n",
            "[2266.52s -> 2267.52s]  Eso es súper importante.\n",
            "[2267.52s -> 2268.52s]  Entonces, notebooks bien.\n",
            "[2269.52s -> 2283.52s]  Algo que no vi que sería interesante que lo revisáramos de una vez, sobre todo aprovechando que la guía del DANE va a salir muy pronto, la guía del ciclo de vida de modelos de ciencia de datos.\n",
            "[2283.52s -> 2285.52s]  No sé si les ha comentado por ahí.\n",
            "[2285.52s -> 2299.52s]  Desafortunadamente va a aparecer mi nombre, afortunadamente porque es para el país, pero pues la verdad yo aporté más bien desde mi criterio experto, pero no he escrito una sola línea de ese documento hasta el momento.\n",
            "[2300.52s -> 2316.52s]  Entonces pues no, pues digo que es desafortunado, pero algo que es muy importante tener en cuenta que se dijo y se propuso es la estandarización de los proyectos que ya van a estar muy cerca a la producción.\n",
            "[2316.52s -> 2333.52s]  Entonces, fíjense algo muy importante. Esto es algo que ustedes deberían adoptar. Los invito si no lo han hecho, que lo piensen en esta tarea, no en la siguiente.\n",
            "[2333.52s -> 2349.52s]  ¿Listo? Recuerden algo muy importante. Esto no me lo estoy inventando yo. Esto es algo que debe estar en su cabecita como estructuradores de proyectos. ¿Listo? Entonces, si, ¿tenen alguna pregunta?\n",
            "[2349.52s -> 2358.52s]  Roge, creo que se le posó la pantalla. Pensé que era yo, pero no. No sé si está interactuando y…\n",
            "[2358.52s -> 2364.52s]  Sí, es que este Google… ¿Ya están viendo? Sí.\n",
            "[2364.52s -> 2367.52s]  Sí, pero…\n",
            "[2367.52s -> 2369.52s]  ¿Listo? ¿Me avisan?\n",
            "[2373.52s -> 2375.52s]  Sí, el escritor y Roge. Sí.\n",
            "[2375.52s -> 2388.52s]  Sí, sí, sí, sí. Esa es la idea. Bueno, para que se vea más. Listo. Entonces, a lo que me refiero es esta estructura. El documento es que no se los puedo mostrar, pero en ese documento va a estar esto que les voy a explicar.\n",
            "[2389.52s -> 2401.52s]  Que tienen que tener una estructura en su proyecto, siempre. Estoy hablando del proyecto de la propuesta que ustedes me van a hacer, ya creo que en una semanita o dos.\n",
            "[2401.52s -> 2413.52s]  No, ahorita miramos el PDA. Entonces, es muy importante que usted siga esta estructura. Eso no me lo inventé yo, como les digo. Eso es algo que se sigue. Yo de pronto me inventé,\n",
            "[2413.52s -> 2425.52s]  ¿Ah? Que quiero poner documentos, porque soy gomelo para tener las referencias a la mano. Sí, docs. Pero algo que está sí o sí en los proyectos es una carpeta de input. ¿Cierto? Entonces, ¿qué iría en la carpeta de input? Pues, los videos.\n",
            "[2425.52s -> 2439.52s]  Una carpeta de models, si es necesario. A veces hay… Sí, pues si usan APIs, pues no. ¿Cierto? Si de pronto el resultado de la actividad es generar un modelo, pues sí.\n",
            "[2439.52s -> 2451.52s]  ¿Cierto? Entonces, este model puede que sí, puede que no. Notebooks, muy importante para prototipar, para hacer pruebas de concepto. Está súper bien, porque lo hayan hecho ahí no está mal.\n",
            "[2451.52s -> 2464.52s]  Es decir, si yo hago notebook todo el tiempo, una carpeta output donde están los resultados. Entonces, por ejemplo, había, vi a alguien que lo hizo, no sé si fue Leonardo, fue Juan.\n",
            "[2464.52s -> 2482.52s]  Pero, donde ponían el output los TXT de respuesta. Entonces, importantísimo separarlo. ¿Por qué? Porque la carpeta más importante de todas es la carpeta source, es ese, que es donde uno consigna el código fuente.\n",
            "[2482.52s -> 2495.52s]  Entonces, por eso decía la pregunta en el grupo, ¿qué tan relacionado está con programación orientada a objetos? Porque la idea de su proyecto, ojalá lleguemos ahí, es que usted pueda hacer un producto que pueda venderlo.\n",
            "[2495.52s -> 2503.52s]  O sea, que ustedes se vuelvan empresarios. Entonces, lo que están haciendo ahorita, que nos dedicamos a una obra básicamente, que nos va a tocar tomar diez minuticos ahorita a las ocho.\n",
            "[2504.52s -> 2515.52s]  Si ustedes me están vendiendo una idea y yo decidí comprar o hacerles algunas anotaciones. La idea es que ustedes vendan una idea el día de su proyecto final.\n",
            "[2515.52s -> 2528.52s]  Y el proyecto tiene que tener esa estructura, porque o si no, usted se va a volver loco, o loca, en el sentido que cada cosita nueva que yo le pida, la va a tocar volver a estructurar.\n",
            "[2528.52s -> 2536.52s]  Por ejemplo, si yo le digo, hicieron un notebook, yo le digo, no, pónganme un proceso intermedio de transformación de datos.\n",
            "[2536.52s -> 2550.52s]  Y usted ya tiene sus celditas así todas bonitas, uf, se vuelve anico, ¿cierto? Entonces, es muy importante ahorita, si no sabe, no importa, podemos repasarlo, la programación orientada a objetos es para eso, y la estructuración de proyectos digitales también.\n",
            "[2550.52s -> 2560.52s]  Importante el source, porque ahí podemos consignar todo el código fuente, pero de manera modular, para que pueda responder a ciertas necesidades.\n",
            "[2560.52s -> 2571.52s]  Y pues voy a terminar con un producto que se los voy a mostrar, fue algo que presentamos ayer a la alcaldía de Tunja, está en pañales, fue un prototipo, fase es menos diez,\n",
            "[2571.52s -> 2579.52s]  pero es algo que yo creo que ustedes ya, con lo que me mostraron hoy, podrían estar en capacidad de comenzar a construir, ¿listo?\n",
            "[2579.52s -> 2589.52s]  Entonces, muy importante, ya les muestro, muy importante esas carpetas, input, output, notebooks, source, donde tiene todo su código fuente,\n",
            "[2589.52s -> 2599.52s]  si no sabe cómo hacerlo, pues fácil, pregúntele al CGPT, pregúntele al profe, eso acá no nos varamos, pero sí muy importante que esta estructura quede, ¿por qué?\n",
            "[2599.52s -> 2608.52s]  Porque es importante, porque es que, por ejemplo, cuando uno tiene esta estructura, uno siempre puede referirse a caminos relativos,\n",
            "[2608.52s -> 2617.52s]  es decir, si yo tengo este cuaderno acá, y yo quiero referirme a datos que están en el input, no tengo que quemar las rutas como C, dos puntos,\n",
            "[2617.52s -> 2626.52s]  Jaime Ito Duque, dos puntos, universidad central 2024, no, yo puedo desde acá, decir, si yo pongo punto, punto, slash, me devuelvo una carpeta,\n",
            "[2626.52s -> 2632.52s]  punto, punto, slash, me devuelvo una carpeta, estoy en el input, y entonces puedo comenzar a hacer un proyecto autocontenido,\n",
            "[2632.52s -> 2642.52s]  de tal manera que si Paula se lo pasa a Juan, Juan es capaz de reproducir los resultados en su computador, eso.\n",
            "[2642.52s -> 2650.52s]  Por eso que les digo que es muy importante, entonces, los que hicieron notebook, está súper bien, no les va a bajar nota,\n",
            "[2650.52s -> 2657.52s]  pero vayanse acostumbrando un poquito a este tipo de organización, ¿cierto? Entonces, algunos estarán pensando,\n",
            "[2657.52s -> 2663.52s]  uy, hoy pucha, me faltó más de la mitad, seguramente, pero pues la idea es que a medida que vayamos avanzando en las clases,\n",
            "[2663.52s -> 2671.52s]  ustedes comiencen a hacer esto, y esto es parte de lo que va a aparecer en la guía nacional del DANE, del ciclo de vida de modelos de datos, ¿sí?\n",
            "[2671.52s -> 2679.52s]  Muy importante. Entonces, también los requirements, de tal manera que si el profe quiere reproducir su tarea, a ver si está bien,\n",
            "[2679.52s -> 2685.52s]  pues pueda instalar los paquetes que usted utilizó, sin tener que buscarlos, sin instalarlos solo,\n",
            "[2685.52s -> 2690.52s]  y entonces que ahí estén consignadas todas las librerías que se le requieran usar.\n",
            "[2690.52s -> 2696.52s]  Entonces, ¿cómo así que es un requirements, profe? No me vuelva loco. Entonces, pues eso seguramente lo vamos a estar repasando\n",
            "[2696.52s -> 2704.52s]  a medida que veamos cositas de redes neuronales de deep learning. Bueno, eso es básicamente, les voy a mostrar,\n",
            "[2704.52s -> 2709.52s]  yo creo que vamos a tomar 8 y 10, y de 8 y 10, 8 y 20, para poderles mostrar las dos cosas.\n",
            "[2709.52s -> 2715.52s]  Entonces, primero la solución de la tarea. Entonces, había muchas formas de solucionar la tarea, como se dan cuenta,\n",
            "[2715.52s -> 2719.52s]  y qué pena mostrar las cosas del trabajo, hay muchas formas de solucionar la tarea.\n",
            "[2719.52s -> 2731.52s]  Una en particular, es, por ejemplo, la que hizo una de las chicas, que era, no, pues yo tengo solo 3 videos,\n",
            "[2731.52s -> 2740.52s]  pues, ¿qué más? Pues, puedo hacer una aplicación, en este caso yo hice una aplicación, y yo en esa aplicación puedo colocar,\n",
            "[2740.52s -> 2750.52s]  por ejemplo, un video, este creo que era uno de los videos, ah, sí, este es la clase anterior, una de las horas,\n",
            "[2750.52s -> 2762.52s]  y yo acá le puedo decir, detach audio, elimino esto, y simplemente, otra vez, detach audio, claro, profe, no tengo más,\n",
            "[2762.52s -> 2770.52s]  ah, sí, es cierto, entonces, cada uno, fíjense, usa los recursos que tienen disponibles para analizar un problema, siempre.\n",
            "[2770.52s -> 2778.52s]  ¿Listo? Entonces, esta es una de las soluciones, no es la solución modular que muchos mostraron,\n",
            "[2778.52s -> 2784.52s]  si hay librerías, ya como saben que las tienen, entonces, yo entre hoy y mañana voy a subir una solución general,\n",
            "[2784.52s -> 2789.52s]  pero esta es una solución creativa, entonces uno dice como, bueno, pues, solo tengo 3 videos, no importa,\n",
            "[2789.52s -> 2796.52s]  ¿qué me demoro más? ¿Escribiendo el código? ¿Buscando el internet? O simplemente diciendo, bueno, pues, hagamos un share,\n",
            "[2796.52s -> 2805.52s]  acá simplemente le voy a decir solo audio, listo, next, y lo pongo en el escritorio, listo, se puso acá, se puso a exportar,\n",
            "[2805.52s -> 2813.52s]  entonces espero un poquito, y luego ese audio sí se lo introduzco a Whisper, algo que sí vi que, pues no, no va a ser nombres obviamente,\n",
            "[2813.52s -> 2821.52s]  pero cuidado, Whisper solo recibe audios, por ahí vi una tarea que decía que estaban insertando directamente el video,\n",
            "[2821.52s -> 2829.52s]  entonces toca saber muy bien las herramientas que estoy usando para lo que lo estoy usando, entonces, ojo ahí.\n",
            "[2829.52s -> 2837.52s]  Lo otro que vi es que, bueno, hasta donde tenía entendido el Whisper, es un modelo que solo recibe 30 segundos de audio,\n",
            "[2838.52s -> 2849.52s]  entonces, ¿cómo hicieron? De pronto la API la mejoraron, ya recibe, bueno, el modelo de pronto ya lo mejoraron y recibe más de 30 segundos, puede ser,\n",
            "[2849.52s -> 2855.52s]  la pregunta que no les hice y que quisiera que pusieran en el chat es ¿cuánto se demoraron en transcribir uno de los videos?\n",
            "[2855.52s -> 2863.52s]  La solución que yo les propongo y que les puse en una pista en, que les puse en una pista en la semana fue,\n",
            "[2863.52s -> 2871.52s]  será que hay otra librería que nos permita, primero, agilizar tiempos y segundo, poner audios más largos,\n",
            "[2871.52s -> 2876.52s]  y por ahí hay una librería que se llama Faster Whisper.\n",
            "[2876.52s -> 2882.52s]  Esto se los digo no porque les diga, oye, lo que hicieron está mal, no, esto es una herramienta que yo les digo,\n",
            "[2882.52s -> 2888.52s]  venga, se los pongo a su consideración, en este curso pues me da el privilegio de estar con un experto en inteligencia artificial\n",
            "[2888.52s -> 2893.52s]  que les puede ayudar a encontrar esas herramientas profesionales que pueden potenciar.\n",
            "[2893.52s -> 2899.52s]  Entonces la base es Whisper, uso Whisper, claro, le dije Whisper, OpenAI, excelente, uso la API, no importa si la uso,\n",
            "[2899.52s -> 2906.52s]  si la uso directamente en HuginFace, ¿cuál es la diferencia entre usar la API o usar el modelo si era abierto?\n",
            "[2906.52s -> 2913.52s]  Pues lo que pasa es que si usas el abierto necesitas tener recursos computacionales, ¿cierto?\n",
            "[2913.52s -> 2919.52s]  que con la API da gratis, si usas la API te toca pagar de pronto un centavo de dólar,\n",
            "[2919.52s -> 2926.52s]  pero tienes el resultado en muy poco tiempo, con la misma calidad y estás usando los computadores de OpenAI.\n",
            "[2926.52s -> 2932.52s]  Entonces hay dos cositas, ¿no? La solución que yo propongo es una intermedia y es no vamos a pagar,\n",
            "[2932.52s -> 2938.52s]  y Faster Whisper, para los que sepan de programación, para los que no, no importa,\n",
            "[2939.52s -> 2952.52s]  Faster Whisper es una tecnología, es una iniciativa que se creó a partir del hecho de que se quiere que todos los modelos de inteligencia artificial\n",
            "[2952.52s -> 2958.52s]  sean capaces de ser ejecutados en un celular, en un Tamagotchi, en cualquier dispositivo,\n",
            "[2958.52s -> 2964.52s]  porque hacia allá es donde vamos, si ustedes se vieron la película Hair, si no se la han visto, véansela, es excelente,\n",
            "[2964.52s -> 2972.52s]  ahí se pone de entrevista que nosotros en un futuro muy cercano, y yo lo veo de pronto el otro año,\n",
            "[2972.52s -> 2978.52s]  vamos a tener nuestros asistentes personales como Jarvis de Iron Man en nuestros computadores,\n",
            "[2978.52s -> 2982.52s]  en nuestros dispositivos y que nos estén todo el tiempo diciendo y recomendando cosas.\n",
            "[2982.52s -> 2987.52s]  Entonces esta era una iniciativa a través de una librería que se llama CTranslate2,\n",
            "[2987.52s -> 2996.52s]  que lo que hace es transformar el código Python de los modelos Transformer, en este caso el Whisper, en código C++,\n",
            "[2996.52s -> 3004.52s]  lo cual hace que, para los que no sepan, el C++ es un lenguaje de programación que es muy cercano al lenguaje máquina,\n",
            "[3004.52s -> 3009.52s]  por lo tanto es muy rápido, y además cuantizaron, que significará cuantizar, cierto,\n",
            "[3010.52s -> 3018.52s]  el modelo, de tal manera que se podía tener la misma precisión que usar el grandote de float16,\n",
            "[3018.52s -> 3028.52s]  con solamente 8 o 16 o una precisión mixta, una cuantización de 8 bits, y eso mejora los tiempos.\n",
            "[3029.52s -> 3039.52s]  Entonces, por ejemplo, acá está una comparación que es Faster Whisper, con una muestra que se tenía de 13 minutos de audio,\n",
            "[3039.52s -> 3045.52s]  usando cada una de las especificaciones, con el OpenAI Whisper que la mayoría usó,\n",
            "[3045.52s -> 3052.52s]  se demora 4 minutos en transcribir los 13 minutos, con Faster Whisper se demora 59 segundos.\n",
            "[3053.52s -> 3057.52s]  Entonces, interesante, ¿no? Use Faster Whisper.\n",
            "[3057.52s -> 3066.52s]  Este se los comento porque el año pasado a la Registraduría Nacional del Estado Civil le hicimos un tablero de alertas tempranas de delitos preelectorales,\n",
            "[3066.52s -> 3075.52s]  y nos dimos cuenta que, pues, ¿qué pasa? Pues están los territoriales, en Chocó, en toda la nación,\n",
            "[3075.52s -> 3081.52s]  y entonces tienen que reportar, pero no te reportaban con hojita, y entonces el camioncito se demoraba,\n",
            "[3081.52s -> 3086.52s]  a veces no llegaba, entonces la solución fue, puse el audio, y el audio lo vamos a transcribir.\n",
            "[3086.52s -> 3093.52s]  Pero como eso puede ser una aplicación que usan miles de personas, entonces usen una solución que no requiera una GPU,\n",
            "[3093.52s -> 3098.52s]  porque las GPU realmente jamás llegaron a la registraduría, acá les estoy dando el chisme, ¿no?\n",
            "[3098.52s -> 3104.52s]  Pero entonces esta solución de Faster Whisper nos permite, de pronto también acá uno puede usar los modelos Medium,\n",
            "[3104.52s -> 3112.52s]  no tener de pronto el mejor modelo, pero no necesita tanto recurso computacional, lo cual hace que muchas personas puedan acceder al tiempo.\n",
            "[3112.52s -> 3114.52s]  Faster Whisper es una solución muy interesante.\n",
            "[3114.52s -> 3120.52s]  Entonces se los voy a dejar acá también, yo creo que es importante que ustedes tengan en sus herramientas,\n",
            "[3120.52s -> 3124.52s]  sobre todo porque esta tarea no fue una tarea inocente, fue una tarea a propósito.\n",
            "[3124.52s -> 3131.52s]  Y fue una tarea a propósito porque lo primero que se tiene que solucionar en su vida es que se quite todo el trabajo manual que tiene, ¿sí?\n",
            "[3131.52s -> 3138.52s]  Yo odio las tareas manuales y siempre que haya una herramienta que nos ayude, yo la voy a difundir.\n",
            "[3138.52s -> 3144.52s]  Por ejemplo, en mi empresa nosotros usamos Faster Whisper para hacer generación automática de actas,\n",
            "[3144.52s -> 3151.52s]  no para reemplazar el trabajo de la persona, la persona sigue haciendo las actas, lo que pasa es que ya se demora menos tiempo y puede poner key points.\n",
            "[3151.52s -> 3160.52s]  Este key points era una cáscara porque la compañera que expuso tiene mucha razón, una de las compañeras que expuso tiene mucha razón,\n",
            "[3160.52s -> 3168.52s]  que es un key point, un key point puede significar muchas cosas, puede ser a nivel de muchas observaciones uno puede tener.\n",
            "[3168.52s -> 3175.52s]  Yo esperaba que quizás alguien, pero pues no, pueden corregirlo de todas maneras, que pusiera key points dependiendo de temáticas.\n",
            "[3175.52s -> 3180.52s]  Si a nivel de conclusiones, profe, a nivel de lo que se habló de los perfiles, estos son los key points,\n",
            "[3180.52s -> 3185.52s]  pero a nivel de lo que habló de inteligencia artificial, estos son los key points, pero a nivel de las tareas, ¿cierto?\n",
            "[3185.52s -> 3192.52s]  Entonces depende de lo que esté preguntando. Si me pareció curioso que solo nos mostraron el DANE, pues también estoy en fiscalía, por favor no lo difundan mucho.\n",
            "[3192.52s -> 3205.52s]  Entonces, lo interesante del prompt, que siempre les va a ser la tarea de lo general, es encontrar los detalles específicos que generan valor.\n",
            "[3205.52s -> 3213.52s]  Entonces, por eso es que uno se demora tanto promteando, porque uno quiere extraer esa pequeña palabra que hizo cambiar el humor de esta persona,\n",
            "[3213.52s -> 3219.52s]  que hizo cambiar todo el rumbo de la conversación o de la charla, ¿cierto? Eso es a lo que quiero que vayan apuntando.\n",
            "[3219.52s -> 3224.52s]  Entonces, está perfecto todo lo que hicieron, me parece fabuloso, lo podemos seguir potenciando.\n",
            "[3224.52s -> 3237.52s]  Entonces, Faster Whisper era la herramienta, digamos que yo digo, listo, el que la encontró, súper, y lo iba a eximir del primer parcial, digamos, parcial que vamos a planear,\n",
            "[3237.52s -> 3249.52s]  pero pues obviamente es para la casa, pero no pasa nada, ¿cierto? ¿Cómo encontré a Faster Whisper? Cacharreando, mirando, oiga, necesito más, menos recursos, más con menos, y así.\n",
            "[3249.52s -> 3256.52s]  Entonces, que lo hayan encontrado, no, pues es una mina de oro. Entonces, se los comparto para que lo usen en su vida profesional.\n",
            "[3257.52s -> 3268.52s]  Hay incluso uno mejor, pero que yo le tengo un poquito menos de fiabilidad, pero lo he usado, y se llama Incendly Fast Whisper, y ese sí ya se va al extremo,\n",
            "[3268.52s -> 3282.52s]  en que pueden con ciertas características de GPUs transcribir 150 minutos en 98 segundos. Bueno, es otra herramienta, úsenla con precaución, con responsabilidad,\n",
            "[3282.52s -> 3295.52s]  pero lo más importante es, muchachos, por favor, evalúen sus resultados. No solamente es key point, si mire, profe, tome sus key points, o yo no sé qué transcribió, pero lo transcribí, ¿cierto?\n",
            "[3295.52s -> 3306.52s]  No, lo importante es, como ustedes están en la clase, por lo menos tener un muestre, o unos chunks, unos pedacitos, decir, oiga, sí, esto lo dijo el profe, realmente, ¿cierto?\n",
            "[3306.52s -> 3315.52s]  Ninguno me mostró, ninguna me mostró la transcripción, parte es que usted diga, profe, recuerda que usted dijo esto, esto y esto, y mire, aquí está la transcripción, ¿cierto?\n",
            "[3315.52s -> 3329.52s]  Entonces, eso es algo clave, porque es que ocurre en algunas entidades, en algunas que trabajo, que si usan la tecnología, por ejemplo, no sé, vamos a hacer un lago de datos,\n",
            "[3329.52s -> 3341.52s]  y entonces hacen el lago de datos, y usan Whisper, entonces, no, vamos a transcribir todas las entrevistas que hagamos en la empresa, listo, y entonces le pagan a una empresa, y entonces, no, sí, hicimos Whisper,\n",
            "[3341.52s -> 3350.52s]  y lo estamos consignando en el lago de datos de la entidad, y uno les pregunta, oiga, usted está verificando que esa información sí está acorde, por eso este key point, ¿qué?\n",
            "[3350.52s -> 3356.52s]  Cambie un poquito donde decía que yo, decía que el point de la educación es importante, es muy importante el tema, ¿cierto?\n",
            "[3356.52s -> 3371.52s]  Si usted cambia la transcripción real, o sea, imagínense, yo en el Dane, entonces, ahí sí viene el key point, yo hago transcripción de encuestas agropecuarias que vienen de entrevistas a hablar,\n",
            "[3372.52s -> 3378.52s]  entonces, ¿a cuánto está la papa, señor? 2,8 lucas, ¿a cuánto trajo de papa? ¿De dónde viene? ¿Cuál es su plan?\n",
            "[3378.52s -> 3388.52s]  Imagínense, si yo no, si ustedes, digamos, ustedes son del Dane, ustedes se están trajendo conmigo en el equipo, si nosotros no validáramos lo que transcribe Whisper,\n",
            "[3389.52s -> 3400.52s]  un solo valor que se cambie del precio de la papa, sistemáticamente, mueve la economía del país, ¿cierto?\n",
            "[3400.52s -> 3411.52s]  Entonces, por eso es que hay que regular la inteligencia artificial, por eso es que estos modelos de deep learning toca supervisarlos, el humano no va a quedar por fuera,\n",
            "[3411.52s -> 3421.52s]  de hecho tiene una tarea mucho más importante ahorita que es verificar que esos resultados tengan sentido, entonces, eso yo sé que no lo hicieron muchos, o muchas,\n",
            "[3421.52s -> 3429.52s]  porfa, revisen, revisen partes de la transcripción, yo sé que son tres horas, y de hecho, parte de la tarea de la próxima es, vuelvanlo a hacer, ¿cierto?\n",
            "[3429.52s -> 3439.52s]  Pero ya como tienen su código, ¿cierto? Entonces ya lo pueden hacer muy sencillo, ejecuto, ya los que tienen transcripción de video, no más es que pongan play a todas las celdas,\n",
            "[3439.52s -> 3448.52s]  o si lo hacen como les estoy diciendo, orientado a objetos, o el chico que tenía el mail no más es ejecutar, entran los videos, salen los keypoints,\n",
            "[3448.52s -> 3458.52s]  pero tienen que hallar de alguna manera, y la tarea, una de las partes de la tarea que va a poner es, y esto es una pregunta abierta,\n",
            "[3458.52s -> 3470.52s]  ¿Cómo va a medir? ¿Cuál va a ser su métrica para medir que la transcripción, de alguna manera está bien hecha?\n",
            "[3470.52s -> 3479.52s]  Suena complicado, piénsenlo, y la otra sesión vienen con propuestas, no, pero a mí se me ocurre que medir de esta manera,\n",
            "[3479.52s -> 3488.52s]  porque, o, no, es que yo me puse a mirar un video de usted, y yo transcribí a mano, y luego vi esa parte, y comparé, y listo.\n",
            "[3488.52s -> 3495.52s]  Bueno, pero traten de hacerlo matemáticamente hablando, ¿cierto? O sea, alguna fórmula, o alguna metodología, ¿cierto?\n",
            "[3495.52s -> 3505.52s]  No sé, profe, yo cogí NLTK, Spicy, quité los tokens, lo tokenicé, y entonces comparé los tokens uno a uno, y me dio esto.\n",
            "[3505.52s -> 3512.52s]  Porque recuerden que Whisper, eso lo vamos a ver más adelante, Whisper tiene una métrica, cuando se entrenó, que se llama el WER,\n",
            "[3512.52s -> 3522.52s]  el Word Error Ratio, o el error del radio de palabra, y ahí se encuentran tres métricas importantísimas.\n",
            "[3522.52s -> 3537.52s]  El número de sustituciones, o sea, que no dijo Transformers, perdón, esa no me sirve, que no dijo, por ejemplo, audio, sino que dijo, music, ¿cierto?\n",
            "[3537.52s -> 3552.52s]  Sustituyó, la segunda es, omitió, es decir, lo quitó, y la tercera es, añadió, lo que se conoce actualmente como alucinación.\n",
            "[3552.52s -> 3563.52s]  Entonces, Whisper, por más large que sea, o small, tiene unas métricas de precisión, de ese WER.\n",
            "[3563.52s -> 3571.52s]  Entonces, interesante, tampoco lo vi, tampoco es que, tranquilos, no les vaya bien la tarea, pero hubiera sido muy interesante que alguien me dice,\n",
            "[3571.52s -> 3582.52s]  profe, listo, píquenme. Yo me puse a leer el paper de Whisper. El paper de Whisper es un paper científico que salió hace mucho tiempo, en el 2016, 17,\n",
            "[3583.52s -> 3587.52s]  por ahí va la cuestión, perdón, en el 2020, por ahí va la cuestión.\n",
            "[3587.52s -> 3595.52s]  Y entonces, profe, yo me puse a leer el paper de Whisper para saber por qué es que yo transcribí, yo no sé.\n",
            "[3595.52s -> 3604.52s]  O sea, dato importante, profe, se entrenó con 680 mil horas, ahí está, 680 mil horas de audios transcritos,\n",
            "[3604.52s -> 3609.52s]  es decir, contrataron a un ejército de gente para transcribir audios, y luego hicieron una red neuronal,\n",
            "[3609.52s -> 3617.52s]  que en la segunda hora, ahorita vamos a ver, ¿cierto?, los principios básicos, porque la votación dijo, vamos a ver, redes neuronales,\n",
            "[3617.52s -> 3625.52s]  no tanto desde cero, pero sí, pues vamos a hacer un pequeño cuisecito de, no calificable, a ver cómo están en eso,\n",
            "[3625.52s -> 3628.52s]  a ver si realmente entendemos lo que es una red neuronal.\n",
            "[3628.52s -> 3636.52s]  Y entonces uno dice, no, vea, profe, ah, mire, es que, ¿dónde están los puntos?, mire, ah, profe, es que mire que el Whisper es un transformer,\n",
            "[3637.52s -> 3645.52s]  y hace unos espectrogramas, es una parte visual, y luego entonces me muestran, ah, mire, profe, mire, es que en estos datasets,\n",
            "[3646.52s -> 3653.52s]  se tiene un WER de tanto, ¿cierto?, acá también hay otra métrica, el RER,\n",
            "[3653.52s -> 3660.52s]  y entonces me dice, profe, de transcripción, yo estoy esperando que en promedio, Whisper se haya equivocado tanto,\n",
            "[3660.52s -> 3665.52s]  por lo tanto, en los keypoints pueda confiar tanto, una decisión importante, ¿cierto?,\n",
            "[3665.52s -> 3672.52s]  a su gerente, o si usted es gerente, usted tiene que tomar esa decisión, y no puede decir, no, pues es que yo transcribí ya,\n",
            "[3672.52s -> 3678.52s]  entonces no lo tengo como un regaño, sino como una oportunidad de mejor, ¿cierto?, importante,\n",
            "[3678.52s -> 3684.52s]  entonces si yo leo el paper, no tiene que leerlo todo, mándeselo a Chet GPT, dígale, oiga, dígame los keypoints de este paper,\n",
            "[3684.52s -> 3690.52s]  porque entiendo, ¿cierto?, todo, pantallacito, ¿qué significa esa vaina?, que es un FLIRS, que es un FLOR, ¿cierto?,\n",
            "[3691.52s -> 3697.52s]  ah, Whisper también puede traducir, no sabía, que interesante, que otras tareas podría hacer Whisper por nosotros,\n",
            "[3698.52s -> 3705.52s]  entonces es más como invitarlos, a que, si está excelente lo que hicieron, me parece fabuloso,\n",
            "[3705.52s -> 3710.52s]  quiere decir que la maestría en analítica está funcionando, porque los ponen a hacer código y lo están haciendo,\n",
            "[3711.52s -> 3716.52s]  seguramente soportado por Chet GPT, pero están entendiendo lo que están haciendo, yo creería que en un 85%,\n",
            "[3717.52s -> 3722.52s]  pero que interesante es ir más allá, ¿cierto?, que es Whisper, que es esa vaina, o sea, si listo, es un modelo,\n",
            "[3722.52s -> 3729.52s]  pero ¿y cómo funciona?, ¿cómo se creó?, ¿por qué lo hicieron?, ¿cierto?, les digo, este Whisper es la herramienta que está incluida\n",
            "[3729.52s -> 3733.52s]  en casi todos los sistemas de reconocimiento, es el que está detrás de Teams, cuando usted, Teams,\n",
            "[3733.52s -> 3740.52s]  todos los que trajan acá, sus empresas tienen Teams, el modelo que transcribe el audio es Whisper,\n",
            "[3741.52s -> 3747.52s]  es una versión de Whisper, que afinaron en Microsoft, entonces, que interesante pues todo esto, ¿cierto?,\n",
            "[3748.52s -> 3752.52s]  y por qué están todos esos idiomas y por qué es tan bueno en español, eso será una cosa interesante de saber,\n",
            "[3753.52s -> 3760.52s]  bueno, anécdotas para el futuro, y les voy a mostrar, pues, les voy a motivar antes de que nos vayamos al descansito,\n",
            "[3760.52s -> 3772.52s]  lo que pueden lograr, si se ponen las pilas y comienzan a estructurar sus proyectos o sus tareas,\n",
            "[3773.52s -> 3778.52s]  como les dije, que se estructuran con esa organización de carpetas, ahorita les mando, digamos, un ejemplo,\n",
            "[3779.52s -> 3787.52s]  pero que podemos lograr, entonces, disculpen, yo busco la aplicación, la tenía por ahí,\n",
            "[3787.52s -> 3795.52s]  pero creo que ya no la, no la va a tener, me toca iniciarla, me toca ir al puerto,\n",
            "[3796.52s -> 3801.52s]  entonces, disculpenme, yo acá me llevo este, este cosito, y bueno, esto fue algo,\n",
            "[3802.52s -> 3809.52s]  algo que le presentamos al alcalde de Tumjam, como ayer, para que las personas se motiven a que deben\n",
            "[3810.52s -> 3815.52s]  disminuir su trabajo manual para hacer cosas más interesantes, no hacer cosas,\n",
            "[3816.52s -> 3822.52s]  solución de PQRs, ¿cierto?, aplicación de Whisper interesante, esto es un producto que ya se puede vender,\n",
            "[3823.52s -> 3831.52s]  si alguien en esta clase quisiera hacer un proyecto de esta índole, yo le garantizo, pues no le puedo garantizar nada,\n",
            "[3831.52s -> 3837.52s]  pero yo tengo mucha fe en que este tipo de productos, primero, le va a dar mucho dinero,\n",
            "[3838.52s -> 3843.52s]  y segundo, le va a abrir muchas puertas en todo el estado y en toda la parte privada,\n",
            "[3844.52s -> 3847.52s]  entonces, ¿pa' qué sirve Whisper?, o al final, ¿pa' qué sirve Faster Whisper?,\n",
            "[3848.52s -> 3854.52s]  es que yo pueda hacer cosas como estas, ¿cierto?, es como, hola, me llamo Daniel Montenegro,\n",
            "[3854.52s -> 3870.52s]  mi cedulación es 1032 438 34, mi celular es 313 200 86 46, y quisiera denunciar al alcalde del municipio de Tenjo,\n",
            "[3871.52s -> 3877.52s]  porque le da contratos a sus amigos, a mí no me da nada, y está cometiendo posible peculado, por favor, ayúdenme.\n",
            "[3877.52s -> 3884.52s]  Entonces, una aplicación real de Whisper, es bien interesante, es, podemos transcribir en tiempo real, ¿sí?,\n",
            "[3885.52s -> 3891.52s]  y luego mandarlo, entonces fíjense que, algo en común de su tarea y esto, es que justamente es usar Whisper,\n",
            "[3892.52s -> 3896.52s]  y algún modelo generativo, ChatGPT si quiere, pero hay muchos, ¿cierto?\n",
            "[3897.52s -> 3905.52s]  Entonces, con muy poco esfuerzo, pero esfuerzo muy guiado, usted puede llegar a hacer cosas como esta,\n",
            "[3905.52s -> 3912.52s]  donde ya no necesitamos que la persona vaya y radique ese PQR por allá, de manera manual,\n",
            "[3913.52s -> 3917.52s]  sino que incluso en un enfoque inclusivo, en el cual hay la redundancia diferencial,\n",
            "[3918.52s -> 3926.52s]  incluso personas que no saben leer ni escribir, podrían acceder al sistema de peticiones, quejas, reclamos y denuncias, ¿cierto?\n",
            "[3927.52s -> 3935.52s]  Entonces, es bien interesante esto, porque justamente, fíjense que su tarea, les abre las puertas a hacer este tipo de aplicaciones,\n",
            "[3936.52s -> 3941.52s]  y solamente hay dos tecnologías involucradas ahí, Whisper, ChatGPT, ¿cierto?\n",
            "[3942.52s -> 3949.52s]  Entonces, piensen, si les gusta este proyecto o similares, les estoy botando ideas para sus propuestas,\n",
            "[3950.52s -> 3955.52s]  esto es algo que se puede hacer en una semana, pues es decir, luego hay que uno ya, pues sí, ¿cierto?\n",
            "[3956.52s -> 3961.52s]  Comienza a aprender con ChatGPT cositas, qué es un front, qué es un back, cómo hago tal, cómo pongo aquí este loguito,\n",
            "[3962.52s -> 3968.52s]  pero fíjense que con dos tecnologías súper básicas que ustedes ya hicieron en su tarea, pueden lograr algo como esto, ¿cierto?\n",
            "[3969.52s -> 3975.52s]  Y además, pues para los que les gusta el back, pues obviamente, esto parece magia,\n",
            "[3975.52s -> 3979.52s]  pero esto es lo que, cosas de las que vamos a aprender en esta asignatura,\n",
            "[3980.52s -> 3986.52s]  y es cómo efectivamente usar los modelos de lenguaje para que a partir de los textos yo pueda obtener,\n",
            "[3987.52s -> 3993.52s]  a partir de un JSON, que realmente es un registro eventualmente de una base de datos, los datos de manera automática, ¿cierto?\n",
            "[3994.52s -> 3999.52s]  Entidad que responde, una explicación, qué tipo de QR es, ¿cierto?\n",
            "[4000.52s -> 4004.52s]  Y si es una FAQ o no es una FAQ, ese tipo de cositas, ¿listo?\n",
            "[4005.52s -> 4010.52s]  Y es para que se animen, entonces yo creo que tomémonos, ah bueno, y el código, ¿no?\n",
            "[4011.52s -> 4015.52s]  Sí, porque, profe, pero usted nos muestra esto, pero, ¿y el código qué? ¿Cómo así? No le creo que sea modular.\n",
            "[4016.52s -> 4030.52s]  Fíjense, la estructura, input, notebooks, source, audios, misma cosa.\n",
            "[4030.52s -> 4035.52s]  Y la única manera de trabajar en equipo, que estoy trabajando acá con un compañero caleño, es eso,\n",
            "[4036.52s -> 4040.52s]  que esté organizado, que se distribuyan tareas, pero que se conserve esa estructura.\n",
            "[4041.52s -> 4047.52s]  Entonces, muy interesante, y que además sea, pues, obviamente, pues, muy modular,\n",
            "[4048.52s -> 4053.52s]  o sea, que yo tenga una clasecita, que tenga un objeto, que tenga un constructor,\n",
            "[4054.52s -> 4057.52s]  que tenga unos atributos, que tenga unas funcionalidades, de tal manera que si yo tomo ese código\n",
            "[4057.52s -> 4064.52s]  y me lo descargo en mi computador, lo puedo ejecutar sin tener que tener nada, pues, adicional,\n",
            "[4065.52s -> 4070.52s]  porque por ahí también deben estar esos requirements que, bueno, dependiendo, ¿cierto?\n",
            "[4071.52s -> 4075.52s]  Hay personas que los hacen de una manera o de otra, que dependen si necesitamos,\n",
            "[4076.52s -> 4080.52s]  siempre me toca regañar al caleño por haberlo puesto así, tal vez.\n",
            "[4080.52s -> 4087.52s]  Entonces, fíjense, con muy poquito, ustedes están a muy poquito de hacer este tipo de cosas.\n",
            "[4088.52s -> 4095.52s]  Entonces, bueno, vámonos a un descansito, acá como no es por hora, entonces, podemos quedarnos las tres horas,\n",
            "[4096.52s -> 4104.52s]  lo que voy a hacer es que voy a parar la grabación, listo, y nos vemos en diez minuticos para, ahora sí, comenzar la clase.\n",
            "[4105.52s -> 4114.52s]  Entonces, yo no sé cómo puedo parar acá, pero creo que sí, yo le pongo como detener grabación,\n",
            "[4115.52s -> 4121.52s]  creería que se para detener grabación, y luego hacemos la segunda parte y la tercera parte.\n",
            "[4122.52s -> 4123.52s]  ¿Veste que usen este link, verdad?\n",
            "[4124.52s -> 4128.52s]  Sí, sí, sí, este como ya es de la Universidad Central, ya podemos grabar, ya podemos tener las tres horas,\n",
            "[4129.52s -> 4131.52s]  pero paro la grabación para que no sea solo un video.\n",
            "[4131.52s -> 4133.52s]  ¿Te pongo el cronómetro?\n",
            "[4134.52s -> 4135.52s]  Porfa, gracias, Paula.\n",
            "[4136.52s -> 4137.52s]  Ay, compadre.\n",
            "time: 4min 26s (started: 2024-08-24 11:57:14 +00:00)\n"
          ]
        }
      ]
    }
  ]
}